{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSDvec_wiki_10000articles.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giSYC16pobiV",
        "outputId": "7b321131-878d-47fa-ed7b-42e320e3ca90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ln -s \"/content/drive/My Drive/\" mydrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ln: failed to create symbolic link 'mydrive/My Drive': File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8obI4WTYHYo"
      },
      "source": [
        "#I. Reproduction des résultats du papier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMxh4eBUtex5"
      },
      "source": [
        "### A. PSDvec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8dw6V6qom4y",
        "outputId": "9c93ad94-eae4-4777-8c3a-bebc0c510a92"
      },
      "source": [
        "%cd /content/mydrive/PSDVEC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PSDVEC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtOYSTdOp7lk",
        "outputId": "955d77d4-7ce3-4b9a-b475-f2596bd16c2a"
      },
      "source": [
        "%cd topicvec/psdvec/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6yO9qCtnxz"
      },
      "source": [
        "#### 1. Generate stream of english words on 10 000 wikipedia article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6BayOzerPzV",
        "outputId": "e968bd18-604f-419b-9eda-a7b6712ed6d8"
      },
      "source": [
        "!python2 extractwiki.py enwiki-latest-pages-articles.xml.bz2 mini_wiki/cleanwiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-29 22:26:31,530: INFO: running extractwiki.py enwiki-latest-pages-articles.xml.bz2 mini_wiki/cleanwiki.txt\n",
            "2021-07-29 22:26:31,743: ERROR: Output file mini_wiki/cleanwiki.txt exists. Change the file name and try again.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yrnYGcy617Z",
        "outputId": "1aeff3e1-4344-4a47-9069-7844b40f40c1"
      },
      "source": [
        "! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-25 15:57:22--  https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py37_4.8.2-Linux-x86_64.sh.4’\n",
            "\n",
            "Miniconda3-py37_4.8 100%[===================>]  81.12M  45.6MB/s    in 1.8s    \n",
            "\n",
            "2021-07-25 15:57:24 (45.6 MB/s) - ‘Miniconda3-py37_4.8.2-Linux-x86_64.sh.4’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ES4f3vJrhf3"
      },
      "source": [
        "#### 3. Count frequencies of unigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhiNkei_ZCxI",
        "outputId": "f3f9137a-4111-4ca3-e03c-e596da19a40f"
      },
      "source": [
        " !conda install -c bioconda perl-list-moreutils "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - perl-list-moreutils\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2021.7.5   |       h06a4308_1         113 KB\n",
            "    certifi-2021.5.30          |   py37h06a4308_0         139 KB\n",
            "    conda-4.10.3               |   py37h06a4308_0         2.9 MB\n",
            "    openssl-1.1.1k             |       h27cfd23_0         2.5 MB\n",
            "    perl-5.26.2                |       h14c3975_0        10.5 MB\n",
            "    perl-exporter-tiny-1.002001|          pl526_0          23 KB  bioconda\n",
            "    perl-list-moreutils-0.428  |          pl526_1          28 KB  bioconda\n",
            "    perl-list-moreutils-xs-0.428|          pl526_0          43 KB  bioconda\n",
            "    perl-xsloader-0.24         |          pl526_0           8 KB  bioconda\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        16.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  perl               pkgs/main/linux-64::perl-5.26.2-h14c3975_0\n",
            "  perl-exporter-tiny bioconda/linux-64::perl-exporter-tiny-1.002001-pl526_0\n",
            "  perl-list-moreuti~ bioconda/linux-64::perl-list-moreutils-0.428-pl526_1\n",
            "  perl-list-moreuti~ bioconda/linux-64::perl-list-moreutils-xs-0.428-pl526_0\n",
            "  perl-xsloader      bioconda/linux-64::perl-xsloader-0.24-pl526_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                                2020.1.1-0 --> 2021.7.5-h06a4308_1\n",
            "  certifi                                 2019.11.28-py37_0 --> 2021.5.30-py37h06a4308_0\n",
            "  conda                                        4.8.2-py37_0 --> 4.10.3-py37h06a4308_0\n",
            "  openssl                                 1.1.1d-h7b6447c_4 --> 1.1.1k-h27cfd23_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "perl-exporter-tiny-1 | 23 KB     | : 100% 1.0/1 [00:00<00:00,  5.33it/s]               \n",
            "ca-certificates-2021 | 113 KB    | : 100% 1.0/1 [00:00<00:00,  6.48it/s]                \n",
            "perl-5.26.2          | 10.5 MB   | : 100% 1.0/1 [00:00<00:00,  1.47it/s]               \n",
            "perl-list-moreutils- | 28 KB     | : 100% 1.0/1 [00:00<00:00, 10.52it/s]\n",
            "conda-4.10.3         | 2.9 MB    | : 100% 1.0/1 [00:00<00:00,  4.93it/s]\n",
            "openssl-1.1.1k       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  5.51it/s]\n",
            "perl-list-moreutils- | 43 KB     | : 100% 1.0/1 [00:00<00:00, 10.49it/s]\n",
            "perl-xsloader-0.24   | 8 KB      | : 100% 1.0/1 [00:00<00:00, 16.29it/s]\n",
            "certifi-2021.5.30    | 139 KB    | : 100% 1.0/1 [00:00<00:00, 10.96it/s]\n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_0QCJYnrvoA",
        "outputId": "2e583afc-b567-4dca-8e8d-2e6e689586b4"
      },
      "source": [
        "!perl gramcount_modif.pl -c -i mini_wiki/cleanwiki.txt -c -m 1 --f1 mini_wiki/top1grams_mini_wiki.txt --dyn 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARN: -c is only valid when --nofilter is specified. It will be disabled now.\n",
            "mini_wiki/cleanwiki.txt:\n",
            "10000, 203.4M\n",
            "1 files, 10000 lines, 10000 sentences, 34070210 words occur, 144211 non Eng, 34070210 interest words occur\n",
            "Words cut-off frequency: 100\n",
            "Saving 20259 words into 'mini_wiki/top1grams_mini_wiki.txt'...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BErKl0JUrq8j"
      },
      "source": [
        "#### 4. Count frequencies of bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avk79wJyt8WZ",
        "outputId": "3de9c7af-3b87-4c97-dec2-b47f5d269d79"
      },
      "source": [
        "!perl gramcount_modif.pl -i mini_wiki/cleanwiki.txt -c -m 2 --f1 mini_wiki/top1grams_mini_wiki.txt --f2 mini_wiki/top2grams_mini_wiki.txt --dyn 0 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20259 words loaded from 'mini_wiki/top1grams_mini_wiki.txt'\n",
            "20259 valid unigrams\n",
            "20259 interest words\n",
            "WARN: -c is only valid when --nofilter is specified. It will be disabled now.\n",
            "mini_wiki/cleanwiki.txt:\n",
            "10000, 203.4M\n",
            "1 files, 10000 lines, 10000 sentences, 34070210 words occur, 144211 non Eng, 31645997 interest words occur\n",
            "Saving bigrams from 20259 words into 'mini_wiki/top2grams_mini_wiki.txt'...\n",
            "20199\n",
            "10814804 bigrams from 20259 words are saved into 'mini_wiki/top2grams_mini_wiki.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oQWJVe8rujL"
      },
      "source": [
        "#### 5. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPsiTn7UPjq3",
        "outputId": "aa18e518-f417-4ee9-b8bc-83da3771b03e"
      },
      "source": [
        "!python2 factorize.py -w 2000 -n 50 mini_wiki/top2grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file 'mini_wiki/top2grams_mini_wiki.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "2000\n",
            "Cut point 348: 6/0.000%\n",
            "Cut point 174: 32/0.002%\n",
            "Cut point 87: 258/0.014%\n",
            "Cut point 44: 1709/0.092%\n",
            "Cut point 22: 9344/0.503%\n",
            "Cut point 11: 35554/1.916%\n",
            "1709 (0.043%) elements in Weight-1 cut off at 43.53\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "999 positive eigenvalues, sum: 25420.736\n",
            "Eigenvalues cut at the 53-th, 58.132 ~ 57.623\n",
            "All eigen sum: 49179.086, Kept eigen sum: 5447.786\n",
            "L1 Weighted: Gi: 47746.738, VV: 34616.867, Gsym-VV: 86219.328, G-VV: 71534.420\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.61698\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.49953\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69324\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.75289\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.53354\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16775\n",
            "19500/571/19544: Add 0.51839, Mul 0.48862\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.51658, Mul Score: 0.48691\n",
            "8000/798/8000: Add 0.33459, Mul 0.25564\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.33459, Mul Score: 0.25564\n",
            "EM iter 1 elapsed: 2.59\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1002 positive eigenvalues, sum: 6611.517\n",
            "Eigenvalues cut at the 52-th, 59.750 ~ 58.732\n",
            "All eigen sum: 7775.247, Kept eigen sum: 5438.617\n",
            "L1 Weighted: Gi: 40923.564, VV: 35540.144, Gsym-VV: 85812.262, G-VV: 70980.139\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.62672\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.50013\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69359\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.76247\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.50464\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16459\n",
            "19500/571/19544: Add 0.53415, Mul 0.48511\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.53229, Mul Score: 0.48342\n",
            "8000/798/8000: Add 0.32957, Mul 0.25564\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.32957, Mul Score: 0.25564\n",
            "EM iter 2 elapsed: 2.45\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1002 positive eigenvalues, sum: 6588.589\n",
            "Eigenvalues cut at the 51-th, 60.903 ~ 60.320\n",
            "All eigen sum: 7738.560, Kept eigen sum: 5428.658\n",
            "L1 Weighted: Gi: 41477.102, VV: 36309.927, Gsym-VV: 85552.611, G-VV: 70554.088\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.62539\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.49478\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69680\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.75490\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.49020\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16352\n",
            "19500/571/19544: Add 0.54291, Mul 0.50613\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.54101, Mul Score: 0.50436\n",
            "8000/798/8000: Add 0.33960, Mul 0.27193\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.33960, Mul Score: 0.27193\n",
            "EM iter 3 elapsed: 2.43\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1001 positive eigenvalues, sum: 6565.885\n",
            "Eigenvalues cut at the 50-th, 62.223 ~ 61.680\n",
            "All eigen sum: 7703.112, Kept eigen sum: 5417.340\n",
            "L1 Weighted: Gi: 41962.512, VV: 36952.009, Gsym-VV: 85377.344, G-VV: 70215.225\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.62802\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.49268\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69663\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.75957\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.50052\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.15301\n",
            "19500/571/19544: Add 0.55166, Mul 0.50088\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.54974, Mul Score: 0.49913\n",
            "8000/798/8000: Add 0.34211, Mul 0.26817\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.34211, Mul Score: 0.26817\n",
            "EM iter 4 elapsed: 2.45\n",
            "we_factorize_EM() elapsed: 9.96\n",
            "\n",
            "Save matrix 'V' into 2000-50-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URQqMmlCsTiu"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVL5rkTcQKbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee756e1e-02fe-4d76-d837-56de406dd969"
      },
      "source": [
        "!python2 factorize.py -v 2000-50-EM.vec -n 50 -o 4000 -t2 mini_wiki/top2grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '2000-50-EM.vec' will be loaded as core\n",
            "Load embedding text file '2000-50-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "\r1000    1000    0    \r\r2000    2000    0    \r\n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file 'mini_wiki/top2grams_mini_wiki.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "2000 (2000 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "6000 (2000 core, 4000 noncore)\n",
            "Cut point 21: 430/0.029%\n",
            "Cut point 11: 6090/0.416%\n",
            "429 (0.005%) elements in Weight-1 cut off at 21.47\n",
            "354 (0.004%) elements in Weight-2 cut off at 21.47\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 3.34/0.18 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-50-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.71964\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.55114\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.67746\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.69341\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.46667\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.23426\n",
            "19500/2955/19544: Add 0.33063, Mul 0.27750\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.32929, Mul Score: 0.27643\n",
            "8000/2502/8000: Add 0.24620, Mul 0.16467\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.24620, Mul Score: 0.16467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2MK3jitskvK"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfeLMvz5BXUn",
        "outputId": "944e3dcc-c6aa-43f4-8d9b-a036e27cece4"
      },
      "source": [
        "!python2 factorize.py -v 2000-6000-50-BLK-2.0.vec -n 50 -b 2000 -o 4000 -t4 mini_wiki/top2grams_mini_wiki.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '2000-6000-50-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '2000-6000-50-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "6000    6000    0    \n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file 'mini_wiki/top2grams_mini_wiki.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "2000 (2000 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "10000 (2000 core, 4000 noncore)\n",
            "1903 (0.024%) elements in Weight-1 cut off at 9.22\n",
            "1509 (0.019%) elements in Weight-2 cut off at 9.22\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 3.24/0.08 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-50-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.69038\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.53675\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.61656\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.60192\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.35959\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.22136\n",
            "19500/5557/19544: Add 0.28163, Mul 0.22908\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.28075, Mul Score: 0.22840\n",
            "8000/3114/8000: Add 0.21162, Mul 0.14001\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.21162, Mul Score: 0.14001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG7yG2Apsor3"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlcer3-ErO5A",
        "outputId": "79ebf9d0-1b68-4ec0-86bf-1beeb2061579"
      },
      "source": [
        "!python2 factorize.py -v 2000-10000-50-BLK-4.0.vec -n 50 -b 2000 -o 4000 -t8 mini_wiki/top2grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '2000-10000-50-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '2000-10000-50-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file 'mini_wiki/top2grams_mini_wiki.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "2000 (2000 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "14000 (2000 core, 4000 noncore)\n",
            "2419 (0.030%) elements in Weight-1 cut off at 6.40\n",
            "1918 (0.024%) elements in Weight-2 cut off at 6.40\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 3.31/0.08 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-50-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.65483\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.53032\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.59180\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.58881\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.37674\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.23576\n",
            "19500/7777/19544: Add 0.23377, Mul 0.19069\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.23303, Mul Score: 0.19011\n",
            "8000/3876/8000: Add 0.18447, Mul 0.11765\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.18447, Mul Score: 0.11765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN-VizKpstA4"
      },
      "source": [
        "#### 7. Evaluation of PSDvec across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpaGdmYARC31",
        "outputId": "d0289213-d331-4f04-c233-dc5e43f58edf"
      },
      "source": [
        "!python2 evaluate.py -m 2000-14000-50-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '2000-14000-50-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.65483\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.53032\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.59180\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.58881\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.37674\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.23577\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.72266\n",
            "\n",
            "19500/7777/19544: Add 0.23377, Mul 0.19069\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.23303, Mul Score: 0.19011\n",
            "8000/3876/8000: Add 0.18447, Mul 0.11765\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.18447, Mul Score: 0.11765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKdQI4M5_vit"
      },
      "source": [
        "# we_path = '/content/mydrive/PSDVEC/topicvec/psdvec/2000-14000-50-BLK-8.0.vec'\n",
        "# import csv\n",
        "# with open (we_path, 'r') as f:\n",
        "#     csv_reader = csv.reader(f,delimiter=' ')\n",
        "#     next(csv_reader)\n",
        "#     psdvec_words = [row[0] for row in csv_reader]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsZErCXkxA05"
      },
      "source": [
        "### B. SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmOrEtjFxEq4"
      },
      "source": [
        "#### 1. Load co-occurence matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2FpxNhyml0i"
      },
      "source": [
        "path = 'mini_wiki/top2grams_mini_wiki.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMNgpNpUgLjB"
      },
      "source": [
        "def from_top2grams_to_matrix(path):\n",
        "  import pandas as pd\n",
        "\n",
        "  top2grams = open(path, \"r\")\n",
        "  lines = []; i=0\n",
        "\n",
        "  while top2grams:\n",
        "    line = top2grams.readline()\n",
        "    lines.append(line)\n",
        "\n",
        "    if line == \"\":\n",
        "      break\n",
        "\n",
        "  top2grams.close() \n",
        "\n",
        "  index = lines.index('Bigrams:\\n')\n",
        "  bigrams = lines[index+1 :]\n",
        "\n",
        "  indices = []\n",
        "  for bigram, idx in zip(bigrams, range(len(bigrams))):\n",
        "    if bigram[:1] != '\\t':\n",
        "      indices.append(idx)\n",
        "\n",
        "  words = []; idx = 0; dict_of_words = {}\n",
        "  for indice in indices[1:]:\n",
        "    try:\n",
        "      words.append(bigrams[idx:indice])\n",
        "      dict_of_words[bigrams[idx].split(',')[1]] = {}\n",
        "      idx = indice\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "  for i, k in zip(range(len(words)), dict_of_words.keys()):\n",
        "    for word in words[i][1:]:\n",
        "      word = word.split('\\t')\n",
        "      for w in word:\n",
        "        try:\n",
        "          W = w.split(',')[0]\n",
        "          C = w.split(',')[1]\n",
        "          dict_of_words[k].update({str(W) : int(C)})\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "  matrice = pd.DataFrame.from_dict(dict_of_words)\n",
        "  matrice = matrice.fillna(0)\n",
        "\n",
        "  return matrice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ZKyrbVgv2m"
      },
      "source": [
        "matrice_cooccurence = from_top2grams_to_matrix(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "4KPGXYLjxxY_",
        "outputId": "76825b11-2d0b-4b9c-de3e-36cbd1edab4c"
      },
      "source": [
        "matrice_cooccurence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>of</th>\n",
              "      <th>and</th>\n",
              "      <th>in</th>\n",
              "      <th>to</th>\n",
              "      <th>is</th>\n",
              "      <th>as</th>\n",
              "      <th>was</th>\n",
              "      <th>for</th>\n",
              "      <th>by</th>\n",
              "      <th>that</th>\n",
              "      <th>with</th>\n",
              "      <th>on</th>\n",
              "      <th>from</th>\n",
              "      <th>are</th>\n",
              "      <th>an</th>\n",
              "      <th>it</th>\n",
              "      <th>his</th>\n",
              "      <th>at</th>\n",
              "      <th>or</th>\n",
              "      <th>be</th>\n",
              "      <th>he</th>\n",
              "      <th>which</th>\n",
              "      <th>were</th>\n",
              "      <th>this</th>\n",
              "      <th>not</th>\n",
              "      <th>also</th>\n",
              "      <th>have</th>\n",
              "      <th>has</th>\n",
              "      <th>one</th>\n",
              "      <th>its</th>\n",
              "      <th>had</th>\n",
              "      <th>their</th>\n",
              "      <th>first</th>\n",
              "      <th>but</th>\n",
              "      <th>other</th>\n",
              "      <th>they</th>\n",
              "      <th>been</th>\n",
              "      <th>new</th>\n",
              "      <th>such</th>\n",
              "      <th>...</th>\n",
              "      <th>resins</th>\n",
              "      <th>fest</th>\n",
              "      <th>hatun</th>\n",
              "      <th>ul</th>\n",
              "      <th>carnivores</th>\n",
              "      <th>moi</th>\n",
              "      <th>orogeny</th>\n",
              "      <th>spp</th>\n",
              "      <th>pastry</th>\n",
              "      <th>ita</th>\n",
              "      <th>amen</th>\n",
              "      <th>ich</th>\n",
              "      <th>vedanta</th>\n",
              "      <th>ud</th>\n",
              "      <th>zur</th>\n",
              "      <th>xn</th>\n",
              "      <th>sidi</th>\n",
              "      <th>cavalier</th>\n",
              "      <th>una</th>\n",
              "      <th>continuo</th>\n",
              "      <th>gmbh</th>\n",
              "      <th>norte</th>\n",
              "      <th>preservative</th>\n",
              "      <th>borealis</th>\n",
              "      <th>wien</th>\n",
              "      <th>columbine</th>\n",
              "      <th>og</th>\n",
              "      <th>bri</th>\n",
              "      <th>daiko</th>\n",
              "      <th>kumar</th>\n",
              "      <th>begum</th>\n",
              "      <th>geschichte</th>\n",
              "      <th>tni</th>\n",
              "      <th>dal</th>\n",
              "      <th>dov</th>\n",
              "      <th>rai</th>\n",
              "      <th>cum</th>\n",
              "      <th>italiana</th>\n",
              "      <th>ind</th>\n",
              "      <th>gyatso</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>121256.0</td>\n",
              "      <td>422538.0</td>\n",
              "      <td>157368.0</td>\n",
              "      <td>302788.0</td>\n",
              "      <td>182079.0</td>\n",
              "      <td>57555.0</td>\n",
              "      <td>64477.0</td>\n",
              "      <td>38743.0</td>\n",
              "      <td>67266.0</td>\n",
              "      <td>76185.0</td>\n",
              "      <td>48643.0</td>\n",
              "      <td>62392.0</td>\n",
              "      <td>80686.0</td>\n",
              "      <td>60807.0</td>\n",
              "      <td>14846.0</td>\n",
              "      <td>4737.0</td>\n",
              "      <td>13179.0</td>\n",
              "      <td>6685.0</td>\n",
              "      <td>55073.0</td>\n",
              "      <td>13583.0</td>\n",
              "      <td>12130.0</td>\n",
              "      <td>9984.0</td>\n",
              "      <td>15935.0</td>\n",
              "      <td>10309.0</td>\n",
              "      <td>9264.0</td>\n",
              "      <td>9687.0</td>\n",
              "      <td>10744.0</td>\n",
              "      <td>7096.0</td>\n",
              "      <td>6474.0</td>\n",
              "      <td>15679.0</td>\n",
              "      <td>4161.0</td>\n",
              "      <td>6520.0</td>\n",
              "      <td>3768.0</td>\n",
              "      <td>5099.0</td>\n",
              "      <td>10704.0</td>\n",
              "      <td>4543.0</td>\n",
              "      <td>3892.0</td>\n",
              "      <td>5822.0</td>\n",
              "      <td>2731.0</td>\n",
              "      <td>6660.0</td>\n",
              "      <td>...</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>330520.0</td>\n",
              "      <td>32525.0</td>\n",
              "      <td>42910.0</td>\n",
              "      <td>37328.0</td>\n",
              "      <td>21022.0</td>\n",
              "      <td>16463.0</td>\n",
              "      <td>23719.0</td>\n",
              "      <td>10428.0</td>\n",
              "      <td>11890.0</td>\n",
              "      <td>11494.0</td>\n",
              "      <td>10949.0</td>\n",
              "      <td>13201.0</td>\n",
              "      <td>10024.0</td>\n",
              "      <td>5853.0</td>\n",
              "      <td>6062.0</td>\n",
              "      <td>12139.0</td>\n",
              "      <td>2263.0</td>\n",
              "      <td>4629.0</td>\n",
              "      <td>8040.0</td>\n",
              "      <td>6343.0</td>\n",
              "      <td>3542.0</td>\n",
              "      <td>1435.0</td>\n",
              "      <td>2066.0</td>\n",
              "      <td>3184.0</td>\n",
              "      <td>3569.0</td>\n",
              "      <td>1937.0</td>\n",
              "      <td>3596.0</td>\n",
              "      <td>1863.0</td>\n",
              "      <td>2559.0</td>\n",
              "      <td>27569.0</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>1921.0</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>7236.0</td>\n",
              "      <td>1684.0</td>\n",
              "      <td>3800.0</td>\n",
              "      <td>560.0</td>\n",
              "      <td>1770.0</td>\n",
              "      <td>2586.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>89593.0</td>\n",
              "      <td>65713.0</td>\n",
              "      <td>20273.0</td>\n",
              "      <td>48615.0</td>\n",
              "      <td>24083.0</td>\n",
              "      <td>7213.0</td>\n",
              "      <td>12406.0</td>\n",
              "      <td>7373.0</td>\n",
              "      <td>12914.0</td>\n",
              "      <td>11113.0</td>\n",
              "      <td>3699.0</td>\n",
              "      <td>11341.0</td>\n",
              "      <td>9618.0</td>\n",
              "      <td>7344.0</td>\n",
              "      <td>4999.0</td>\n",
              "      <td>3354.0</td>\n",
              "      <td>2632.0</td>\n",
              "      <td>6702.0</td>\n",
              "      <td>4296.0</td>\n",
              "      <td>3156.0</td>\n",
              "      <td>3313.0</td>\n",
              "      <td>2549.0</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>3935.0</td>\n",
              "      <td>1980.0</td>\n",
              "      <td>1515.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>1271.0</td>\n",
              "      <td>2276.0</td>\n",
              "      <td>3468.0</td>\n",
              "      <td>1337.0</td>\n",
              "      <td>4013.0</td>\n",
              "      <td>2395.0</td>\n",
              "      <td>778.0</td>\n",
              "      <td>2380.0</td>\n",
              "      <td>838.0</td>\n",
              "      <td>1456.0</td>\n",
              "      <td>2881.0</td>\n",
              "      <td>1895.0</td>\n",
              "      <td>...</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>56872.0</td>\n",
              "      <td>42426.0</td>\n",
              "      <td>43330.0</td>\n",
              "      <td>26361.0</td>\n",
              "      <td>23725.0</td>\n",
              "      <td>19136.0</td>\n",
              "      <td>12933.0</td>\n",
              "      <td>24881.0</td>\n",
              "      <td>9029.0</td>\n",
              "      <td>7673.0</td>\n",
              "      <td>7062.0</td>\n",
              "      <td>6126.0</td>\n",
              "      <td>5831.0</td>\n",
              "      <td>5617.0</td>\n",
              "      <td>10596.0</td>\n",
              "      <td>8579.0</td>\n",
              "      <td>5616.0</td>\n",
              "      <td>6664.0</td>\n",
              "      <td>4624.0</td>\n",
              "      <td>5856.0</td>\n",
              "      <td>8702.0</td>\n",
              "      <td>3750.0</td>\n",
              "      <td>4712.0</td>\n",
              "      <td>9069.0</td>\n",
              "      <td>3192.0</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>5415.0</td>\n",
              "      <td>2912.0</td>\n",
              "      <td>4271.0</td>\n",
              "      <td>2674.0</td>\n",
              "      <td>4687.0</td>\n",
              "      <td>3166.0</td>\n",
              "      <td>3203.0</td>\n",
              "      <td>5263.0</td>\n",
              "      <td>3307.0</td>\n",
              "      <td>2729.0</td>\n",
              "      <td>1870.0</td>\n",
              "      <td>4910.0</td>\n",
              "      <td>1963.0</td>\n",
              "      <td>983.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>39560.0</td>\n",
              "      <td>17538.0</td>\n",
              "      <td>28475.0</td>\n",
              "      <td>22276.0</td>\n",
              "      <td>13644.0</td>\n",
              "      <td>25407.0</td>\n",
              "      <td>8155.0</td>\n",
              "      <td>22716.0</td>\n",
              "      <td>5893.0</td>\n",
              "      <td>3901.0</td>\n",
              "      <td>5439.0</td>\n",
              "      <td>4407.0</td>\n",
              "      <td>5037.0</td>\n",
              "      <td>16798.0</td>\n",
              "      <td>10754.0</td>\n",
              "      <td>5041.0</td>\n",
              "      <td>9052.0</td>\n",
              "      <td>5237.0</td>\n",
              "      <td>1334.0</td>\n",
              "      <td>4502.0</td>\n",
              "      <td>9694.0</td>\n",
              "      <td>6608.0</td>\n",
              "      <td>4682.0</td>\n",
              "      <td>9384.0</td>\n",
              "      <td>3139.0</td>\n",
              "      <td>7787.0</td>\n",
              "      <td>5047.0</td>\n",
              "      <td>5062.0</td>\n",
              "      <td>3752.0</td>\n",
              "      <td>2346.0</td>\n",
              "      <td>2355.0</td>\n",
              "      <td>6539.0</td>\n",
              "      <td>2847.0</td>\n",
              "      <td>4561.0</td>\n",
              "      <td>2573.0</td>\n",
              "      <td>1234.0</td>\n",
              "      <td>3119.0</td>\n",
              "      <td>4732.0</td>\n",
              "      <td>1081.0</td>\n",
              "      <td>409.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>provoke</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recreate</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diclofenac</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>liturgics</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fontsize</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20259 rows × 20259 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 the        of       and        in  ...  cum  italiana  ind  gyatso\n",
              "the         121256.0  422538.0  157368.0  302788.0  ...  4.0      11.0  0.0    17.0\n",
              "of          330520.0   32525.0   42910.0   37328.0  ...  2.0       2.0  1.0     2.0\n",
              "and          89593.0   65713.0   20273.0   48615.0  ...  0.0       9.0  3.0     8.0\n",
              "in           56872.0   42426.0   43330.0   26361.0  ...  8.0      14.0  2.0     9.0\n",
              "to           39560.0   17538.0   28475.0   22276.0  ...  1.0       4.0  1.0     6.0\n",
              "...              ...       ...       ...       ...  ...  ...       ...  ...     ...\n",
              "provoke          0.0       0.0       8.0       3.0  ...  0.0       0.0  0.0     0.0\n",
              "recreate         0.0       0.0       9.0       1.0  ...  0.0       0.0  0.0     0.0\n",
              "diclofenac       0.0       0.0       6.0       0.0  ...  0.0       0.0  0.0     0.0\n",
              "liturgics        0.0       0.0       0.0       1.0  ...  0.0       0.0  0.0     0.0\n",
              "fontsize         0.0       0.0       0.0       0.0  ...  0.0       0.0  0.0     0.0\n",
              "\n",
              "[20259 rows x 20259 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gImvYLq6RMH"
      },
      "source": [
        "rows_vocab = list(matrice_cooccurence.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtyCRlOzGxGM"
      },
      "source": [
        "column_vocab = list(matrice_cooccurence.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXSeqZrwWDhz"
      },
      "source": [
        "####2. SVD decomposition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPk16KW0x5Jf"
      },
      "source": [
        "from numpy import array\n",
        "from scipy.linalg import svd\n",
        "from numpy import diag\n",
        "from numpy import dot\n",
        "\n",
        "# define a matrix\n",
        "A = array(matrice_cooccurence)\n",
        "print('Matrix A is: \\n')\n",
        "print(A)\n",
        "# SVD\n",
        "U, s, VT = svd(A)\n",
        "print('*'*120)\n",
        "print('Matrix U is: \\n')\n",
        "print(U)\n",
        "print('*'*120)\n",
        "Sigma = diag(s)\n",
        "print('Matrix Sigma is: \\n')\n",
        "print(Sigma)\n",
        "print('*'*120)\n",
        "print('Matrix VT is: \\n')\n",
        "print(VT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idIqxBy17PLW"
      },
      "source": [
        "k = 100\n",
        "U_100=U[:,:k]\n",
        "Sigma_100=Sigma[:k,:k]\n",
        "VT_100=VT[:k,:14000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dadem-ko2wPI"
      },
      "source": [
        "import pandas as pd\n",
        "V_100 = VT_100.transpose()\n",
        "V_df=pd.DataFrame(V_100,index=column_vocab[:14000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "lidEmlVs_N2b",
        "outputId": "03bcad44-b225-41d3-c3e6-5b6bb041b89b"
      },
      "source": [
        "V_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.379887</td>\n",
              "      <td>-0.896998</td>\n",
              "      <td>-0.122347</td>\n",
              "      <td>0.001245</td>\n",
              "      <td>-0.018417</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>-0.073487</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>-0.005295</td>\n",
              "      <td>0.012908</td>\n",
              "      <td>-0.010053</td>\n",
              "      <td>0.012042</td>\n",
              "      <td>0.050410</td>\n",
              "      <td>-0.011068</td>\n",
              "      <td>-0.069955</td>\n",
              "      <td>-0.026749</td>\n",
              "      <td>0.096943</td>\n",
              "      <td>-0.023178</td>\n",
              "      <td>0.075418</td>\n",
              "      <td>-0.025292</td>\n",
              "      <td>0.008369</td>\n",
              "      <td>-0.001936</td>\n",
              "      <td>0.012440</td>\n",
              "      <td>-0.005133</td>\n",
              "      <td>2.987841e-04</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.004784</td>\n",
              "      <td>-0.012473</td>\n",
              "      <td>0.027418</td>\n",
              "      <td>-0.028917</td>\n",
              "      <td>0.031914</td>\n",
              "      <td>0.004821</td>\n",
              "      <td>-0.001755</td>\n",
              "      <td>-0.018468</td>\n",
              "      <td>0.023513</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>-0.017988</td>\n",
              "      <td>0.003340</td>\n",
              "      <td>0.005266</td>\n",
              "      <td>0.004452</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001833</td>\n",
              "      <td>-0.006361</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.002489</td>\n",
              "      <td>-0.003933</td>\n",
              "      <td>-0.001501</td>\n",
              "      <td>-0.000322</td>\n",
              "      <td>-0.002717</td>\n",
              "      <td>0.002062</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>-0.001045</td>\n",
              "      <td>-0.002056</td>\n",
              "      <td>-0.002174</td>\n",
              "      <td>0.002661</td>\n",
              "      <td>0.002947</td>\n",
              "      <td>-0.000425</td>\n",
              "      <td>-0.001366</td>\n",
              "      <td>-0.008791</td>\n",
              "      <td>-0.004100</td>\n",
              "      <td>0.002018</td>\n",
              "      <td>0.002628</td>\n",
              "      <td>0.001315</td>\n",
              "      <td>-0.000455</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>-0.002706</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>-0.002317</td>\n",
              "      <td>0.003222</td>\n",
              "      <td>-0.005482</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>-0.000162</td>\n",
              "      <td>0.001472</td>\n",
              "      <td>-0.001635</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>-0.000128</td>\n",
              "      <td>0.000606</td>\n",
              "      <td>-0.000491</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>-0.001718</td>\n",
              "      <td>0.000229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.620340</td>\n",
              "      <td>0.327147</td>\n",
              "      <td>-0.238326</td>\n",
              "      <td>0.109843</td>\n",
              "      <td>0.108892</td>\n",
              "      <td>0.041360</td>\n",
              "      <td>-0.085626</td>\n",
              "      <td>-0.188101</td>\n",
              "      <td>0.105692</td>\n",
              "      <td>0.186071</td>\n",
              "      <td>0.187745</td>\n",
              "      <td>-0.133985</td>\n",
              "      <td>0.380936</td>\n",
              "      <td>0.045266</td>\n",
              "      <td>-0.031879</td>\n",
              "      <td>0.019918</td>\n",
              "      <td>0.161947</td>\n",
              "      <td>0.080715</td>\n",
              "      <td>-0.124075</td>\n",
              "      <td>-0.026761</td>\n",
              "      <td>0.033264</td>\n",
              "      <td>-0.046794</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.021702</td>\n",
              "      <td>-9.683077e-03</td>\n",
              "      <td>-0.124426</td>\n",
              "      <td>0.020758</td>\n",
              "      <td>-0.016282</td>\n",
              "      <td>0.078782</td>\n",
              "      <td>0.047255</td>\n",
              "      <td>-0.039476</td>\n",
              "      <td>0.164068</td>\n",
              "      <td>0.031256</td>\n",
              "      <td>0.022893</td>\n",
              "      <td>0.024384</td>\n",
              "      <td>-0.004036</td>\n",
              "      <td>0.010998</td>\n",
              "      <td>-0.005835</td>\n",
              "      <td>-0.025163</td>\n",
              "      <td>0.012991</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012644</td>\n",
              "      <td>0.027422</td>\n",
              "      <td>0.025616</td>\n",
              "      <td>0.002094</td>\n",
              "      <td>0.029772</td>\n",
              "      <td>0.015776</td>\n",
              "      <td>-0.002163</td>\n",
              "      <td>0.010715</td>\n",
              "      <td>-0.023656</td>\n",
              "      <td>-0.003379</td>\n",
              "      <td>0.036210</td>\n",
              "      <td>0.016859</td>\n",
              "      <td>-0.007898</td>\n",
              "      <td>-0.003045</td>\n",
              "      <td>0.008571</td>\n",
              "      <td>-0.007887</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.000672</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.004409</td>\n",
              "      <td>0.002898</td>\n",
              "      <td>0.010425</td>\n",
              "      <td>0.017638</td>\n",
              "      <td>0.007639</td>\n",
              "      <td>0.008603</td>\n",
              "      <td>0.007550</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>-0.000832</td>\n",
              "      <td>-0.015197</td>\n",
              "      <td>0.007652</td>\n",
              "      <td>0.002244</td>\n",
              "      <td>-0.008229</td>\n",
              "      <td>-0.007367</td>\n",
              "      <td>-0.007142</td>\n",
              "      <td>-0.003713</td>\n",
              "      <td>-0.000336</td>\n",
              "      <td>0.003998</td>\n",
              "      <td>-0.006175</td>\n",
              "      <td>-0.002425</td>\n",
              "      <td>0.001059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.261604</td>\n",
              "      <td>0.014721</td>\n",
              "      <td>0.403302</td>\n",
              "      <td>0.163301</td>\n",
              "      <td>0.031483</td>\n",
              "      <td>-0.043966</td>\n",
              "      <td>0.507074</td>\n",
              "      <td>-0.167181</td>\n",
              "      <td>0.030677</td>\n",
              "      <td>-0.376322</td>\n",
              "      <td>-0.034272</td>\n",
              "      <td>0.018204</td>\n",
              "      <td>0.042861</td>\n",
              "      <td>-0.126224</td>\n",
              "      <td>-0.069321</td>\n",
              "      <td>0.181376</td>\n",
              "      <td>-0.095810</td>\n",
              "      <td>0.171458</td>\n",
              "      <td>0.282484</td>\n",
              "      <td>0.037364</td>\n",
              "      <td>0.053485</td>\n",
              "      <td>-0.032551</td>\n",
              "      <td>0.132067</td>\n",
              "      <td>0.078558</td>\n",
              "      <td>-1.959709e-02</td>\n",
              "      <td>-0.111019</td>\n",
              "      <td>-0.159015</td>\n",
              "      <td>0.044085</td>\n",
              "      <td>-0.075748</td>\n",
              "      <td>-0.007090</td>\n",
              "      <td>-0.010044</td>\n",
              "      <td>0.056247</td>\n",
              "      <td>-0.004975</td>\n",
              "      <td>0.029128</td>\n",
              "      <td>-0.042270</td>\n",
              "      <td>-0.007028</td>\n",
              "      <td>0.043435</td>\n",
              "      <td>-0.075531</td>\n",
              "      <td>0.029056</td>\n",
              "      <td>0.014878</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033820</td>\n",
              "      <td>0.016875</td>\n",
              "      <td>0.016890</td>\n",
              "      <td>-0.004363</td>\n",
              "      <td>0.020769</td>\n",
              "      <td>0.007467</td>\n",
              "      <td>0.030313</td>\n",
              "      <td>-0.021900</td>\n",
              "      <td>0.025716</td>\n",
              "      <td>-0.005943</td>\n",
              "      <td>-0.015930</td>\n",
              "      <td>0.010541</td>\n",
              "      <td>0.037584</td>\n",
              "      <td>0.005369</td>\n",
              "      <td>0.031671</td>\n",
              "      <td>0.021534</td>\n",
              "      <td>0.024046</td>\n",
              "      <td>-0.012744</td>\n",
              "      <td>-0.005373</td>\n",
              "      <td>0.013390</td>\n",
              "      <td>-0.011068</td>\n",
              "      <td>-0.014622</td>\n",
              "      <td>-0.013829</td>\n",
              "      <td>0.001360</td>\n",
              "      <td>0.004917</td>\n",
              "      <td>-0.011566</td>\n",
              "      <td>-0.017519</td>\n",
              "      <td>0.024657</td>\n",
              "      <td>-0.000156</td>\n",
              "      <td>0.001283</td>\n",
              "      <td>-0.000268</td>\n",
              "      <td>-0.001794</td>\n",
              "      <td>-0.001710</td>\n",
              "      <td>0.003671</td>\n",
              "      <td>-0.004479</td>\n",
              "      <td>-0.000207</td>\n",
              "      <td>-0.008098</td>\n",
              "      <td>0.008728</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>-0.009666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.453927</td>\n",
              "      <td>0.193403</td>\n",
              "      <td>-0.140625</td>\n",
              "      <td>0.020452</td>\n",
              "      <td>0.146790</td>\n",
              "      <td>-0.001661</td>\n",
              "      <td>-0.155337</td>\n",
              "      <td>0.240211</td>\n",
              "      <td>-0.142523</td>\n",
              "      <td>-0.150220</td>\n",
              "      <td>-0.295656</td>\n",
              "      <td>0.222282</td>\n",
              "      <td>-0.490592</td>\n",
              "      <td>-0.071891</td>\n",
              "      <td>-0.126658</td>\n",
              "      <td>-0.204959</td>\n",
              "      <td>-0.185756</td>\n",
              "      <td>0.026187</td>\n",
              "      <td>0.071659</td>\n",
              "      <td>-0.006769</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>-0.022958</td>\n",
              "      <td>-0.119434</td>\n",
              "      <td>-0.104080</td>\n",
              "      <td>-5.236517e-02</td>\n",
              "      <td>-0.094581</td>\n",
              "      <td>0.097091</td>\n",
              "      <td>0.125092</td>\n",
              "      <td>-0.043343</td>\n",
              "      <td>0.063499</td>\n",
              "      <td>0.052103</td>\n",
              "      <td>-0.102978</td>\n",
              "      <td>0.058443</td>\n",
              "      <td>-0.012227</td>\n",
              "      <td>-0.076531</td>\n",
              "      <td>0.004732</td>\n",
              "      <td>0.056107</td>\n",
              "      <td>-0.001550</td>\n",
              "      <td>0.002355</td>\n",
              "      <td>-0.015010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040608</td>\n",
              "      <td>0.007143</td>\n",
              "      <td>0.014378</td>\n",
              "      <td>-0.004355</td>\n",
              "      <td>0.005745</td>\n",
              "      <td>0.011728</td>\n",
              "      <td>0.004956</td>\n",
              "      <td>-0.004711</td>\n",
              "      <td>-0.013216</td>\n",
              "      <td>-0.011079</td>\n",
              "      <td>0.023169</td>\n",
              "      <td>0.013604</td>\n",
              "      <td>0.006237</td>\n",
              "      <td>0.006747</td>\n",
              "      <td>0.005061</td>\n",
              "      <td>-0.007733</td>\n",
              "      <td>-0.006068</td>\n",
              "      <td>-0.007256</td>\n",
              "      <td>-0.003937</td>\n",
              "      <td>0.005232</td>\n",
              "      <td>-0.009158</td>\n",
              "      <td>0.003724</td>\n",
              "      <td>-0.000049</td>\n",
              "      <td>0.015308</td>\n",
              "      <td>0.013902</td>\n",
              "      <td>0.003250</td>\n",
              "      <td>0.002916</td>\n",
              "      <td>-0.001788</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>-0.004584</td>\n",
              "      <td>-0.006141</td>\n",
              "      <td>0.009331</td>\n",
              "      <td>-0.001495</td>\n",
              "      <td>0.006359</td>\n",
              "      <td>-0.010259</td>\n",
              "      <td>-0.004370</td>\n",
              "      <td>0.013709</td>\n",
              "      <td>-0.008742</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>-0.002232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.274300</td>\n",
              "      <td>0.123797</td>\n",
              "      <td>0.101571</td>\n",
              "      <td>-0.342939</td>\n",
              "      <td>-0.678548</td>\n",
              "      <td>-0.082330</td>\n",
              "      <td>-0.011013</td>\n",
              "      <td>-0.018960</td>\n",
              "      <td>-0.018710</td>\n",
              "      <td>-0.023572</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>-0.010308</td>\n",
              "      <td>0.003548</td>\n",
              "      <td>-0.094639</td>\n",
              "      <td>0.056015</td>\n",
              "      <td>0.030214</td>\n",
              "      <td>-0.000816</td>\n",
              "      <td>-0.039616</td>\n",
              "      <td>0.033683</td>\n",
              "      <td>-0.016469</td>\n",
              "      <td>0.026533</td>\n",
              "      <td>0.058797</td>\n",
              "      <td>0.088734</td>\n",
              "      <td>0.086656</td>\n",
              "      <td>9.787251e-02</td>\n",
              "      <td>0.204719</td>\n",
              "      <td>0.279591</td>\n",
              "      <td>-0.076576</td>\n",
              "      <td>0.006591</td>\n",
              "      <td>-0.037372</td>\n",
              "      <td>0.054507</td>\n",
              "      <td>-0.233611</td>\n",
              "      <td>-0.106390</td>\n",
              "      <td>-0.023963</td>\n",
              "      <td>0.101924</td>\n",
              "      <td>0.002909</td>\n",
              "      <td>0.069329</td>\n",
              "      <td>-0.039255</td>\n",
              "      <td>0.095628</td>\n",
              "      <td>0.054087</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014478</td>\n",
              "      <td>-0.002921</td>\n",
              "      <td>0.005127</td>\n",
              "      <td>0.023258</td>\n",
              "      <td>-0.000244</td>\n",
              "      <td>0.019604</td>\n",
              "      <td>-0.038578</td>\n",
              "      <td>-0.010282</td>\n",
              "      <td>0.007406</td>\n",
              "      <td>-0.010536</td>\n",
              "      <td>-0.011192</td>\n",
              "      <td>0.018343</td>\n",
              "      <td>-0.016904</td>\n",
              "      <td>-0.012937</td>\n",
              "      <td>0.010223</td>\n",
              "      <td>-0.000833</td>\n",
              "      <td>-0.012039</td>\n",
              "      <td>0.005828</td>\n",
              "      <td>0.007163</td>\n",
              "      <td>0.011318</td>\n",
              "      <td>-0.000362</td>\n",
              "      <td>-0.000491</td>\n",
              "      <td>0.015539</td>\n",
              "      <td>-0.003055</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.002880</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.002098</td>\n",
              "      <td>-0.009067</td>\n",
              "      <td>0.007270</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>-0.006746</td>\n",
              "      <td>-0.004233</td>\n",
              "      <td>-0.001360</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>-0.000772</td>\n",
              "      <td>0.002341</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>-0.000605</td>\n",
              "      <td>0.004374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drafting</th>\n",
              "      <td>-0.000073</td>\n",
              "      <td>-0.000101</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>-0.000153</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>-0.000301</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>-0.000411</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>-0.000146</td>\n",
              "      <td>-0.000017</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>-6.077842e-07</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>-0.000115</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>-0.000128</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>-0.000119</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>-0.000242</td>\n",
              "      <td>-0.000207</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>-0.000509</td>\n",
              "      <td>-0.000347</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>-0.000537</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>-0.000118</td>\n",
              "      <td>-0.000504</td>\n",
              "      <td>-0.000486</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>-0.000022</td>\n",
              "      <td>-0.000415</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>-0.000374</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>-0.000164</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-0.000165</td>\n",
              "      <td>-0.000458</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>-0.000012</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>-0.000315</td>\n",
              "      <td>-0.000133</td>\n",
              "      <td>-0.000211</td>\n",
              "      <td>0.000051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surveying</th>\n",
              "      <td>-0.000075</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.000579</td>\n",
              "      <td>-0.000384</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>-0.000161</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>-0.000113</td>\n",
              "      <td>-0.000292</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>-0.000374</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>-0.000301</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>2.069961e-04</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>-0.000264</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000028</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>-0.000227</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-0.000053</td>\n",
              "      <td>-0.000041</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>-0.000053</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000173</td>\n",
              "      <td>-0.000131</td>\n",
              "      <td>-0.000033</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>-0.000118</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>-0.000559</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>-0.000118</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>-0.000260</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>-0.000120</td>\n",
              "      <td>-0.000330</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>-0.000159</td>\n",
              "      <td>-0.000223</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>-0.000232</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.000069</td>\n",
              "      <td>-0.000061</td>\n",
              "      <td>-0.000095</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>-0.000081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delegate</th>\n",
              "      <td>-0.000086</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000373</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>-0.000314</td>\n",
              "      <td>-0.000372</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>-0.000487</td>\n",
              "      <td>-0.000534</td>\n",
              "      <td>0.000364</td>\n",
              "      <td>-0.000193</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>-0.000090</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>-0.000311</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>-0.000028</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-3.474167e-05</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>0.001101</td>\n",
              "      <td>-0.000500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>-0.000443</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>-0.000065</td>\n",
              "      <td>-0.000105</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>-0.000357</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>-0.000188</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000179</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>-0.000075</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000717</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>-0.000141</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>-0.000325</td>\n",
              "      <td>-0.000363</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>-0.000249</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>-0.000189</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.000251</td>\n",
              "      <td>-0.000298</td>\n",
              "      <td>-0.000062</td>\n",
              "      <td>-0.000144</td>\n",
              "      <td>-0.000028</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>-0.000074</td>\n",
              "      <td>-0.000081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disposition</th>\n",
              "      <td>-0.000071</td>\n",
              "      <td>-0.000139</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.000110</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>-0.000068</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>-0.000134</td>\n",
              "      <td>-0.000125</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.000125</td>\n",
              "      <td>-0.000102</td>\n",
              "      <td>-0.000033</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>-0.000378</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>-0.000640</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>-0.000041</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>-1.742540e-04</td>\n",
              "      <td>-0.000130</td>\n",
              "      <td>-0.000091</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>-0.000203</td>\n",
              "      <td>-0.000165</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>-0.000113</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000355</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000225</td>\n",
              "      <td>-0.000113</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>-0.000209</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>-0.000123</td>\n",
              "      <td>-0.000313</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>-0.000377</td>\n",
              "      <td>-0.000183</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>-0.000101</td>\n",
              "      <td>-0.000108</td>\n",
              "      <td>-0.000012</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>-0.000199</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>-0.000162</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>-0.000140</td>\n",
              "      <td>-0.000229</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>-0.000207</td>\n",
              "      <td>-0.000212</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000415</td>\n",
              "      <td>-0.000391</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>-0.000194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allele</th>\n",
              "      <td>-0.000033</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>-0.000075</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-0.000112</td>\n",
              "      <td>-0.000137</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>-0.000118</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>-0.000434</td>\n",
              "      <td>-0.000453</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>-0.000048</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>2.661329e-04</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>-0.000193</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>-0.000141</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>-0.000055</td>\n",
              "      <td>-0.000176</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.000326</td>\n",
              "      <td>-0.000707</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.000220</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>-0.000110</td>\n",
              "      <td>-0.000159</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>-0.000234</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000328</td>\n",
              "      <td>-0.000249</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>-0.000702</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>-0.000287</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>-0.000553</td>\n",
              "      <td>0.000291</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>-0.000178</td>\n",
              "      <td>-0.000394</td>\n",
              "      <td>0.000244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2   ...        97        98        99\n",
              "the         -0.379887 -0.896998 -0.122347  ...  0.000657 -0.001718  0.000229\n",
              "of          -0.620340  0.327147 -0.238326  ... -0.006175 -0.002425  0.001059\n",
              "and         -0.261604  0.014721  0.403302  ...  0.008728  0.012500 -0.009666\n",
              "in          -0.453927  0.193403 -0.140625  ... -0.008742  0.004203 -0.002232\n",
              "to          -0.274300  0.123797  0.101571  ...  0.000560 -0.000605  0.004374\n",
              "...               ...       ...       ...  ...       ...       ...       ...\n",
              "drafting    -0.000073 -0.000101 -0.000072  ... -0.000133 -0.000211  0.000051\n",
              "surveying   -0.000075 -0.000021  0.000007  ... -0.000095  0.000435 -0.000081\n",
              "delegate    -0.000086  0.000004  0.000373  ...  0.000481 -0.000074 -0.000081\n",
              "disposition -0.000071 -0.000139  0.000041  ... -0.000391  0.000055 -0.000194\n",
              "allele      -0.000033 -0.000025  0.000197  ... -0.000178 -0.000394  0.000244\n",
              "\n",
              "[14000 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elxmVqttO2RT"
      },
      "source": [
        "import csv   \n",
        "fields=['14000','100']\n",
        "with open(r'14000-100-SVD.vec', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=' ')\n",
        "    writer.writerow(fields)\n",
        "V_df.to_csv(r'14000-100-SVD.vec', header=None, index=column_vocab[:14000], sep=' ', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHTzQVm-WAIi"
      },
      "source": [
        "#### 3. Evaluation of SVD across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPVf_0ZuRAt7",
        "outputId": "25abe392-bebe-4a50-f36f-eba8fcc6ed32"
      },
      "source": [
        "!python2 evaluate.py -m 14000-100-SVD.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '14000-100-SVD.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.48234\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.24135\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.37619\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.42449\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.30397\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.15729\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.46502\n",
            "\n",
            "19500/7777/19544: Add 0.07329, Mul 0.06288\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.07328, Mul Score: 0.06264\n",
            "8000/3876/8000: Add 0.08127, Mul 0.05676\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.08127, Mul Score: 0.05676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71p8tmsAWl9B"
      },
      "source": [
        "#II. Etude comparative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSu9JCnYZ6Tm"
      },
      "source": [
        "### A. Transformation AFC en utilisant SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlJjxdmo2dpS"
      },
      "source": [
        "rows_vocab = list(matrice_cooccurence.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0SybIpX2dpV"
      },
      "source": [
        "column_vocab = list(matrice_cooccurence.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArB2v188QOgi"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Du95vD-XXbO"
      },
      "source": [
        "def afc_transformation(matrix):\n",
        "  import numpy as np\n",
        "\n",
        "  #Dr somme selon les colonnes, axis = 1 on somme suivant les colonnes\n",
        "  #Dc some selon les lignes, axis = 0 on somme suivant les lignes\n",
        "  dr, dc = matrix.sum(1), matrix.sum(0)\n",
        "\n",
        "  dr, dc = np.array(dr), np.array(dc)\n",
        "  dr_inv, dc_inv = np.power(dr,-1/2), np.power(dc, -1/2)\n",
        "  Dr_inv, Dc_inv = np.diag(dr_inv), np.diag(dc_inv)\n",
        "\n",
        "  # return np.multiply(matrix - Dr_Dc, Dr_Dc_inv)\n",
        "  return Dr_inv.dot(matrice_cooccurence).dot(Dc_inv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUQpaLJeXXbP"
      },
      "source": [
        "AFC_mat = afc_transformation(matrice_cooccurence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gR5WEbtQfQF"
      },
      "source": [
        "#### 2. SVD on Co-occurence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2PCKSdE14SW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264bfec6-ddd7-4ed8-acc2-78559d08f4a0"
      },
      "source": [
        "from numpy import array\n",
        "from scipy.linalg import svd\n",
        "from numpy import diag\n",
        "from numpy import dot\n",
        "\n",
        "# define a matrix\n",
        "A = array(AFC_mat)\n",
        "print('Matrix A is: \\n')\n",
        "print(A)\n",
        "# SVD\n",
        "U, s, VT = svd(A)\n",
        "print('*'*120)\n",
        "print('Matrix U is: \\n')\n",
        "print(U)\n",
        "print('*'*120)\n",
        "Sigma = diag(s)\n",
        "print('Matrix Sigma is: \\n')\n",
        "print(Sigma)\n",
        "print('*'*120)\n",
        "print('Matrix VT is: \\n')\n",
        "print(VT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A is: \n",
            "\n",
            "[[2.58279507e-02 1.22910929e-01 5.18926654e-02 ... 3.71614442e-04\n",
            "  0.00000000e+00 5.53742868e-04]\n",
            " [9.54224726e-02 1.28235492e-02 1.91784842e-02 ... 9.15790439e-05\n",
            "  5.50486900e-05 8.82989280e-05]\n",
            " [3.00906787e-02 3.01402753e-02 1.05409260e-02 ... 4.79417189e-04\n",
            "  1.92120286e-04 4.10885111e-04]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 3.21831482e-04 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "************************************************************************************************************************\n",
            "Matrix U is: \n",
            "\n",
            "[[-2.82999586e-01  1.15801844e-03 -6.67601813e-04 ...  1.05805512e-03\n",
            "  -1.69875332e-03  2.40294375e-04]\n",
            " [-2.08794757e-01  7.60286437e-04 -5.20603403e-04 ...  5.64927942e-04\n",
            "   8.33813480e-05  7.52436326e-04]\n",
            " [-1.79479400e-01  7.54137626e-04 -3.65578745e-04 ... -3.64755796e-04\n",
            "  -4.84838857e-05 -3.02590029e-03]\n",
            " ...\n",
            " [-1.73979500e-03 -8.38028164e-05 -4.68812116e-06 ... -8.01678406e-03\n",
            "  -6.53530008e-03 -9.75327706e-04]\n",
            " [-2.91065296e-03  1.59549407e-05 -6.83851110e-06 ... -1.22508880e-03\n",
            "   4.16700816e-04 -2.65775304e-03]\n",
            " [-2.08872573e-03  1.08135974e-05 -6.37008780e-06 ...  6.67242496e-03\n",
            "  -6.62637232e-03  5.66407963e-03]]\n",
            "************************************************************************************************************************\n",
            "Matrix Sigma is: \n",
            "\n",
            "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 8.86414029e-01 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 8.32359030e-01 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.09239067e-06\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  3.30604034e-06 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 2.07756040e-06]]\n",
            "************************************************************************************************************************\n",
            "Matrix VT is: \n",
            "\n",
            "[[-2.80524383e-01 -2.05415210e-01 -1.81203997e-01 ... -1.76871324e-03\n",
            "  -1.47121637e-03 -1.83441715e-03]\n",
            " [ 1.22559238e-03  6.50700296e-04  8.29818232e-04 ...  2.85917224e-06\n",
            "   6.73194143e-06  9.28554439e-06]\n",
            " [-5.24571791e-04 -3.34439511e-04 -3.27663335e-04 ... -4.65956982e-06\n",
            "  -3.37917329e-06 -4.80463227e-06]\n",
            " ...\n",
            " [ 9.33210521e-04  2.15393617e-03  8.15369533e-04 ... -1.08113537e-02\n",
            "   3.94065389e-03 -2.38172241e-03]\n",
            " [ 6.91631458e-04 -3.26126555e-03 -1.40245121e-03 ... -7.54789490e-03\n",
            "  -3.39244419e-03  4.55125765e-04]\n",
            " [ 3.73013131e-04 -1.77809763e-03 -3.84479264e-03 ... -1.20376402e-02\n",
            "   3.49943980e-03 -1.49679038e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq_e-bLP14SZ"
      },
      "source": [
        "k = 100\n",
        "U_100=U[:,:k]\n",
        "Sigma_100=Sigma[:k,:k]\n",
        "VT_100=VT[:k,:14000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kTwl-IO14Sa"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "V_100 = VT_100.transpose()\n",
        "V_df=pd.DataFrame(V_100,index=column_vocab[:14000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeZLgK1s14Sc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "3426ca4f-4c23-4096-9ebf-bcf2f9547a0a"
      },
      "source": [
        "V_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.280524</td>\n",
              "      <td>0.001226</td>\n",
              "      <td>-0.000525</td>\n",
              "      <td>0.001018</td>\n",
              "      <td>-6.338547e-04</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>-0.001350</td>\n",
              "      <td>0.001812</td>\n",
              "      <td>0.001448</td>\n",
              "      <td>-0.002217</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>-0.000937</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>-0.002031</td>\n",
              "      <td>0.007281</td>\n",
              "      <td>-0.001201</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>-0.004459</td>\n",
              "      <td>0.004834</td>\n",
              "      <td>-0.000095</td>\n",
              "      <td>0.014966</td>\n",
              "      <td>-0.006549</td>\n",
              "      <td>0.005877</td>\n",
              "      <td>-0.023247</td>\n",
              "      <td>0.009240</td>\n",
              "      <td>-0.006744</td>\n",
              "      <td>0.005375</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>-0.018542</td>\n",
              "      <td>-0.006986</td>\n",
              "      <td>-0.008968</td>\n",
              "      <td>0.027543</td>\n",
              "      <td>-0.030103</td>\n",
              "      <td>-0.194237</td>\n",
              "      <td>0.066007</td>\n",
              "      <td>-0.019231</td>\n",
              "      <td>0.028547</td>\n",
              "      <td>0.008820</td>\n",
              "      <td>-0.004883</td>\n",
              "      <td>-0.006464</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002899</td>\n",
              "      <td>-0.064030</td>\n",
              "      <td>0.003757</td>\n",
              "      <td>-0.005003</td>\n",
              "      <td>0.007582</td>\n",
              "      <td>-0.002423</td>\n",
              "      <td>-0.014964</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>-0.014910</td>\n",
              "      <td>0.005497</td>\n",
              "      <td>-0.018620</td>\n",
              "      <td>-0.019550</td>\n",
              "      <td>0.004502</td>\n",
              "      <td>-0.030752</td>\n",
              "      <td>0.073443</td>\n",
              "      <td>-0.021687</td>\n",
              "      <td>0.005928</td>\n",
              "      <td>-0.051162</td>\n",
              "      <td>-0.007459</td>\n",
              "      <td>0.040267</td>\n",
              "      <td>0.020726</td>\n",
              "      <td>0.041422</td>\n",
              "      <td>-0.056063</td>\n",
              "      <td>0.025981</td>\n",
              "      <td>0.002840</td>\n",
              "      <td>-0.038720</td>\n",
              "      <td>-0.068245</td>\n",
              "      <td>0.028824</td>\n",
              "      <td>0.128815</td>\n",
              "      <td>-0.121315</td>\n",
              "      <td>0.178358</td>\n",
              "      <td>-0.025351</td>\n",
              "      <td>-8.720099e-02</td>\n",
              "      <td>-0.118676</td>\n",
              "      <td>0.018052</td>\n",
              "      <td>0.133581</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>-0.010313</td>\n",
              "      <td>-0.189722</td>\n",
              "      <td>0.052516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.205415</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>-0.000334</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>-7.450046e-04</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>-0.000321</td>\n",
              "      <td>-0.000209</td>\n",
              "      <td>-0.000478</td>\n",
              "      <td>-0.001085</td>\n",
              "      <td>-0.000651</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>-0.001447</td>\n",
              "      <td>0.002041</td>\n",
              "      <td>0.002667</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>-0.000184</td>\n",
              "      <td>0.003539</td>\n",
              "      <td>0.001853</td>\n",
              "      <td>0.001278</td>\n",
              "      <td>-0.002378</td>\n",
              "      <td>0.004987</td>\n",
              "      <td>-0.004088</td>\n",
              "      <td>0.006018</td>\n",
              "      <td>-0.001806</td>\n",
              "      <td>0.003429</td>\n",
              "      <td>0.001114</td>\n",
              "      <td>-0.006120</td>\n",
              "      <td>-0.002619</td>\n",
              "      <td>-0.001124</td>\n",
              "      <td>0.006750</td>\n",
              "      <td>-0.014882</td>\n",
              "      <td>-0.065245</td>\n",
              "      <td>0.022551</td>\n",
              "      <td>-0.001890</td>\n",
              "      <td>0.005186</td>\n",
              "      <td>0.001385</td>\n",
              "      <td>-0.001369</td>\n",
              "      <td>-0.004278</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002321</td>\n",
              "      <td>-0.015749</td>\n",
              "      <td>-0.002077</td>\n",
              "      <td>-0.018928</td>\n",
              "      <td>0.003107</td>\n",
              "      <td>-0.001647</td>\n",
              "      <td>-0.008124</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>-0.013996</td>\n",
              "      <td>0.009320</td>\n",
              "      <td>0.004940</td>\n",
              "      <td>0.014025</td>\n",
              "      <td>-0.005010</td>\n",
              "      <td>0.025561</td>\n",
              "      <td>-0.033099</td>\n",
              "      <td>0.005179</td>\n",
              "      <td>0.001047</td>\n",
              "      <td>-0.033430</td>\n",
              "      <td>-0.008573</td>\n",
              "      <td>0.009660</td>\n",
              "      <td>0.016978</td>\n",
              "      <td>0.025003</td>\n",
              "      <td>-0.071776</td>\n",
              "      <td>-0.002965</td>\n",
              "      <td>-0.001230</td>\n",
              "      <td>0.011526</td>\n",
              "      <td>0.009381</td>\n",
              "      <td>-0.002048</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.010346</td>\n",
              "      <td>-0.042576</td>\n",
              "      <td>0.005106</td>\n",
              "      <td>2.470121e-02</td>\n",
              "      <td>0.035324</td>\n",
              "      <td>0.028263</td>\n",
              "      <td>-0.022874</td>\n",
              "      <td>-0.004154</td>\n",
              "      <td>-0.003562</td>\n",
              "      <td>0.027112</td>\n",
              "      <td>-0.006722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.181204</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>-0.000328</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>-3.777940e-04</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000612</td>\n",
              "      <td>0.001126</td>\n",
              "      <td>-0.000469</td>\n",
              "      <td>0.001265</td>\n",
              "      <td>-0.000390</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>-0.000405</td>\n",
              "      <td>-0.001787</td>\n",
              "      <td>-0.000840</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.002345</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>0.002731</td>\n",
              "      <td>-0.010834</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.000082</td>\n",
              "      <td>-0.000212</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>-0.003603</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>-0.000312</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>-0.004416</td>\n",
              "      <td>-0.034471</td>\n",
              "      <td>0.010368</td>\n",
              "      <td>-0.002483</td>\n",
              "      <td>0.002283</td>\n",
              "      <td>-0.001678</td>\n",
              "      <td>-0.000514</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001134</td>\n",
              "      <td>0.023723</td>\n",
              "      <td>-0.002941</td>\n",
              "      <td>0.008343</td>\n",
              "      <td>-0.002455</td>\n",
              "      <td>0.002781</td>\n",
              "      <td>0.011911</td>\n",
              "      <td>-0.001900</td>\n",
              "      <td>0.018502</td>\n",
              "      <td>-0.006299</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>-0.009161</td>\n",
              "      <td>0.002341</td>\n",
              "      <td>-0.000802</td>\n",
              "      <td>-0.003445</td>\n",
              "      <td>0.002845</td>\n",
              "      <td>-0.000992</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.001102</td>\n",
              "      <td>0.003832</td>\n",
              "      <td>-0.003485</td>\n",
              "      <td>-0.003213</td>\n",
              "      <td>-0.005122</td>\n",
              "      <td>-0.032743</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.045780</td>\n",
              "      <td>0.007492</td>\n",
              "      <td>-0.002648</td>\n",
              "      <td>-0.018897</td>\n",
              "      <td>0.034252</td>\n",
              "      <td>0.013928</td>\n",
              "      <td>-0.003394</td>\n",
              "      <td>-1.324827e-02</td>\n",
              "      <td>-0.001292</td>\n",
              "      <td>0.013539</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.002534</td>\n",
              "      <td>0.005779</td>\n",
              "      <td>-0.036612</td>\n",
              "      <td>0.011929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.174166</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>-0.000204</td>\n",
              "      <td>-0.001352</td>\n",
              "      <td>-4.527745e-04</td>\n",
              "      <td>-0.001923</td>\n",
              "      <td>-0.000358</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>-0.008896</td>\n",
              "      <td>-0.002680</td>\n",
              "      <td>-0.007767</td>\n",
              "      <td>0.005394</td>\n",
              "      <td>-0.007666</td>\n",
              "      <td>0.002698</td>\n",
              "      <td>0.011150</td>\n",
              "      <td>-0.000560</td>\n",
              "      <td>-0.000391</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.004450</td>\n",
              "      <td>-0.000365</td>\n",
              "      <td>0.001940</td>\n",
              "      <td>-0.002201</td>\n",
              "      <td>0.006891</td>\n",
              "      <td>-0.003247</td>\n",
              "      <td>0.009717</td>\n",
              "      <td>-0.001389</td>\n",
              "      <td>0.009984</td>\n",
              "      <td>0.000944</td>\n",
              "      <td>-0.003598</td>\n",
              "      <td>-0.001130</td>\n",
              "      <td>-0.001266</td>\n",
              "      <td>0.005218</td>\n",
              "      <td>-0.013913</td>\n",
              "      <td>-0.058335</td>\n",
              "      <td>0.020484</td>\n",
              "      <td>0.001833</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>0.008774</td>\n",
              "      <td>-0.002028</td>\n",
              "      <td>-0.000988</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000049</td>\n",
              "      <td>-0.001463</td>\n",
              "      <td>-0.001742</td>\n",
              "      <td>-0.032939</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.001145</td>\n",
              "      <td>0.001813</td>\n",
              "      <td>-0.004516</td>\n",
              "      <td>0.005870</td>\n",
              "      <td>0.004163</td>\n",
              "      <td>0.017798</td>\n",
              "      <td>-0.003736</td>\n",
              "      <td>0.033316</td>\n",
              "      <td>-0.056465</td>\n",
              "      <td>0.006233</td>\n",
              "      <td>0.006294</td>\n",
              "      <td>-0.036516</td>\n",
              "      <td>0.002829</td>\n",
              "      <td>0.024162</td>\n",
              "      <td>0.017118</td>\n",
              "      <td>0.036049</td>\n",
              "      <td>-0.073432</td>\n",
              "      <td>-0.003631</td>\n",
              "      <td>-0.004353</td>\n",
              "      <td>0.006599</td>\n",
              "      <td>-0.001364</td>\n",
              "      <td>0.008721</td>\n",
              "      <td>0.058168</td>\n",
              "      <td>-0.021428</td>\n",
              "      <td>-0.076404</td>\n",
              "      <td>0.002124</td>\n",
              "      <td>5.140188e-02</td>\n",
              "      <td>0.055049</td>\n",
              "      <td>0.020147</td>\n",
              "      <td>-0.051703</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.003247</td>\n",
              "      <td>0.094078</td>\n",
              "      <td>-0.025774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.152626</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>-0.000360</td>\n",
              "      <td>0.000479</td>\n",
              "      <td>-7.805281e-04</td>\n",
              "      <td>0.003099</td>\n",
              "      <td>-0.000320</td>\n",
              "      <td>-0.001063</td>\n",
              "      <td>0.001519</td>\n",
              "      <td>0.002750</td>\n",
              "      <td>0.000388</td>\n",
              "      <td>-0.000227</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>-0.000746</td>\n",
              "      <td>-0.004089</td>\n",
              "      <td>-0.000681</td>\n",
              "      <td>-0.001835</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>0.016731</td>\n",
              "      <td>-0.001113</td>\n",
              "      <td>-0.006614</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>-0.009963</td>\n",
              "      <td>0.025276</td>\n",
              "      <td>-0.009032</td>\n",
              "      <td>0.002211</td>\n",
              "      <td>-0.004296</td>\n",
              "      <td>-0.004349</td>\n",
              "      <td>0.012427</td>\n",
              "      <td>0.003215</td>\n",
              "      <td>0.005862</td>\n",
              "      <td>-0.011802</td>\n",
              "      <td>0.020925</td>\n",
              "      <td>0.125161</td>\n",
              "      <td>-0.037761</td>\n",
              "      <td>0.008255</td>\n",
              "      <td>-0.010711</td>\n",
              "      <td>-0.000880</td>\n",
              "      <td>-0.000221</td>\n",
              "      <td>0.002761</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003078</td>\n",
              "      <td>-0.014484</td>\n",
              "      <td>-0.005474</td>\n",
              "      <td>-0.012730</td>\n",
              "      <td>0.006254</td>\n",
              "      <td>-0.009979</td>\n",
              "      <td>-0.027337</td>\n",
              "      <td>-0.000412</td>\n",
              "      <td>-0.010208</td>\n",
              "      <td>0.003784</td>\n",
              "      <td>-0.017392</td>\n",
              "      <td>-0.011363</td>\n",
              "      <td>0.006928</td>\n",
              "      <td>-0.031375</td>\n",
              "      <td>0.107518</td>\n",
              "      <td>-0.052353</td>\n",
              "      <td>0.012844</td>\n",
              "      <td>-0.076503</td>\n",
              "      <td>-0.040664</td>\n",
              "      <td>-0.018430</td>\n",
              "      <td>0.115787</td>\n",
              "      <td>0.147148</td>\n",
              "      <td>-0.397115</td>\n",
              "      <td>-0.002255</td>\n",
              "      <td>0.009189</td>\n",
              "      <td>0.018577</td>\n",
              "      <td>0.024909</td>\n",
              "      <td>0.000822</td>\n",
              "      <td>-0.005422</td>\n",
              "      <td>0.044206</td>\n",
              "      <td>-0.014391</td>\n",
              "      <td>0.003815</td>\n",
              "      <td>1.498259e-02</td>\n",
              "      <td>0.015682</td>\n",
              "      <td>0.014111</td>\n",
              "      <td>-0.002954</td>\n",
              "      <td>-0.006295</td>\n",
              "      <td>-0.008634</td>\n",
              "      <td>-0.015971</td>\n",
              "      <td>0.008169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drafting</th>\n",
              "      <td>-0.002362</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-8.094775e-07</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>-0.000034</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-0.000102</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-0.000022</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>-0.000089</td>\n",
              "      <td>-0.000651</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>-0.000100</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>-0.000060</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000030</td>\n",
              "      <td>-0.000307</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>-0.000139</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-0.000121</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>-0.000189</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>-0.000153</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>-0.000048</td>\n",
              "      <td>-0.000176</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>4.652510e-05</td>\n",
              "      <td>-0.000136</td>\n",
              "      <td>-0.000234</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surveying</th>\n",
              "      <td>-0.002380</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-1.437600e-05</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000022</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>-0.000117</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>-0.000049</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>-0.000022</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>-0.000167</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>-0.000047</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-0.000017</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>-0.000041</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000022</td>\n",
              "      <td>-0.000568</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000104</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>-0.000244</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-0.000115</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>-0.000128</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>-0.001386</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>-0.000036</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>-0.000191</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000443</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>-0.000349</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>-6.118745e-07</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>-0.000197</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>-0.000028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delegate</th>\n",
              "      <td>-0.002457</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>-9.517949e-06</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-0.000046</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>-0.000059</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.000012</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.000017</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000076</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>-0.000070</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>-0.000049</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>-0.000167</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>-0.001179</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>-0.000219</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>-0.001305</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>-0.000108</td>\n",
              "      <td>-0.000074</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000705</td>\n",
              "      <td>0.001543</td>\n",
              "      <td>-0.002775</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>1.931552e-03</td>\n",
              "      <td>0.001759</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>-0.001537</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.002077</td>\n",
              "      <td>-0.000653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disposition</th>\n",
              "      <td>-0.002355</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-1.685415e-05</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>-0.000027</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>-0.000137</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>-0.000126</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>-0.000057</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-0.000108</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>-0.000250</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000033</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000031</td>\n",
              "      <td>-0.000339</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>-0.000060</td>\n",
              "      <td>-0.000253</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>-0.000248</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>-0.000242</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>-0.000027</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>-0.000518</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>-0.000039</td>\n",
              "      <td>-0.000279</td>\n",
              "      <td>0.000730</td>\n",
              "      <td>-0.000825</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>5.492112e-04</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>-0.000518</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>0.000083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allele</th>\n",
              "      <td>-0.002297</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-1.771599e-05</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>-0.000017</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-0.000133</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>-0.000132</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>-0.000209</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>-0.000199</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>-0.000110</td>\n",
              "      <td>-0.000090</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>-0.000201</td>\n",
              "      <td>0.000364</td>\n",
              "      <td>0.002187</td>\n",
              "      <td>-0.000632</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>-0.000113</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-0.000878</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>-0.000076</td>\n",
              "      <td>-0.000424</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>-0.000447</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>-0.000012</td>\n",
              "      <td>-0.000326</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>-0.000146</td>\n",
              "      <td>-0.000073</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>-0.000063</td>\n",
              "      <td>-0.000399</td>\n",
              "      <td>-0.000284</td>\n",
              "      <td>-0.000691</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>-0.000109</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>-0.000421</td>\n",
              "      <td>-0.001884</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000607</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>-6.144331e-04</td>\n",
              "      <td>-0.000530</td>\n",
              "      <td>-0.000449</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>-0.000216</td>\n",
              "      <td>-0.000488</td>\n",
              "      <td>0.000159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2   ...        97        98        99\n",
              "the         -0.280524  0.001226 -0.000525  ... -0.010313 -0.189722  0.052516\n",
              "of          -0.205415  0.000651 -0.000334  ... -0.003562  0.027112 -0.006722\n",
              "and         -0.181204  0.000830 -0.000328  ...  0.005779 -0.036612  0.011929\n",
              "in          -0.174166  0.000042 -0.000204  ... -0.003247  0.094078 -0.025774\n",
              "to          -0.152626  0.000601 -0.000360  ... -0.008634 -0.015971  0.008169\n",
              "...               ...       ...       ...  ...       ...       ...       ...\n",
              "drafting    -0.002362  0.000011 -0.000006  ...  0.000115 -0.000014  0.000003\n",
              "surveying   -0.002380  0.000011 -0.000007  ...  0.000038  0.000058 -0.000028\n",
              "delegate    -0.002457  0.000012 -0.000007  ...  0.000419  0.002077 -0.000653\n",
              "disposition -0.002355  0.000011 -0.000007  ... -0.000009 -0.000306  0.000083\n",
              "allele      -0.002297  0.000011 -0.000006  ... -0.000216 -0.000488  0.000159\n",
              "\n",
              "[14000 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UYBLuQ92-FC"
      },
      "source": [
        "import csv   \n",
        "fields=['14000','100']\n",
        "with open(r'14000-100-AFC-SVD.vec', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=' ')\n",
        "    writer.writerow(fields)\n",
        "V_df.to_csv(r'14000-100-AFC-SVD.vec', header=None, index=column_vocab[:14000], sep=' ', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKAhlyrU-FyR"
      },
      "source": [
        "#### 3. Evaluation of AFC across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7cH_VpP-FyR",
        "outputId": "2eb5fd4f-56bf-49e2-b38f-7b240da8505b"
      },
      "source": [
        "!python2 evaluate.py -m 14000-100-AFC-SVD.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '14000-100-AFC-SVD.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.44602\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.18856\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.33156\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.37787\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.17782\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.11807\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.51034\n",
            "\n",
            "19500/7777/19544: Add 0.05632, Mul 0.04385\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.05611, Mul Score: 0.04368\n",
            "8000/3876/8000: Add 0.03664, Mul 0.02193\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.03664, Mul Score: 0.02193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RutAYPjcaXmg"
      },
      "source": [
        "### B. Transformation puissance alpha = 0 en utilisant SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkhfaWmQpyB"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Y_HtwNBlxp"
      },
      "source": [
        "def power_of_alpha(matrix, alpha):\n",
        "  \n",
        "  new_matrix = matrix.to_numpy()\n",
        "  new_matrix = np.power(matrix, alpha)\n",
        "\n",
        "  return new_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0dspsGCCM7D"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "alph_mat = power_of_alpha(matrice_cooccurence, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfIGdXtBQpyC"
      },
      "source": [
        "#### 2. SVD on Co-occurence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRDdp1n1-xVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a50293e-b8b5-4593-dbc5-ff0e2ec4cc61"
      },
      "source": [
        "from numpy import array\n",
        "from scipy.linalg import svd\n",
        "from numpy import diag\n",
        "from numpy import dot\n",
        "\n",
        "# define a matrix\n",
        "A = array(alph_mat)\n",
        "print('Matrix A is: \\n')\n",
        "print(A)\n",
        "# SVD\n",
        "U, s, VT = svd(A)\n",
        "print('*'*120)\n",
        "print('Matrix U is: \\n')\n",
        "print(U)\n",
        "print('*'*120)\n",
        "Sigma = diag(s)\n",
        "print('Matrix Sigma is: \\n')\n",
        "print(Sigma)\n",
        "print('*'*120)\n",
        "print('Matrix VT is: \\n')\n",
        "print(VT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A is: \n",
            "\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " ...\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "************************************************************************************************************************\n",
            "Matrix U is: \n",
            "\n",
            "[[-7.02572259e-03  3.57902856e-04 -5.10551649e-04 ...  3.07883821e-01\n",
            "   3.50543435e-02 -4.44379893e-02]\n",
            " [-7.02572259e-03 -5.10708823e-04  7.35860563e-04 ... -5.54750863e-01\n",
            "  -5.77709305e-02  2.21330985e-02]\n",
            " [-7.02572259e-03 -3.08657181e-04  3.70811349e-04 ... -2.02653349e-02\n",
            "  -1.47995587e-02  6.40732503e-02]\n",
            " ...\n",
            " [-7.02572259e-03  2.87460805e-05 -5.44164098e-05 ...  3.19837410e-05\n",
            "  -2.60116123e-05  3.63873555e-05]\n",
            " [-7.02572259e-03 -2.58396790e-04  2.58439326e-04 ... -4.98271464e-03\n",
            "   7.13900090e-03 -1.32747631e-02]\n",
            " [-7.02572259e-03 -2.51367427e-03  1.00826958e-02 ... -7.14204858e-04\n",
            "   4.02919834e-03 -1.31068356e-02]]\n",
            "************************************************************************************************************************\n",
            "Matrix Sigma is: \n",
            "\n",
            "[[2.02590000e+04 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 3.49574191e-09 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 3.21477626e-09 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.85184421e-12\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  1.13142049e-12 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 4.21647519e-13]]\n",
            "************************************************************************************************************************\n",
            "Matrix VT is: \n",
            "\n",
            "[[-7.02572259e-03 -7.02572259e-03 -7.02572259e-03 ... -7.02572259e-03\n",
            "  -7.02572259e-03 -7.02572259e-03]\n",
            " [-1.45080114e-06  3.81971413e-05  6.69744509e-05 ... -1.39369135e-07\n",
            "   3.37653811e-04  3.72706353e-04]\n",
            " [ 0.00000000e+00 -5.75940401e-05 -1.40906736e-04 ...  4.77020097e-05\n",
            "  -2.27277753e-03 -2.21055550e-03]\n",
            " ...\n",
            " [-3.18485956e-01 -3.13917819e-03 -3.10296125e-03 ...  6.48819379e-05\n",
            "  -1.90667130e-02 -1.94481958e-02]\n",
            " [ 0.00000000e+00 -3.28032067e-04 -3.57185941e-04 ...  1.21875834e-04\n",
            "   3.80865425e-02  3.66827911e-02]\n",
            " [ 0.00000000e+00  9.34373368e-04 -4.46826432e-03 ...  1.36376359e-04\n",
            "  -3.91803900e-02 -4.72222879e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktn9LZuw-xVe"
      },
      "source": [
        "k = 100\n",
        "U_100=U[:,:k]\n",
        "Sigma_100=Sigma[:k,:k]\n",
        "VT_100=VT[:k,:14000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z38cFuwM-xVe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "V_100 = VT_100.transpose()\n",
        "V_df=pd.DataFrame(V_100,index=column_vocab[:14000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "MhzjpfhS-xVf",
        "outputId": "39dde0d0-c160-4bc4-b084-6c93392a6fce"
      },
      "source": [
        "V_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>-1.450801e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-6.804083e-18</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>4.128303e-18</td>\n",
              "      <td>-6.440149e-06</td>\n",
              "      <td>6.191289e-11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>6.256340e-12</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>-0.000752</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>-0.002109</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.000056</td>\n",
              "      <td>-7.343031e-09</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-0.000106</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>-0.003262</td>\n",
              "      <td>-0.008122</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>-0.000116</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>-0.016877</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.001578</td>\n",
              "      <td>0.028896</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.092161</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.026836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.942389</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>3.819714e-05</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>-1.696312e-03</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>2.035179e-04</td>\n",
              "      <td>1.299258e-04</td>\n",
              "      <td>9.603886e-04</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.676941</td>\n",
              "      <td>8.400046e-04</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.233348</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>-0.000946</td>\n",
              "      <td>0.551127</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>7.984100e-05</td>\n",
              "      <td>-0.002531</td>\n",
              "      <td>0.002909</td>\n",
              "      <td>-0.000069</td>\n",
              "      <td>-0.001037</td>\n",
              "      <td>0.255341</td>\n",
              "      <td>0.333377</td>\n",
              "      <td>-0.013195</td>\n",
              "      <td>-0.003994</td>\n",
              "      <td>0.003399</td>\n",
              "      <td>-0.000688</td>\n",
              "      <td>0.082903</td>\n",
              "      <td>-0.000069</td>\n",
              "      <td>-0.000894</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>-0.000187</td>\n",
              "      <td>-0.000172</td>\n",
              "      <td>-0.000017</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>-0.006448</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-3.460824e-06</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>5.236033e-06</td>\n",
              "      <td>-4.133414e-06</td>\n",
              "      <td>-4.973967e-06</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>-4.118533e-07</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>6.371254e-07</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>3.297593e-07</td>\n",
              "      <td>-7.828955e-07</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>6.002543e-07</td>\n",
              "      <td>-9.687973e-07</td>\n",
              "      <td>-1.505074e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>8.893672e-07</td>\n",
              "      <td>9.838799e-07</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>2.696567e-07</td>\n",
              "      <td>1.234628e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>2.443523e-06</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-1.149733e-06</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>-2.258941e-06</td>\n",
              "      <td>-8.906395e-07</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>1.509885e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>6.697445e-05</td>\n",
              "      <td>-0.000141</td>\n",
              "      <td>-2.935973e-03</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>3.180466e-04</td>\n",
              "      <td>2.244936e-04</td>\n",
              "      <td>1.667179e-03</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.673465</td>\n",
              "      <td>1.523621e-03</td>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.054838</td>\n",
              "      <td>0.003464</td>\n",
              "      <td>-0.002106</td>\n",
              "      <td>0.272838</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.002280</td>\n",
              "      <td>1.411549e-04</td>\n",
              "      <td>-0.004121</td>\n",
              "      <td>0.004983</td>\n",
              "      <td>-0.000169</td>\n",
              "      <td>-0.002023</td>\n",
              "      <td>0.373165</td>\n",
              "      <td>0.552657</td>\n",
              "      <td>-0.021854</td>\n",
              "      <td>-0.006920</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>-0.001080</td>\n",
              "      <td>0.153330</td>\n",
              "      <td>-0.000151</td>\n",
              "      <td>-0.001930</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>-0.000051</td>\n",
              "      <td>0.001989</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>-0.000493</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>-0.007247</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>1.189678e-05</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>-0.000041</td>\n",
              "      <td>-0.000056</td>\n",
              "      <td>3.813350e-07</td>\n",
              "      <td>3.010916e-05</td>\n",
              "      <td>4.183468e-05</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-0.000027</td>\n",
              "      <td>-0.000031</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-2.286921e-05</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>-1.897862e-05</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>6.910337e-06</td>\n",
              "      <td>-1.143514e-06</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>-3.251820e-06</td>\n",
              "      <td>3.545877e-06</td>\n",
              "      <td>1.975794e-06</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>1.697009e-05</td>\n",
              "      <td>-2.697823e-06</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>-1.945558e-06</td>\n",
              "      <td>-9.615525e-07</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>-1.140393e-05</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>1.495967e-05</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>7.805556e-07</td>\n",
              "      <td>-8.512595e-06</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>-2.368837e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>7.056841e-05</td>\n",
              "      <td>-0.000155</td>\n",
              "      <td>-3.167236e-03</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>3.431829e-04</td>\n",
              "      <td>2.324536e-04</td>\n",
              "      <td>1.771674e-03</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>-0.294822</td>\n",
              "      <td>1.649336e-03</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>-0.317043</td>\n",
              "      <td>0.005904</td>\n",
              "      <td>-0.002741</td>\n",
              "      <td>-0.673929</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.002350</td>\n",
              "      <td>1.510068e-04</td>\n",
              "      <td>-0.004126</td>\n",
              "      <td>0.005046</td>\n",
              "      <td>-0.000188</td>\n",
              "      <td>-0.002673</td>\n",
              "      <td>0.215498</td>\n",
              "      <td>0.522145</td>\n",
              "      <td>-0.020586</td>\n",
              "      <td>-0.007221</td>\n",
              "      <td>0.007092</td>\n",
              "      <td>-0.001001</td>\n",
              "      <td>0.196281</td>\n",
              "      <td>-0.000097</td>\n",
              "      <td>-0.002829</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>0.001951</td>\n",
              "      <td>-0.000041</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>-0.000479</td>\n",
              "      <td>-0.000224</td>\n",
              "      <td>-0.006410</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-3.028431e-07</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>5.593094e-06</td>\n",
              "      <td>4.995031e-07</td>\n",
              "      <td>6.504452e-07</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>-1.145553e-05</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>5.703731e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>1.271574e-06</td>\n",
              "      <td>-6.478318e-06</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-5.058927e-06</td>\n",
              "      <td>7.542355e-06</td>\n",
              "      <td>-2.070978e-07</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>9.025432e-07</td>\n",
              "      <td>3.795797e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>3.965491e-06</td>\n",
              "      <td>8.926264e-06</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>9.923610e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>2.624198e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-3.581697e-06</td>\n",
              "      <td>-6.180173e-06</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-6.692861e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>1.447173e-04</td>\n",
              "      <td>-0.000512</td>\n",
              "      <td>-1.079418e-02</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>1.181315e-03</td>\n",
              "      <td>3.630383e-04</td>\n",
              "      <td>6.061785e-03</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.033176</td>\n",
              "      <td>5.592544e-03</td>\n",
              "      <td>0.001487</td>\n",
              "      <td>0.573805</td>\n",
              "      <td>0.014087</td>\n",
              "      <td>-0.008185</td>\n",
              "      <td>-0.145353</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.003717</td>\n",
              "      <td>5.111846e-04</td>\n",
              "      <td>-0.013649</td>\n",
              "      <td>0.006796</td>\n",
              "      <td>-0.000620</td>\n",
              "      <td>-0.012378</td>\n",
              "      <td>-0.606285</td>\n",
              "      <td>0.253835</td>\n",
              "      <td>-0.010194</td>\n",
              "      <td>-0.008186</td>\n",
              "      <td>0.020494</td>\n",
              "      <td>-0.002122</td>\n",
              "      <td>0.463405</td>\n",
              "      <td>-0.000569</td>\n",
              "      <td>-0.006096</td>\n",
              "      <td>0.003018</td>\n",
              "      <td>-0.000278</td>\n",
              "      <td>0.004852</td>\n",
              "      <td>-0.000099</td>\n",
              "      <td>0.001435</td>\n",
              "      <td>-0.001196</td>\n",
              "      <td>-0.000435</td>\n",
              "      <td>-0.006113</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000031</td>\n",
              "      <td>-1.312517e-05</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>-6.069112e-07</td>\n",
              "      <td>1.265091e-06</td>\n",
              "      <td>-7.723221e-06</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>-0.000022</td>\n",
              "      <td>-1.241832e-05</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>1.256922e-05</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>-9.340191e-06</td>\n",
              "      <td>-3.832746e-06</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>-4.497031e-06</td>\n",
              "      <td>2.357494e-05</td>\n",
              "      <td>6.983872e-06</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>-3.633035e-06</td>\n",
              "      <td>9.669993e-06</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>9.564042e-06</td>\n",
              "      <td>9.907918e-06</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>-2.107008e-06</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>-3.190494e-06</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>-4.514363e-06</td>\n",
              "      <td>-6.195225e-06</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>-4.968816e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drafting</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>-1.393691e-07</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>1.471013e-04</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-1.207070e-05</td>\n",
              "      <td>-6.779533e-07</td>\n",
              "      <td>-8.037733e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-8.421455e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-0.000198</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>5.257834e-08</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>-0.000253</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>-0.000230</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>-0.000258</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000144</td>\n",
              "      <td>2.099162e-04</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>2.928782e-06</td>\n",
              "      <td>-1.996384e-05</td>\n",
              "      <td>-5.472512e-05</td>\n",
              "      <td>-0.000090</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>7.647521e-05</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>8.148031e-06</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>2.601348e-05</td>\n",
              "      <td>3.775838e-05</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-9.700572e-06</td>\n",
              "      <td>2.371555e-05</td>\n",
              "      <td>-3.250387e-05</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-1.022401e-05</td>\n",
              "      <td>1.164219e-06</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-5.927644e-05</td>\n",
              "      <td>-3.143781e-05</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-7.009314e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>5.489752e-07</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>2.947894e-06</td>\n",
              "      <td>-3.377912e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-4.419717e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surveying</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>-1.393691e-07</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>1.471013e-04</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-1.207070e-05</td>\n",
              "      <td>-6.779533e-07</td>\n",
              "      <td>-8.037733e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-8.421455e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-0.000198</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>5.257834e-08</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>-0.000253</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>-0.000230</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>-0.000258</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000144</td>\n",
              "      <td>2.099162e-04</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>2.928782e-06</td>\n",
              "      <td>-1.996384e-05</td>\n",
              "      <td>-5.472512e-05</td>\n",
              "      <td>-0.000090</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>7.647521e-05</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>8.148031e-06</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>2.601348e-05</td>\n",
              "      <td>3.775838e-05</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-9.700572e-06</td>\n",
              "      <td>2.371555e-05</td>\n",
              "      <td>-3.250387e-05</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-1.022401e-05</td>\n",
              "      <td>1.164219e-06</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-5.927644e-05</td>\n",
              "      <td>-3.143781e-05</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-7.009314e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>5.489752e-07</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>2.947894e-06</td>\n",
              "      <td>-3.377912e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-4.419717e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delegate</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>-1.393691e-07</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>1.471013e-04</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-1.207070e-05</td>\n",
              "      <td>-6.779533e-07</td>\n",
              "      <td>-8.037733e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-8.421455e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-0.000198</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>5.257834e-08</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>-0.000253</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>-0.000230</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>-0.000258</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000144</td>\n",
              "      <td>2.099162e-04</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>2.928782e-06</td>\n",
              "      <td>-1.996384e-05</td>\n",
              "      <td>-5.472512e-05</td>\n",
              "      <td>-0.000090</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>7.647521e-05</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>8.148031e-06</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>2.601348e-05</td>\n",
              "      <td>3.775838e-05</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-9.700572e-06</td>\n",
              "      <td>2.371555e-05</td>\n",
              "      <td>-3.250387e-05</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-1.022401e-05</td>\n",
              "      <td>1.164219e-06</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-5.927644e-05</td>\n",
              "      <td>-3.143781e-05</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-7.009314e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>5.489752e-07</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>2.947894e-06</td>\n",
              "      <td>-3.377912e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-4.419717e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disposition</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>-1.393691e-07</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>1.471013e-04</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-1.207070e-05</td>\n",
              "      <td>-6.779533e-07</td>\n",
              "      <td>-8.037733e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-8.421455e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-0.000198</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>5.257834e-08</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>-0.000253</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>-0.000230</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>-0.000258</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000144</td>\n",
              "      <td>2.099162e-04</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>2.928782e-06</td>\n",
              "      <td>-1.996384e-05</td>\n",
              "      <td>-5.472512e-05</td>\n",
              "      <td>-0.000090</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>7.647521e-05</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>8.148031e-06</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>2.601348e-05</td>\n",
              "      <td>3.775838e-05</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-9.700572e-06</td>\n",
              "      <td>2.371555e-05</td>\n",
              "      <td>-3.250387e-05</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-1.022401e-05</td>\n",
              "      <td>1.164219e-06</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-5.927644e-05</td>\n",
              "      <td>-3.143781e-05</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-7.009314e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>5.489752e-07</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>2.947894e-06</td>\n",
              "      <td>-3.377912e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-4.419717e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allele</th>\n",
              "      <td>-0.007026</td>\n",
              "      <td>-1.393691e-07</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>1.471013e-04</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-1.207070e-05</td>\n",
              "      <td>-6.779533e-07</td>\n",
              "      <td>-8.037733e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-8.421455e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-0.000198</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>5.257834e-08</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>-0.000021</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>-0.000253</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>-0.000230</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>-0.000258</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000144</td>\n",
              "      <td>2.099162e-04</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>2.928782e-06</td>\n",
              "      <td>-1.996384e-05</td>\n",
              "      <td>-5.472512e-05</td>\n",
              "      <td>-0.000090</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>7.647521e-05</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>8.148031e-06</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>2.601348e-05</td>\n",
              "      <td>3.775838e-05</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-9.700572e-06</td>\n",
              "      <td>2.371555e-05</td>\n",
              "      <td>-3.250387e-05</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-1.022401e-05</td>\n",
              "      <td>1.164219e-06</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-5.927644e-05</td>\n",
              "      <td>-3.143781e-05</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-7.009314e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>5.489752e-07</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>2.947894e-06</td>\n",
              "      <td>-3.377912e-05</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-4.419717e-05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0             1   ...        98            99\n",
              "the         -0.007026 -1.450801e-06  ...  0.000000  0.000000e+00\n",
              "of          -0.007026  3.819714e-05  ... -0.000001  1.509885e-07\n",
              "and         -0.007026  6.697445e-05  ... -0.000007 -2.368837e-06\n",
              "in          -0.007026  7.056841e-05  ...  0.000003 -6.692861e-07\n",
              "to          -0.007026  1.447173e-04  ...  0.000010 -4.968816e-06\n",
              "...               ...           ...  ...       ...           ...\n",
              "drafting    -0.007026 -1.393691e-07  ... -0.000029 -4.419717e-05\n",
              "surveying   -0.007026 -1.393691e-07  ... -0.000029 -4.419717e-05\n",
              "delegate    -0.007026 -1.393691e-07  ... -0.000029 -4.419717e-05\n",
              "disposition -0.007026 -1.393691e-07  ... -0.000029 -4.419717e-05\n",
              "allele      -0.007026 -1.393691e-07  ... -0.000029 -4.419717e-05\n",
              "\n",
              "[14000 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q7OJnoEQpyD"
      },
      "source": [
        "#### 3. Evaluation of alpha (0) transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YD6kzMB-xVg"
      },
      "source": [
        "import csv   \n",
        "fields=['14000','100']\n",
        "with open(r'14000-100-alpha0.vec', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=' ')\n",
        "    writer.writerow(fields)\n",
        "V_df.to_csv(r'14000-100-alpha0-SVD.vec'', header=None, index=column_vocab[:14000], sep=' ', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG5yV0rC-xVh",
        "outputId": "d24662b9-3f34-401a-e0a4-01a6b559bd5d"
      },
      "source": [
        "!python2 evaluate.py -m 14000-100-alpha0-SVD.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '14000-100-alpha0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , -0.12524\n",
            "ws353_relatedness: 252 test pairs, 217 valid , -0.07575\n",
            "bruni_men: 3000 test pairs, 1817 valid , -0.02100\n",
            "radinsky_mturk: 287 test pairs, 237 valid , -0.04567\n",
            "luong_rare: 2034 test pairs, 262 valid , -0.09126\n",
            "simlex_999a: 999 test pairs, 774 valid , -0.03615\n",
            "EN-RG-65: 65 test pairs, 29 valid , -0.13553\n",
            "\n",
            "19500/7777/19544: Add 0.00000, Mul 0.00000\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.00000, Mul Score: 0.00000\n",
            "8000/3876/8000: Add 0.00000, Mul 0.00000\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.00000, Mul Score: 0.00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df1uHtaXUnlH"
      },
      "source": [
        "### B. Transformation puissance alpha = 0.1 en utilisant SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au3wuKJsUnlI"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA1uP_WrUnlI"
      },
      "source": [
        "def power_of_alpha(matrix, alpha):\n",
        "  \n",
        "  new_matrix = matrix.to_numpy()\n",
        "  new_matrix = np.power(matrix, alpha)\n",
        "\n",
        "  return new_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jp0KWSdUnlJ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "alph_mat = power_of_alpha(matrice_cooccurence, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdxLMn9kUnlJ"
      },
      "source": [
        "#### 2. SVD on Co-occurence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2DXDuPTUnlK",
        "outputId": "f6316ea2-160c-4787-922d-d6b496f0c949"
      },
      "source": [
        "from numpy import array\n",
        "from scipy.linalg import svd\n",
        "from numpy import diag\n",
        "from numpy import dot\n",
        "\n",
        "# define a matrix\n",
        "A = array(alph_mat)\n",
        "print('Matrix A is: \\n')\n",
        "print(A)\n",
        "# SVD\n",
        "U, s, VT = svd(A)\n",
        "print('*'*120)\n",
        "print('Matrix U is: \\n')\n",
        "print(U)\n",
        "print('*'*120)\n",
        "Sigma = diag(s)\n",
        "print('Matrix Sigma is: \\n')\n",
        "print(Sigma)\n",
        "print('*'*120)\n",
        "print('Matrix VT is: \\n')\n",
        "print(VT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A is: \n",
            "\n",
            "[[3.22381657 3.65246933 3.30896095 ... 1.27098162 0.         1.32753167]\n",
            " [3.56385294 2.8263226  2.90573398 ... 1.07177346 1.         1.07177346]\n",
            " [3.12771669 3.03225087 2.69582566 ... 1.24573094 1.11612317 1.23114441]\n",
            " ...\n",
            " [0.         0.         1.1962312  ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            "************************************************************************************************************************\n",
            "Matrix U is: \n",
            "\n",
            "[[-0.09081207 -0.15514762 -0.0059504  ... -0.00016232  0.00053653\n",
            "  -0.00562341]\n",
            " [-0.08396313 -0.14155144 -0.0087189  ...  0.00176066  0.00198565\n",
            "   0.00121508]\n",
            " [-0.08585605 -0.15195418  0.00238036 ...  0.00194643 -0.00440428\n",
            "   0.00286539]\n",
            " ...\n",
            " [-0.0004229   0.00128039 -0.00087937 ... -0.01413505  0.01025262\n",
            "   0.0130947 ]\n",
            " [-0.00023406  0.00040845  0.00126359 ...  0.06049173 -0.0144965\n",
            "  -0.05024986]\n",
            " [-0.00022009  0.0003298   0.0002828  ... -0.00718361 -0.0309971\n",
            "   0.03122253]]\n",
            "************************************************************************************************************************\n",
            "Matrix Sigma is: \n",
            "\n",
            "[[1.78862643e+03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.43182034e+02 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 3.86349062e+02 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.38073499e-03\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  9.22245701e-04 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 7.51240791e-05]]\n",
            "************************************************************************************************************************\n",
            "Matrix VT is: \n",
            "\n",
            "[[-0.08691721 -0.08165943 -0.08117289 ... -0.00157776 -0.00100378\n",
            "  -0.0017223 ]\n",
            " [ 0.16431432  0.15457578  0.15308215 ... -0.00347067 -0.00189901\n",
            "  -0.00412326]\n",
            " [-0.00017623  0.0012356   0.0058923  ...  0.00101468  0.00107496\n",
            "   0.00113698]\n",
            " ...\n",
            " [ 0.00218253 -0.00164823  0.00110116 ... -0.0020352  -0.02037539\n",
            "   0.00850232]\n",
            " [ 0.00469922 -0.00268993 -0.00134593 ... -0.00303489 -0.00899939\n",
            "   0.00959287]\n",
            " [ 0.001918   -0.00054517 -0.00033538 ...  0.00622548 -0.00782991\n",
            "  -0.01078212]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbbqU8JtUnlL"
      },
      "source": [
        "k = 100\n",
        "U_100=U[:,:k]\n",
        "Sigma_100=Sigma[:k,:k]\n",
        "VT_100=VT[:k,:14000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkSD_FmfUnlL"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "V_100 = VT_100.transpose()\n",
        "V_df=pd.DataFrame(V_100,index=column_vocab[:14000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "6oREy7WwUnlM",
        "outputId": "7eef0cc5-2689-48fa-e894-00f7ab2e2489"
      },
      "source": [
        "V_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.086917</td>\n",
              "      <td>0.164314</td>\n",
              "      <td>-0.000176</td>\n",
              "      <td>0.194462</td>\n",
              "      <td>-0.063715</td>\n",
              "      <td>0.009207</td>\n",
              "      <td>-0.004529</td>\n",
              "      <td>0.040402</td>\n",
              "      <td>-0.016173</td>\n",
              "      <td>0.092476</td>\n",
              "      <td>-0.079768</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>-0.033498</td>\n",
              "      <td>0.057705</td>\n",
              "      <td>-0.081946</td>\n",
              "      <td>0.019388</td>\n",
              "      <td>-0.067810</td>\n",
              "      <td>0.032221</td>\n",
              "      <td>-0.016150</td>\n",
              "      <td>0.013140</td>\n",
              "      <td>-0.034794</td>\n",
              "      <td>0.013858</td>\n",
              "      <td>0.028321</td>\n",
              "      <td>-0.056686</td>\n",
              "      <td>0.008091</td>\n",
              "      <td>-0.063703</td>\n",
              "      <td>0.042508</td>\n",
              "      <td>-0.050898</td>\n",
              "      <td>0.003428</td>\n",
              "      <td>-0.009436</td>\n",
              "      <td>0.001386</td>\n",
              "      <td>-0.030185</td>\n",
              "      <td>0.050160</td>\n",
              "      <td>-0.008409</td>\n",
              "      <td>0.012859</td>\n",
              "      <td>-0.030669</td>\n",
              "      <td>0.028477</td>\n",
              "      <td>-0.024917</td>\n",
              "      <td>-0.026346</td>\n",
              "      <td>0.029201</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007602</td>\n",
              "      <td>0.002257</td>\n",
              "      <td>-0.021185</td>\n",
              "      <td>0.009651</td>\n",
              "      <td>-0.007378</td>\n",
              "      <td>0.019421</td>\n",
              "      <td>-0.003676</td>\n",
              "      <td>0.029577</td>\n",
              "      <td>-0.004572</td>\n",
              "      <td>0.003647</td>\n",
              "      <td>0.004632</td>\n",
              "      <td>-0.004051</td>\n",
              "      <td>0.023876</td>\n",
              "      <td>0.004646</td>\n",
              "      <td>-0.021856</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.008676</td>\n",
              "      <td>0.011318</td>\n",
              "      <td>-0.014180</td>\n",
              "      <td>-0.000879</td>\n",
              "      <td>0.005464</td>\n",
              "      <td>0.003063</td>\n",
              "      <td>0.016672</td>\n",
              "      <td>-0.023979</td>\n",
              "      <td>0.015151</td>\n",
              "      <td>0.003291</td>\n",
              "      <td>0.008216</td>\n",
              "      <td>-0.008553</td>\n",
              "      <td>-0.031210</td>\n",
              "      <td>0.011369</td>\n",
              "      <td>0.009616</td>\n",
              "      <td>0.008112</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.004199</td>\n",
              "      <td>0.009816</td>\n",
              "      <td>-0.016926</td>\n",
              "      <td>0.010080</td>\n",
              "      <td>-0.009822</td>\n",
              "      <td>0.003166</td>\n",
              "      <td>-0.003512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.081659</td>\n",
              "      <td>0.154576</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>0.187418</td>\n",
              "      <td>-0.062549</td>\n",
              "      <td>0.013240</td>\n",
              "      <td>0.006228</td>\n",
              "      <td>0.043783</td>\n",
              "      <td>-0.024622</td>\n",
              "      <td>0.102389</td>\n",
              "      <td>-0.079329</td>\n",
              "      <td>0.003878</td>\n",
              "      <td>-0.033089</td>\n",
              "      <td>0.052663</td>\n",
              "      <td>-0.081360</td>\n",
              "      <td>0.014193</td>\n",
              "      <td>-0.026882</td>\n",
              "      <td>0.009943</td>\n",
              "      <td>-0.000183</td>\n",
              "      <td>0.013375</td>\n",
              "      <td>-0.027616</td>\n",
              "      <td>0.022959</td>\n",
              "      <td>0.018007</td>\n",
              "      <td>-0.060260</td>\n",
              "      <td>0.011218</td>\n",
              "      <td>-0.032447</td>\n",
              "      <td>0.051291</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>-0.004857</td>\n",
              "      <td>0.008157</td>\n",
              "      <td>-0.029965</td>\n",
              "      <td>0.032919</td>\n",
              "      <td>0.008884</td>\n",
              "      <td>0.033057</td>\n",
              "      <td>-0.037627</td>\n",
              "      <td>0.028320</td>\n",
              "      <td>-0.025691</td>\n",
              "      <td>-0.023111</td>\n",
              "      <td>0.019022</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016698</td>\n",
              "      <td>-0.017312</td>\n",
              "      <td>-0.023000</td>\n",
              "      <td>0.012064</td>\n",
              "      <td>-0.002314</td>\n",
              "      <td>0.007492</td>\n",
              "      <td>0.006015</td>\n",
              "      <td>0.020155</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.010606</td>\n",
              "      <td>0.008126</td>\n",
              "      <td>-0.005987</td>\n",
              "      <td>0.011806</td>\n",
              "      <td>-0.001334</td>\n",
              "      <td>-0.015935</td>\n",
              "      <td>0.013223</td>\n",
              "      <td>0.010735</td>\n",
              "      <td>0.005093</td>\n",
              "      <td>-0.013388</td>\n",
              "      <td>-0.003431</td>\n",
              "      <td>0.014787</td>\n",
              "      <td>-0.011300</td>\n",
              "      <td>0.024731</td>\n",
              "      <td>-0.021607</td>\n",
              "      <td>0.009478</td>\n",
              "      <td>0.006499</td>\n",
              "      <td>-0.001893</td>\n",
              "      <td>-0.021575</td>\n",
              "      <td>-0.026983</td>\n",
              "      <td>0.012412</td>\n",
              "      <td>-0.006041</td>\n",
              "      <td>0.006463</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>-0.003289</td>\n",
              "      <td>-0.001774</td>\n",
              "      <td>-0.008246</td>\n",
              "      <td>-0.002111</td>\n",
              "      <td>-0.005113</td>\n",
              "      <td>-0.001534</td>\n",
              "      <td>-0.005187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.081173</td>\n",
              "      <td>0.153082</td>\n",
              "      <td>0.005892</td>\n",
              "      <td>0.187572</td>\n",
              "      <td>-0.047765</td>\n",
              "      <td>-0.008511</td>\n",
              "      <td>-0.003220</td>\n",
              "      <td>0.055180</td>\n",
              "      <td>-0.007418</td>\n",
              "      <td>0.090960</td>\n",
              "      <td>-0.078696</td>\n",
              "      <td>-0.019869</td>\n",
              "      <td>-0.043010</td>\n",
              "      <td>0.027171</td>\n",
              "      <td>-0.084064</td>\n",
              "      <td>0.025425</td>\n",
              "      <td>-0.043751</td>\n",
              "      <td>0.019232</td>\n",
              "      <td>-0.017132</td>\n",
              "      <td>0.021893</td>\n",
              "      <td>-0.045084</td>\n",
              "      <td>0.012038</td>\n",
              "      <td>0.014701</td>\n",
              "      <td>-0.050008</td>\n",
              "      <td>0.031778</td>\n",
              "      <td>-0.040132</td>\n",
              "      <td>0.059940</td>\n",
              "      <td>-0.053401</td>\n",
              "      <td>0.010919</td>\n",
              "      <td>-0.009239</td>\n",
              "      <td>0.002198</td>\n",
              "      <td>-0.029178</td>\n",
              "      <td>0.018240</td>\n",
              "      <td>0.007888</td>\n",
              "      <td>0.029563</td>\n",
              "      <td>-0.040994</td>\n",
              "      <td>0.027639</td>\n",
              "      <td>-0.025199</td>\n",
              "      <td>-0.024404</td>\n",
              "      <td>0.027540</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019036</td>\n",
              "      <td>-0.004946</td>\n",
              "      <td>-0.028824</td>\n",
              "      <td>0.014934</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>-0.000413</td>\n",
              "      <td>-0.002554</td>\n",
              "      <td>0.026563</td>\n",
              "      <td>0.011060</td>\n",
              "      <td>-0.002244</td>\n",
              "      <td>0.002198</td>\n",
              "      <td>-0.011155</td>\n",
              "      <td>0.008183</td>\n",
              "      <td>0.005972</td>\n",
              "      <td>-0.004942</td>\n",
              "      <td>0.020417</td>\n",
              "      <td>0.007153</td>\n",
              "      <td>-0.001275</td>\n",
              "      <td>-0.019209</td>\n",
              "      <td>0.002378</td>\n",
              "      <td>0.002683</td>\n",
              "      <td>-0.012106</td>\n",
              "      <td>0.016981</td>\n",
              "      <td>-0.008137</td>\n",
              "      <td>0.002402</td>\n",
              "      <td>0.013048</td>\n",
              "      <td>0.001162</td>\n",
              "      <td>-0.020620</td>\n",
              "      <td>-0.029603</td>\n",
              "      <td>0.008222</td>\n",
              "      <td>-0.007539</td>\n",
              "      <td>0.009907</td>\n",
              "      <td>-0.005529</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000822</td>\n",
              "      <td>-0.005053</td>\n",
              "      <td>-0.003829</td>\n",
              "      <td>-0.012582</td>\n",
              "      <td>0.008044</td>\n",
              "      <td>0.000216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.076822</td>\n",
              "      <td>0.140813</td>\n",
              "      <td>0.009037</td>\n",
              "      <td>0.164903</td>\n",
              "      <td>-0.058012</td>\n",
              "      <td>0.010333</td>\n",
              "      <td>-0.006063</td>\n",
              "      <td>0.044612</td>\n",
              "      <td>-0.018773</td>\n",
              "      <td>0.097883</td>\n",
              "      <td>-0.083729</td>\n",
              "      <td>0.014095</td>\n",
              "      <td>-0.007983</td>\n",
              "      <td>0.035852</td>\n",
              "      <td>-0.060985</td>\n",
              "      <td>0.014567</td>\n",
              "      <td>-0.017888</td>\n",
              "      <td>0.019409</td>\n",
              "      <td>-0.004305</td>\n",
              "      <td>0.024494</td>\n",
              "      <td>-0.021372</td>\n",
              "      <td>0.021623</td>\n",
              "      <td>0.026488</td>\n",
              "      <td>-0.047271</td>\n",
              "      <td>0.005641</td>\n",
              "      <td>-0.041155</td>\n",
              "      <td>0.038165</td>\n",
              "      <td>-0.053282</td>\n",
              "      <td>-0.000849</td>\n",
              "      <td>0.007835</td>\n",
              "      <td>-0.001841</td>\n",
              "      <td>-0.025110</td>\n",
              "      <td>0.011779</td>\n",
              "      <td>0.021649</td>\n",
              "      <td>0.012097</td>\n",
              "      <td>-0.040823</td>\n",
              "      <td>0.035257</td>\n",
              "      <td>-0.032157</td>\n",
              "      <td>-0.032766</td>\n",
              "      <td>0.004827</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004458</td>\n",
              "      <td>0.005251</td>\n",
              "      <td>-0.015963</td>\n",
              "      <td>0.004464</td>\n",
              "      <td>-0.009931</td>\n",
              "      <td>-0.008427</td>\n",
              "      <td>0.011422</td>\n",
              "      <td>0.021137</td>\n",
              "      <td>0.006547</td>\n",
              "      <td>-0.006435</td>\n",
              "      <td>-0.004448</td>\n",
              "      <td>0.005158</td>\n",
              "      <td>0.005358</td>\n",
              "      <td>0.005940</td>\n",
              "      <td>-0.021117</td>\n",
              "      <td>0.009702</td>\n",
              "      <td>0.005419</td>\n",
              "      <td>0.009160</td>\n",
              "      <td>0.004848</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.011304</td>\n",
              "      <td>-0.012068</td>\n",
              "      <td>0.019370</td>\n",
              "      <td>-0.016806</td>\n",
              "      <td>0.010906</td>\n",
              "      <td>0.010988</td>\n",
              "      <td>-0.002323</td>\n",
              "      <td>-0.011302</td>\n",
              "      <td>-0.018920</td>\n",
              "      <td>0.004495</td>\n",
              "      <td>-0.005270</td>\n",
              "      <td>0.007250</td>\n",
              "      <td>-0.014599</td>\n",
              "      <td>0.010718</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>-0.005348</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>-0.004655</td>\n",
              "      <td>0.006175</td>\n",
              "      <td>-0.005724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.075542</td>\n",
              "      <td>0.140740</td>\n",
              "      <td>-0.005253</td>\n",
              "      <td>0.165772</td>\n",
              "      <td>-0.044757</td>\n",
              "      <td>-0.019889</td>\n",
              "      <td>-0.003233</td>\n",
              "      <td>0.025169</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.087238</td>\n",
              "      <td>-0.077141</td>\n",
              "      <td>-0.000125</td>\n",
              "      <td>-0.024263</td>\n",
              "      <td>0.006018</td>\n",
              "      <td>-0.079907</td>\n",
              "      <td>0.029070</td>\n",
              "      <td>-0.037777</td>\n",
              "      <td>0.008754</td>\n",
              "      <td>0.016378</td>\n",
              "      <td>0.041080</td>\n",
              "      <td>-0.036313</td>\n",
              "      <td>-0.026333</td>\n",
              "      <td>0.044614</td>\n",
              "      <td>-0.027922</td>\n",
              "      <td>0.019925</td>\n",
              "      <td>-0.053561</td>\n",
              "      <td>0.056748</td>\n",
              "      <td>-0.026544</td>\n",
              "      <td>-0.000979</td>\n",
              "      <td>0.020063</td>\n",
              "      <td>0.010943</td>\n",
              "      <td>-0.047926</td>\n",
              "      <td>0.041021</td>\n",
              "      <td>0.006742</td>\n",
              "      <td>0.012108</td>\n",
              "      <td>-0.041453</td>\n",
              "      <td>0.025300</td>\n",
              "      <td>-0.035671</td>\n",
              "      <td>-0.016870</td>\n",
              "      <td>0.008307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023415</td>\n",
              "      <td>-0.002213</td>\n",
              "      <td>-0.005541</td>\n",
              "      <td>-0.000047</td>\n",
              "      <td>0.003793</td>\n",
              "      <td>-0.008721</td>\n",
              "      <td>0.008620</td>\n",
              "      <td>0.009517</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>-0.018824</td>\n",
              "      <td>0.010430</td>\n",
              "      <td>-0.008476</td>\n",
              "      <td>0.012722</td>\n",
              "      <td>0.003794</td>\n",
              "      <td>-0.003014</td>\n",
              "      <td>0.013766</td>\n",
              "      <td>0.007916</td>\n",
              "      <td>-0.007729</td>\n",
              "      <td>0.003731</td>\n",
              "      <td>0.014712</td>\n",
              "      <td>0.007914</td>\n",
              "      <td>-0.017272</td>\n",
              "      <td>0.004787</td>\n",
              "      <td>-0.023370</td>\n",
              "      <td>0.011570</td>\n",
              "      <td>0.007093</td>\n",
              "      <td>-0.002421</td>\n",
              "      <td>-0.022857</td>\n",
              "      <td>-0.027421</td>\n",
              "      <td>-0.002241</td>\n",
              "      <td>-0.005091</td>\n",
              "      <td>0.016099</td>\n",
              "      <td>-0.006311</td>\n",
              "      <td>-0.001446</td>\n",
              "      <td>-0.003378</td>\n",
              "      <td>-0.011642</td>\n",
              "      <td>0.003454</td>\n",
              "      <td>0.001767</td>\n",
              "      <td>-0.007641</td>\n",
              "      <td>0.004338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drafting</th>\n",
              "      <td>-0.002095</td>\n",
              "      <td>-0.003536</td>\n",
              "      <td>-0.000396</td>\n",
              "      <td>0.003458</td>\n",
              "      <td>-0.002114</td>\n",
              "      <td>0.002209</td>\n",
              "      <td>0.003466</td>\n",
              "      <td>-0.000659</td>\n",
              "      <td>0.006631</td>\n",
              "      <td>-0.002792</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.004311</td>\n",
              "      <td>0.001635</td>\n",
              "      <td>-0.002741</td>\n",
              "      <td>0.001427</td>\n",
              "      <td>0.000510</td>\n",
              "      <td>0.001291</td>\n",
              "      <td>-0.001387</td>\n",
              "      <td>-0.000220</td>\n",
              "      <td>-0.000732</td>\n",
              "      <td>0.004832</td>\n",
              "      <td>0.002559</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.000772</td>\n",
              "      <td>0.001135</td>\n",
              "      <td>-0.004822</td>\n",
              "      <td>-0.003116</td>\n",
              "      <td>-0.003839</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>-0.005013</td>\n",
              "      <td>0.002184</td>\n",
              "      <td>0.001841</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.003145</td>\n",
              "      <td>-0.000481</td>\n",
              "      <td>-0.003454</td>\n",
              "      <td>0.006174</td>\n",
              "      <td>-0.001205</td>\n",
              "      <td>0.001564</td>\n",
              "      <td>0.006991</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000575</td>\n",
              "      <td>0.006121</td>\n",
              "      <td>0.002073</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.000825</td>\n",
              "      <td>0.003202</td>\n",
              "      <td>-0.000813</td>\n",
              "      <td>0.001940</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.004172</td>\n",
              "      <td>-0.002112</td>\n",
              "      <td>-0.002466</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>0.008375</td>\n",
              "      <td>-0.008912</td>\n",
              "      <td>-0.002701</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>-0.002059</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>-0.002232</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.003438</td>\n",
              "      <td>-0.001979</td>\n",
              "      <td>-0.001678</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>-0.001450</td>\n",
              "      <td>0.002580</td>\n",
              "      <td>0.001889</td>\n",
              "      <td>0.001971</td>\n",
              "      <td>-0.000807</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>-0.002290</td>\n",
              "      <td>0.002817</td>\n",
              "      <td>0.001396</td>\n",
              "      <td>0.001168</td>\n",
              "      <td>-0.001677</td>\n",
              "      <td>-0.004175</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>-0.003111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surveying</th>\n",
              "      <td>-0.002237</td>\n",
              "      <td>-0.003556</td>\n",
              "      <td>-0.001909</td>\n",
              "      <td>0.002313</td>\n",
              "      <td>-0.003127</td>\n",
              "      <td>0.002818</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.002891</td>\n",
              "      <td>-0.002683</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.003343</td>\n",
              "      <td>-0.001305</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.004671</td>\n",
              "      <td>0.002619</td>\n",
              "      <td>-0.002138</td>\n",
              "      <td>-0.004776</td>\n",
              "      <td>0.004923</td>\n",
              "      <td>-0.005682</td>\n",
              "      <td>-0.002127</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>-0.003181</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.001985</td>\n",
              "      <td>-0.002383</td>\n",
              "      <td>-0.002957</td>\n",
              "      <td>-0.002432</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.007587</td>\n",
              "      <td>-0.000952</td>\n",
              "      <td>0.005633</td>\n",
              "      <td>-0.004787</td>\n",
              "      <td>-0.001150</td>\n",
              "      <td>0.003057</td>\n",
              "      <td>-0.002757</td>\n",
              "      <td>0.002470</td>\n",
              "      <td>0.003634</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004323</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.005658</td>\n",
              "      <td>0.002229</td>\n",
              "      <td>0.004712</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>-0.003169</td>\n",
              "      <td>-0.001122</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.000841</td>\n",
              "      <td>-0.003364</td>\n",
              "      <td>-0.004074</td>\n",
              "      <td>0.004984</td>\n",
              "      <td>0.005787</td>\n",
              "      <td>0.004336</td>\n",
              "      <td>-0.001801</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.002390</td>\n",
              "      <td>0.001936</td>\n",
              "      <td>0.002713</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>-0.003802</td>\n",
              "      <td>0.005360</td>\n",
              "      <td>0.002191</td>\n",
              "      <td>-0.001035</td>\n",
              "      <td>0.001656</td>\n",
              "      <td>0.004444</td>\n",
              "      <td>0.006549</td>\n",
              "      <td>0.004585</td>\n",
              "      <td>-0.001621</td>\n",
              "      <td>-0.002992</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>-0.001961</td>\n",
              "      <td>-0.006451</td>\n",
              "      <td>0.003487</td>\n",
              "      <td>-0.003281</td>\n",
              "      <td>0.000832</td>\n",
              "      <td>-0.004726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delegate</th>\n",
              "      <td>-0.002183</td>\n",
              "      <td>-0.003942</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>0.004059</td>\n",
              "      <td>-0.001876</td>\n",
              "      <td>-0.000664</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.001542</td>\n",
              "      <td>0.004875</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.004117</td>\n",
              "      <td>0.001107</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>-0.001961</td>\n",
              "      <td>-0.001827</td>\n",
              "      <td>-0.005223</td>\n",
              "      <td>0.002231</td>\n",
              "      <td>-0.003276</td>\n",
              "      <td>0.000627</td>\n",
              "      <td>-0.000926</td>\n",
              "      <td>0.002645</td>\n",
              "      <td>0.002570</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.003137</td>\n",
              "      <td>-0.000916</td>\n",
              "      <td>-0.000897</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.001614</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>-0.000250</td>\n",
              "      <td>-0.003023</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>-0.004691</td>\n",
              "      <td>-0.006201</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>-0.000912</td>\n",
              "      <td>-0.003799</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.002828</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>-0.001185</td>\n",
              "      <td>-0.000423</td>\n",
              "      <td>0.007692</td>\n",
              "      <td>0.001668</td>\n",
              "      <td>-0.003187</td>\n",
              "      <td>-0.000571</td>\n",
              "      <td>-0.001666</td>\n",
              "      <td>0.005220</td>\n",
              "      <td>-0.000271</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>0.002018</td>\n",
              "      <td>-0.001824</td>\n",
              "      <td>-0.000964</td>\n",
              "      <td>-0.004517</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>0.004339</td>\n",
              "      <td>-0.003619</td>\n",
              "      <td>0.002015</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.006507</td>\n",
              "      <td>0.002433</td>\n",
              "      <td>-0.000169</td>\n",
              "      <td>-0.000813</td>\n",
              "      <td>-0.004635</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>-0.003962</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>-0.001953</td>\n",
              "      <td>0.008785</td>\n",
              "      <td>-0.002308</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.004501</td>\n",
              "      <td>-0.003491</td>\n",
              "      <td>0.004541</td>\n",
              "      <td>0.006261</td>\n",
              "      <td>-0.003471</td>\n",
              "      <td>0.003130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disposition</th>\n",
              "      <td>-0.002446</td>\n",
              "      <td>-0.005139</td>\n",
              "      <td>-0.001404</td>\n",
              "      <td>0.005031</td>\n",
              "      <td>-0.000252</td>\n",
              "      <td>-0.001577</td>\n",
              "      <td>0.003590</td>\n",
              "      <td>-0.002520</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>-0.001208</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>-0.000292</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>-0.000130</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>-0.005485</td>\n",
              "      <td>0.001576</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>-0.001513</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>-0.000582</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>-0.002699</td>\n",
              "      <td>0.002222</td>\n",
              "      <td>-0.000448</td>\n",
              "      <td>-0.003579</td>\n",
              "      <td>-0.000110</td>\n",
              "      <td>-0.001872</td>\n",
              "      <td>-0.005344</td>\n",
              "      <td>-0.002966</td>\n",
              "      <td>-0.000202</td>\n",
              "      <td>0.003634</td>\n",
              "      <td>-0.002087</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>-0.001847</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>-0.001389</td>\n",
              "      <td>-0.002175</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004331</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>-0.001917</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>-0.003447</td>\n",
              "      <td>-0.000107</td>\n",
              "      <td>-0.003782</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>0.006459</td>\n",
              "      <td>-0.002586</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.003348</td>\n",
              "      <td>-0.000322</td>\n",
              "      <td>0.000619</td>\n",
              "      <td>0.005558</td>\n",
              "      <td>0.000790</td>\n",
              "      <td>0.003691</td>\n",
              "      <td>-0.001753</td>\n",
              "      <td>0.002355</td>\n",
              "      <td>-0.008901</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.002358</td>\n",
              "      <td>0.004275</td>\n",
              "      <td>-0.005897</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>-0.004431</td>\n",
              "      <td>-0.000995</td>\n",
              "      <td>0.007260</td>\n",
              "      <td>-0.001042</td>\n",
              "      <td>-0.001048</td>\n",
              "      <td>-0.003780</td>\n",
              "      <td>-0.002661</td>\n",
              "      <td>-0.002370</td>\n",
              "      <td>0.002068</td>\n",
              "      <td>-0.001463</td>\n",
              "      <td>0.002608</td>\n",
              "      <td>0.003039</td>\n",
              "      <td>0.003339</td>\n",
              "      <td>-0.006716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allele</th>\n",
              "      <td>-0.002496</td>\n",
              "      <td>-0.005792</td>\n",
              "      <td>-0.003963</td>\n",
              "      <td>0.006249</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>-0.001977</td>\n",
              "      <td>0.000616</td>\n",
              "      <td>0.002887</td>\n",
              "      <td>-0.001941</td>\n",
              "      <td>0.002145</td>\n",
              "      <td>0.001080</td>\n",
              "      <td>-0.000540</td>\n",
              "      <td>-0.002033</td>\n",
              "      <td>-0.003839</td>\n",
              "      <td>0.007484</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.002573</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.001514</td>\n",
              "      <td>-0.001417</td>\n",
              "      <td>0.002617</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>-0.003354</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>-0.001774</td>\n",
              "      <td>-0.001317</td>\n",
              "      <td>-0.002842</td>\n",
              "      <td>-0.002194</td>\n",
              "      <td>-0.000549</td>\n",
              "      <td>0.000954</td>\n",
              "      <td>-0.003289</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>-0.001605</td>\n",
              "      <td>0.003409</td>\n",
              "      <td>0.001268</td>\n",
              "      <td>0.002212</td>\n",
              "      <td>0.002145</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.006586</td>\n",
              "      <td>0.004149</td>\n",
              "      <td>-0.000936</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>-0.007738</td>\n",
              "      <td>-0.004191</td>\n",
              "      <td>-0.004676</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>-0.003766</td>\n",
              "      <td>-0.002358</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.004778</td>\n",
              "      <td>0.003596</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.002426</td>\n",
              "      <td>-0.009280</td>\n",
              "      <td>-0.000636</td>\n",
              "      <td>0.003213</td>\n",
              "      <td>0.001350</td>\n",
              "      <td>0.001872</td>\n",
              "      <td>-0.000369</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>-0.005381</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.001275</td>\n",
              "      <td>-0.003918</td>\n",
              "      <td>-0.000924</td>\n",
              "      <td>-0.000160</td>\n",
              "      <td>0.004097</td>\n",
              "      <td>0.000627</td>\n",
              "      <td>-0.003520</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>-0.004086</td>\n",
              "      <td>-0.000550</td>\n",
              "      <td>-0.007412</td>\n",
              "      <td>-0.007037</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.003369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2   ...        97        98        99\n",
              "the         -0.086917  0.164314 -0.000176  ... -0.009822  0.003166 -0.003512\n",
              "of          -0.081659  0.154576  0.001236  ... -0.005113 -0.001534 -0.005187\n",
              "and         -0.081173  0.153082  0.005892  ... -0.012582  0.008044  0.000216\n",
              "in          -0.076822  0.140813  0.009037  ... -0.004655  0.006175 -0.005724\n",
              "to          -0.075542  0.140740 -0.005253  ...  0.001767 -0.007641  0.004338\n",
              "...               ...       ...       ...  ...       ...       ...       ...\n",
              "drafting    -0.002095 -0.003536 -0.000396  ... -0.004175  0.000495 -0.003111\n",
              "surveying   -0.002237 -0.003556 -0.001909  ... -0.003281  0.000832 -0.004726\n",
              "delegate    -0.002183 -0.003942  0.002654  ...  0.006261 -0.003471  0.003130\n",
              "disposition -0.002446 -0.005139 -0.001404  ...  0.003039  0.003339 -0.006716\n",
              "allele      -0.002496 -0.005792 -0.003963  ... -0.007037  0.001082  0.003369\n",
              "\n",
              "[14000 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzVeC482UnlN"
      },
      "source": [
        "#### 3. Evaluation of alpha (0.1) transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R1UWkXwUnlN"
      },
      "source": [
        "import csv   \n",
        "fields=['14000','100']\n",
        "with open(r'14000-100-alpha01.vec', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=' ')\n",
        "    writer.writerow(fields)\n",
        "V_df.to_csv(r'14000-100-alpha01-SVD.vec', header=None, index=column_vocab[:14000], sep=' ', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZU4M8yOUnlO",
        "outputId": "f97852d2-10c9-4a96-e67e-7a14bc0f3e31"
      },
      "source": [
        "!python2 evaluate.py -m 14000-100-alpha01-SVD.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '14000-100-alpha01.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.67663\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.56281\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.63267\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.55504\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.42469\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.27437\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.64631\n",
            "\n",
            "19500/7777/19544: Add 0.23299, Mul 0.20381\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.23303, Mul Score: 0.20356\n",
            "8000/3876/8000: Add 0.27632, Mul 0.21672\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.27632, Mul Score: 0.21672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5D3Al_1Uudk"
      },
      "source": [
        "### B. Transformation puissance alpha = 0.4 en utilisant SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcYnhbzjUudk"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XtXxnTPUudk"
      },
      "source": [
        "def power_of_alpha(matrix, alpha):\n",
        "  \n",
        "  new_matrix = matrix.to_numpy()\n",
        "  new_matrix = np.power(matrix, alpha)\n",
        "\n",
        "  return new_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3dYlKm9Uudk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "alph_mat = power_of_alpha(matrice_cooccurence, 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph4qTwa0Uudl"
      },
      "source": [
        "#### 2. SVD on Co-occurence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3pcE02aUudl",
        "outputId": "4f5deaec-2880-4bd6-cc16-3831ffe290e4"
      },
      "source": [
        "from numpy import array\n",
        "from scipy.linalg import svd\n",
        "from numpy import diag\n",
        "from numpy import dot\n",
        "\n",
        "# define a matrix\n",
        "A = array(alph_mat)\n",
        "print('Matrix A is: \\n')\n",
        "print(A)\n",
        "# SVD\n",
        "U, s, VT = svd(A)\n",
        "print('*'*120)\n",
        "print('Matrix U is: \\n')\n",
        "print(U)\n",
        "print('*'*120)\n",
        "Sigma = diag(s)\n",
        "print('Matrix Sigma is: \\n')\n",
        "print(Sigma)\n",
        "print('*'*120)\n",
        "print('Matrix VT is: \\n')\n",
        "print(VT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A is: \n",
            "\n",
            "[[108.01430984 177.96979858 119.8854755  ...   2.60949864   0.\n",
            "    3.1058435 ]\n",
            " [161.31661398  63.80973222  71.28914496 ...   1.31950791   1.\n",
            "    1.31950791]\n",
            " [ 95.69949183  84.53966408  52.81620688 ...   2.40822469   1.55184557\n",
            "    2.29739671]\n",
            " ...\n",
            " [  0.           0.           2.04767251 ...   0.           0.\n",
            "    0.        ]\n",
            " [  0.           0.           0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [  0.           0.           0.         ...   0.           0.\n",
            "    0.        ]]\n",
            "************************************************************************************************************************\n",
            "Matrix U is: \n",
            "\n",
            "[[-2.56797828e-01 -2.06034351e-01 -7.47838264e-02 ... -1.61261458e-03\n",
            "  -5.08721789e-04 -9.42233316e-04]\n",
            " [-1.92471733e-01 -1.91767200e-01 -7.98902153e-02 ...  6.19338158e-05\n",
            "   9.41575839e-05 -2.54216030e-04]\n",
            " [-1.83351233e-01 -2.38122674e-01 -1.59855598e-01 ...  1.21380024e-03\n",
            "  -1.70955851e-04  5.55299355e-05]\n",
            " ...\n",
            " [-4.51616930e-04  1.17727839e-03  6.31202780e-04 ...  1.50888752e-03\n",
            "   1.48017306e-02  1.02248899e-02]\n",
            " [-2.38070837e-04  9.53718730e-05 -1.47357157e-03 ...  3.52773784e-02\n",
            "   1.06428913e-02  1.95950345e-02]\n",
            " [-2.37723024e-04  1.32822258e-04 -8.79505853e-04 ...  6.30260799e-03\n",
            "   6.65591983e-03  6.05065251e-03]]\n",
            "************************************************************************************************************************\n",
            "Matrix Sigma is: \n",
            "\n",
            "[[3.70852446e+03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.22080436e+03 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 6.43846462e+02 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.35468808e-03\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  7.31136755e-04 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 6.32773721e-05]]\n",
            "************************************************************************************************************************\n",
            "Matrix VT is: \n",
            "\n",
            "[[-2.48048217e-01 -1.94641233e-01 -1.82941870e-01 ... -1.32171327e-03\n",
            "  -6.73200816e-04 -1.44004587e-03]\n",
            " [ 3.65964928e-01  2.51691012e-01  2.25295462e-01 ... -2.82081561e-03\n",
            "  -1.18550085e-03 -3.35589158e-03]\n",
            " [-1.54180527e-01 -1.14704270e-01 -1.09295924e-02 ... -2.04352203e-03\n",
            "  -1.11645006e-03 -1.12269567e-03]\n",
            " ...\n",
            " [ 1.60641267e-04  1.76623670e-04 -1.10593419e-03 ... -1.53515119e-02\n",
            "  -1.45444636e-02 -1.95701082e-03]\n",
            " [ 4.56574212e-04  6.64794338e-05 -3.44294598e-04 ...  7.22172010e-03\n",
            "   9.28202290e-03 -2.76469150e-03]\n",
            " [ 1.36053300e-04  4.03538987e-04  4.41483558e-04 ... -1.67381232e-02\n",
            "   4.09729444e-03  1.42160330e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgyt4n91Uudm"
      },
      "source": [
        "k = 100\n",
        "U_100=U[:,:k]\n",
        "Sigma_100=Sigma[:k,:k]\n",
        "VT_100=VT[:k,:14000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_vzvGTOUudm"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "V_100 = VT_100.transpose()\n",
        "V_df=pd.DataFrame(V_100,index=column_vocab[:14000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "kIeEcRDSUudm",
        "outputId": "d6af45bf-1f2f-4480-f6fe-47d6c9988e3d"
      },
      "source": [
        "V_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.248048</td>\n",
              "      <td>0.365965</td>\n",
              "      <td>-0.154181</td>\n",
              "      <td>0.155160</td>\n",
              "      <td>-0.118076</td>\n",
              "      <td>0.058265</td>\n",
              "      <td>-0.195005</td>\n",
              "      <td>0.331130</td>\n",
              "      <td>-0.158638</td>\n",
              "      <td>0.233484</td>\n",
              "      <td>-0.054463</td>\n",
              "      <td>0.319458</td>\n",
              "      <td>-0.084112</td>\n",
              "      <td>0.119928</td>\n",
              "      <td>-0.097428</td>\n",
              "      <td>0.001573</td>\n",
              "      <td>-0.035665</td>\n",
              "      <td>0.050819</td>\n",
              "      <td>-0.019634</td>\n",
              "      <td>0.016438</td>\n",
              "      <td>-0.354913</td>\n",
              "      <td>0.150656</td>\n",
              "      <td>-0.023767</td>\n",
              "      <td>-0.062495</td>\n",
              "      <td>-0.025517</td>\n",
              "      <td>0.001802</td>\n",
              "      <td>-0.128819</td>\n",
              "      <td>0.152403</td>\n",
              "      <td>-0.110490</td>\n",
              "      <td>0.153042</td>\n",
              "      <td>-0.093514</td>\n",
              "      <td>0.009884</td>\n",
              "      <td>-0.014696</td>\n",
              "      <td>0.025049</td>\n",
              "      <td>-0.037876</td>\n",
              "      <td>0.008180</td>\n",
              "      <td>-0.050478</td>\n",
              "      <td>0.100173</td>\n",
              "      <td>-0.088684</td>\n",
              "      <td>0.055766</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.013362</td>\n",
              "      <td>0.027763</td>\n",
              "      <td>-0.088357</td>\n",
              "      <td>0.036434</td>\n",
              "      <td>-0.042227</td>\n",
              "      <td>0.004518</td>\n",
              "      <td>-0.027016</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>-0.003717</td>\n",
              "      <td>0.021339</td>\n",
              "      <td>-0.044999</td>\n",
              "      <td>0.028035</td>\n",
              "      <td>-0.001107</td>\n",
              "      <td>0.021051</td>\n",
              "      <td>-0.030490</td>\n",
              "      <td>0.020526</td>\n",
              "      <td>-0.009320</td>\n",
              "      <td>0.007408</td>\n",
              "      <td>0.008541</td>\n",
              "      <td>-0.009509</td>\n",
              "      <td>-0.020609</td>\n",
              "      <td>0.009402</td>\n",
              "      <td>-0.020660</td>\n",
              "      <td>0.006073</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>-0.008757</td>\n",
              "      <td>-0.003698</td>\n",
              "      <td>0.025272</td>\n",
              "      <td>-0.006209</td>\n",
              "      <td>-0.016038</td>\n",
              "      <td>-0.001321</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>0.016937</td>\n",
              "      <td>-0.023313</td>\n",
              "      <td>-0.028602</td>\n",
              "      <td>0.020781</td>\n",
              "      <td>0.023454</td>\n",
              "      <td>-0.018286</td>\n",
              "      <td>-0.013211</td>\n",
              "      <td>-0.020255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.194641</td>\n",
              "      <td>0.251691</td>\n",
              "      <td>-0.114704</td>\n",
              "      <td>0.125702</td>\n",
              "      <td>0.125312</td>\n",
              "      <td>0.019420</td>\n",
              "      <td>-0.167772</td>\n",
              "      <td>-0.222629</td>\n",
              "      <td>-0.014047</td>\n",
              "      <td>-0.009036</td>\n",
              "      <td>0.046960</td>\n",
              "      <td>-0.099584</td>\n",
              "      <td>0.088699</td>\n",
              "      <td>0.068943</td>\n",
              "      <td>0.019902</td>\n",
              "      <td>0.039063</td>\n",
              "      <td>0.032836</td>\n",
              "      <td>0.032151</td>\n",
              "      <td>0.055627</td>\n",
              "      <td>-0.019236</td>\n",
              "      <td>0.096920</td>\n",
              "      <td>-0.058777</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>0.083322</td>\n",
              "      <td>0.045021</td>\n",
              "      <td>-0.119885</td>\n",
              "      <td>-0.157317</td>\n",
              "      <td>0.070021</td>\n",
              "      <td>-0.010873</td>\n",
              "      <td>-0.118933</td>\n",
              "      <td>-0.149968</td>\n",
              "      <td>0.006055</td>\n",
              "      <td>0.079576</td>\n",
              "      <td>-0.135901</td>\n",
              "      <td>-0.042122</td>\n",
              "      <td>0.162930</td>\n",
              "      <td>0.006957</td>\n",
              "      <td>-0.016346</td>\n",
              "      <td>0.082072</td>\n",
              "      <td>-0.010507</td>\n",
              "      <td>...</td>\n",
              "      <td>0.096460</td>\n",
              "      <td>0.061895</td>\n",
              "      <td>-0.068380</td>\n",
              "      <td>-0.064116</td>\n",
              "      <td>0.034324</td>\n",
              "      <td>-0.010493</td>\n",
              "      <td>-0.075696</td>\n",
              "      <td>-0.132525</td>\n",
              "      <td>-0.042247</td>\n",
              "      <td>-0.144652</td>\n",
              "      <td>0.003477</td>\n",
              "      <td>-0.041724</td>\n",
              "      <td>-0.014697</td>\n",
              "      <td>0.010567</td>\n",
              "      <td>-0.063535</td>\n",
              "      <td>-0.033909</td>\n",
              "      <td>-0.107756</td>\n",
              "      <td>-0.140224</td>\n",
              "      <td>0.016843</td>\n",
              "      <td>0.022287</td>\n",
              "      <td>0.081082</td>\n",
              "      <td>-0.011735</td>\n",
              "      <td>0.057780</td>\n",
              "      <td>-0.034793</td>\n",
              "      <td>0.012732</td>\n",
              "      <td>-0.149660</td>\n",
              "      <td>-0.036381</td>\n",
              "      <td>-0.078621</td>\n",
              "      <td>-0.028254</td>\n",
              "      <td>0.014994</td>\n",
              "      <td>0.073083</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>-0.046418</td>\n",
              "      <td>-0.037761</td>\n",
              "      <td>0.132278</td>\n",
              "      <td>-0.009919</td>\n",
              "      <td>-0.094544</td>\n",
              "      <td>0.016436</td>\n",
              "      <td>0.004505</td>\n",
              "      <td>-0.055943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.182942</td>\n",
              "      <td>0.225295</td>\n",
              "      <td>-0.010930</td>\n",
              "      <td>-0.071004</td>\n",
              "      <td>0.037488</td>\n",
              "      <td>0.111367</td>\n",
              "      <td>-0.108659</td>\n",
              "      <td>-0.229262</td>\n",
              "      <td>0.028028</td>\n",
              "      <td>-0.098760</td>\n",
              "      <td>-0.081688</td>\n",
              "      <td>0.034800</td>\n",
              "      <td>0.147330</td>\n",
              "      <td>-0.006418</td>\n",
              "      <td>-0.030158</td>\n",
              "      <td>-0.076462</td>\n",
              "      <td>-0.034521</td>\n",
              "      <td>-0.021277</td>\n",
              "      <td>-0.012679</td>\n",
              "      <td>-0.086560</td>\n",
              "      <td>-0.050959</td>\n",
              "      <td>-0.138628</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>-0.167150</td>\n",
              "      <td>-0.199484</td>\n",
              "      <td>-0.131882</td>\n",
              "      <td>0.175123</td>\n",
              "      <td>0.023727</td>\n",
              "      <td>0.052299</td>\n",
              "      <td>0.024044</td>\n",
              "      <td>0.036618</td>\n",
              "      <td>-0.120776</td>\n",
              "      <td>0.014374</td>\n",
              "      <td>0.034898</td>\n",
              "      <td>-0.015143</td>\n",
              "      <td>-0.106821</td>\n",
              "      <td>0.028511</td>\n",
              "      <td>0.017725</td>\n",
              "      <td>-0.051275</td>\n",
              "      <td>0.016364</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038302</td>\n",
              "      <td>0.082846</td>\n",
              "      <td>-0.023319</td>\n",
              "      <td>0.039963</td>\n",
              "      <td>0.075574</td>\n",
              "      <td>0.036046</td>\n",
              "      <td>0.064754</td>\n",
              "      <td>0.074102</td>\n",
              "      <td>0.038303</td>\n",
              "      <td>0.025720</td>\n",
              "      <td>0.067656</td>\n",
              "      <td>-0.038598</td>\n",
              "      <td>0.132216</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.034332</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.111897</td>\n",
              "      <td>0.070689</td>\n",
              "      <td>-0.011007</td>\n",
              "      <td>-0.104489</td>\n",
              "      <td>0.043134</td>\n",
              "      <td>-0.047421</td>\n",
              "      <td>0.034803</td>\n",
              "      <td>-0.022028</td>\n",
              "      <td>0.080309</td>\n",
              "      <td>0.012114</td>\n",
              "      <td>-0.021592</td>\n",
              "      <td>0.012893</td>\n",
              "      <td>0.053262</td>\n",
              "      <td>-0.039156</td>\n",
              "      <td>-0.108834</td>\n",
              "      <td>0.036127</td>\n",
              "      <td>-0.010824</td>\n",
              "      <td>0.051844</td>\n",
              "      <td>-0.084814</td>\n",
              "      <td>0.016813</td>\n",
              "      <td>0.029266</td>\n",
              "      <td>0.003329</td>\n",
              "      <td>0.031929</td>\n",
              "      <td>-0.026995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.165656</td>\n",
              "      <td>0.170582</td>\n",
              "      <td>-0.136067</td>\n",
              "      <td>0.047240</td>\n",
              "      <td>0.102345</td>\n",
              "      <td>-0.096186</td>\n",
              "      <td>-0.154215</td>\n",
              "      <td>-0.065198</td>\n",
              "      <td>-0.033381</td>\n",
              "      <td>-0.112914</td>\n",
              "      <td>0.065683</td>\n",
              "      <td>-0.239898</td>\n",
              "      <td>-0.059034</td>\n",
              "      <td>0.123138</td>\n",
              "      <td>-0.074921</td>\n",
              "      <td>0.037917</td>\n",
              "      <td>-0.045855</td>\n",
              "      <td>-0.013429</td>\n",
              "      <td>-0.074520</td>\n",
              "      <td>0.105065</td>\n",
              "      <td>0.081852</td>\n",
              "      <td>-0.015917</td>\n",
              "      <td>-0.113404</td>\n",
              "      <td>0.235307</td>\n",
              "      <td>-0.145948</td>\n",
              "      <td>0.034612</td>\n",
              "      <td>0.165371</td>\n",
              "      <td>-0.058758</td>\n",
              "      <td>-0.122601</td>\n",
              "      <td>0.016361</td>\n",
              "      <td>-0.022088</td>\n",
              "      <td>-0.243359</td>\n",
              "      <td>0.080323</td>\n",
              "      <td>0.181439</td>\n",
              "      <td>0.187512</td>\n",
              "      <td>0.091210</td>\n",
              "      <td>-0.016848</td>\n",
              "      <td>0.091445</td>\n",
              "      <td>0.090084</td>\n",
              "      <td>0.096519</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.099632</td>\n",
              "      <td>-0.073506</td>\n",
              "      <td>0.123910</td>\n",
              "      <td>-0.031656</td>\n",
              "      <td>-0.033736</td>\n",
              "      <td>0.061686</td>\n",
              "      <td>0.017925</td>\n",
              "      <td>0.038313</td>\n",
              "      <td>0.020234</td>\n",
              "      <td>0.001565</td>\n",
              "      <td>0.021883</td>\n",
              "      <td>0.072786</td>\n",
              "      <td>-0.096150</td>\n",
              "      <td>0.027973</td>\n",
              "      <td>0.053134</td>\n",
              "      <td>-0.025385</td>\n",
              "      <td>0.134782</td>\n",
              "      <td>0.057825</td>\n",
              "      <td>-0.062096</td>\n",
              "      <td>-0.048881</td>\n",
              "      <td>-0.052671</td>\n",
              "      <td>0.065704</td>\n",
              "      <td>-0.012106</td>\n",
              "      <td>0.028650</td>\n",
              "      <td>-0.015972</td>\n",
              "      <td>-0.019899</td>\n",
              "      <td>-0.054876</td>\n",
              "      <td>0.006431</td>\n",
              "      <td>-0.058244</td>\n",
              "      <td>0.042761</td>\n",
              "      <td>0.015849</td>\n",
              "      <td>-0.016343</td>\n",
              "      <td>0.005385</td>\n",
              "      <td>-0.063406</td>\n",
              "      <td>-0.000094</td>\n",
              "      <td>0.062654</td>\n",
              "      <td>0.085569</td>\n",
              "      <td>-0.003486</td>\n",
              "      <td>0.036354</td>\n",
              "      <td>0.028384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.148392</td>\n",
              "      <td>0.171562</td>\n",
              "      <td>0.083040</td>\n",
              "      <td>-0.011006</td>\n",
              "      <td>0.207975</td>\n",
              "      <td>-0.038944</td>\n",
              "      <td>0.011658</td>\n",
              "      <td>-0.152992</td>\n",
              "      <td>-0.198912</td>\n",
              "      <td>-0.171916</td>\n",
              "      <td>-0.160202</td>\n",
              "      <td>0.129288</td>\n",
              "      <td>-0.219137</td>\n",
              "      <td>-0.062995</td>\n",
              "      <td>-0.231174</td>\n",
              "      <td>-0.051499</td>\n",
              "      <td>-0.014520</td>\n",
              "      <td>-0.018728</td>\n",
              "      <td>-0.043937</td>\n",
              "      <td>0.044367</td>\n",
              "      <td>0.238895</td>\n",
              "      <td>0.154947</td>\n",
              "      <td>0.078430</td>\n",
              "      <td>-0.115353</td>\n",
              "      <td>0.055858</td>\n",
              "      <td>-0.071064</td>\n",
              "      <td>0.007473</td>\n",
              "      <td>-0.036579</td>\n",
              "      <td>-0.009605</td>\n",
              "      <td>0.032422</td>\n",
              "      <td>-0.028490</td>\n",
              "      <td>0.038576</td>\n",
              "      <td>-0.014086</td>\n",
              "      <td>-0.042460</td>\n",
              "      <td>0.039779</td>\n",
              "      <td>-0.131115</td>\n",
              "      <td>0.037873</td>\n",
              "      <td>0.010301</td>\n",
              "      <td>-0.030574</td>\n",
              "      <td>-0.080257</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.086388</td>\n",
              "      <td>-0.016522</td>\n",
              "      <td>0.083667</td>\n",
              "      <td>0.114116</td>\n",
              "      <td>-0.033255</td>\n",
              "      <td>0.051083</td>\n",
              "      <td>-0.051297</td>\n",
              "      <td>-0.004612</td>\n",
              "      <td>-0.012948</td>\n",
              "      <td>0.037658</td>\n",
              "      <td>0.032813</td>\n",
              "      <td>-0.106847</td>\n",
              "      <td>0.033977</td>\n",
              "      <td>-0.068803</td>\n",
              "      <td>-0.087186</td>\n",
              "      <td>-0.043219</td>\n",
              "      <td>-0.093044</td>\n",
              "      <td>0.102198</td>\n",
              "      <td>0.117674</td>\n",
              "      <td>0.042317</td>\n",
              "      <td>0.021457</td>\n",
              "      <td>0.014585</td>\n",
              "      <td>-0.048604</td>\n",
              "      <td>0.045097</td>\n",
              "      <td>-0.041281</td>\n",
              "      <td>-0.009582</td>\n",
              "      <td>-0.000404</td>\n",
              "      <td>0.063334</td>\n",
              "      <td>0.034601</td>\n",
              "      <td>0.029422</td>\n",
              "      <td>0.104459</td>\n",
              "      <td>-0.060354</td>\n",
              "      <td>0.021741</td>\n",
              "      <td>0.009847</td>\n",
              "      <td>0.016579</td>\n",
              "      <td>-0.003762</td>\n",
              "      <td>-0.086041</td>\n",
              "      <td>-0.004970</td>\n",
              "      <td>-0.027499</td>\n",
              "      <td>-0.014699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drafting</th>\n",
              "      <td>-0.001838</td>\n",
              "      <td>-0.003358</td>\n",
              "      <td>-0.001966</td>\n",
              "      <td>0.002594</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>0.001043</td>\n",
              "      <td>-0.000815</td>\n",
              "      <td>0.001723</td>\n",
              "      <td>-0.000475</td>\n",
              "      <td>0.005444</td>\n",
              "      <td>-0.004456</td>\n",
              "      <td>-0.000886</td>\n",
              "      <td>0.003435</td>\n",
              "      <td>-0.002087</td>\n",
              "      <td>-0.007791</td>\n",
              "      <td>0.002487</td>\n",
              "      <td>0.004078</td>\n",
              "      <td>-0.000862</td>\n",
              "      <td>-0.003021</td>\n",
              "      <td>-0.002545</td>\n",
              "      <td>0.001279</td>\n",
              "      <td>-0.000166</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.002341</td>\n",
              "      <td>-0.002075</td>\n",
              "      <td>-0.000444</td>\n",
              "      <td>-0.002455</td>\n",
              "      <td>-0.002242</td>\n",
              "      <td>-0.000838</td>\n",
              "      <td>0.001896</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>-0.000452</td>\n",
              "      <td>-0.003116</td>\n",
              "      <td>0.004431</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.003710</td>\n",
              "      <td>0.004404</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.001621</td>\n",
              "      <td>0.000747</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>-0.000351</td>\n",
              "      <td>0.004708</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>-0.000980</td>\n",
              "      <td>0.000838</td>\n",
              "      <td>0.001777</td>\n",
              "      <td>-0.002093</td>\n",
              "      <td>-0.001022</td>\n",
              "      <td>0.002322</td>\n",
              "      <td>0.007126</td>\n",
              "      <td>0.003205</td>\n",
              "      <td>0.005862</td>\n",
              "      <td>-0.001596</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>-0.003163</td>\n",
              "      <td>-0.003316</td>\n",
              "      <td>-0.002671</td>\n",
              "      <td>0.000793</td>\n",
              "      <td>0.001893</td>\n",
              "      <td>-0.000281</td>\n",
              "      <td>-0.003161</td>\n",
              "      <td>0.001849</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>-0.003848</td>\n",
              "      <td>-0.004714</td>\n",
              "      <td>0.004391</td>\n",
              "      <td>-0.005398</td>\n",
              "      <td>0.005245</td>\n",
              "      <td>0.001345</td>\n",
              "      <td>0.002037</td>\n",
              "      <td>-0.000256</td>\n",
              "      <td>-0.000534</td>\n",
              "      <td>-0.002774</td>\n",
              "      <td>-0.001437</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.008981</td>\n",
              "      <td>-0.000879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surveying</th>\n",
              "      <td>-0.001911</td>\n",
              "      <td>-0.003502</td>\n",
              "      <td>-0.001009</td>\n",
              "      <td>0.002566</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>-0.001759</td>\n",
              "      <td>-0.000295</td>\n",
              "      <td>0.000956</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>-0.002792</td>\n",
              "      <td>0.000904</td>\n",
              "      <td>0.002549</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>-0.004375</td>\n",
              "      <td>-0.000408</td>\n",
              "      <td>0.003822</td>\n",
              "      <td>-0.004219</td>\n",
              "      <td>-0.003901</td>\n",
              "      <td>0.001222</td>\n",
              "      <td>0.004024</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>-0.001043</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.000308</td>\n",
              "      <td>-0.001887</td>\n",
              "      <td>0.001543</td>\n",
              "      <td>-0.005761</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.002357</td>\n",
              "      <td>-0.002453</td>\n",
              "      <td>0.006064</td>\n",
              "      <td>0.003357</td>\n",
              "      <td>0.001516</td>\n",
              "      <td>-0.003579</td>\n",
              "      <td>0.001674</td>\n",
              "      <td>-0.001181</td>\n",
              "      <td>-0.002888</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.004160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004911</td>\n",
              "      <td>-0.000480</td>\n",
              "      <td>-0.007788</td>\n",
              "      <td>-0.006166</td>\n",
              "      <td>0.000564</td>\n",
              "      <td>-0.003589</td>\n",
              "      <td>-0.002417</td>\n",
              "      <td>0.000862</td>\n",
              "      <td>-0.004463</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>0.003328</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.004697</td>\n",
              "      <td>0.004708</td>\n",
              "      <td>0.006758</td>\n",
              "      <td>0.002503</td>\n",
              "      <td>0.001404</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>-0.000322</td>\n",
              "      <td>-0.000314</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>-0.001908</td>\n",
              "      <td>-0.003157</td>\n",
              "      <td>0.002747</td>\n",
              "      <td>0.003674</td>\n",
              "      <td>0.006266</td>\n",
              "      <td>-0.004713</td>\n",
              "      <td>-0.004309</td>\n",
              "      <td>0.000565</td>\n",
              "      <td>-0.003103</td>\n",
              "      <td>0.002359</td>\n",
              "      <td>0.004963</td>\n",
              "      <td>0.002073</td>\n",
              "      <td>-0.002711</td>\n",
              "      <td>0.003166</td>\n",
              "      <td>-0.003339</td>\n",
              "      <td>-0.003354</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>0.005511</td>\n",
              "      <td>0.005364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delegate</th>\n",
              "      <td>-0.001958</td>\n",
              "      <td>-0.003883</td>\n",
              "      <td>-0.002090</td>\n",
              "      <td>-0.000164</td>\n",
              "      <td>0.005057</td>\n",
              "      <td>-0.000899</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-0.000624</td>\n",
              "      <td>-0.001648</td>\n",
              "      <td>0.003969</td>\n",
              "      <td>-0.004905</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.002368</td>\n",
              "      <td>-0.001070</td>\n",
              "      <td>-0.003859</td>\n",
              "      <td>0.002813</td>\n",
              "      <td>0.005108</td>\n",
              "      <td>-0.000393</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.006011</td>\n",
              "      <td>-0.006063</td>\n",
              "      <td>-0.006502</td>\n",
              "      <td>-0.002705</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>-0.001969</td>\n",
              "      <td>0.001819</td>\n",
              "      <td>-0.002574</td>\n",
              "      <td>-0.000314</td>\n",
              "      <td>-0.001321</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>0.002010</td>\n",
              "      <td>-0.000486</td>\n",
              "      <td>-0.001956</td>\n",
              "      <td>-0.002552</td>\n",
              "      <td>0.001398</td>\n",
              "      <td>0.002324</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>-0.004720</td>\n",
              "      <td>0.000826</td>\n",
              "      <td>-0.000387</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005097</td>\n",
              "      <td>-0.001023</td>\n",
              "      <td>-0.001376</td>\n",
              "      <td>-0.004312</td>\n",
              "      <td>-0.001686</td>\n",
              "      <td>-0.006846</td>\n",
              "      <td>0.006328</td>\n",
              "      <td>-0.003444</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>-0.003291</td>\n",
              "      <td>0.004634</td>\n",
              "      <td>-0.001282</td>\n",
              "      <td>-0.002115</td>\n",
              "      <td>-0.002438</td>\n",
              "      <td>-0.003616</td>\n",
              "      <td>-0.001480</td>\n",
              "      <td>-0.000416</td>\n",
              "      <td>0.001311</td>\n",
              "      <td>-0.003561</td>\n",
              "      <td>-0.002539</td>\n",
              "      <td>-0.003175</td>\n",
              "      <td>0.000470</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>-0.001647</td>\n",
              "      <td>0.000688</td>\n",
              "      <td>-0.001501</td>\n",
              "      <td>0.000822</td>\n",
              "      <td>-0.001728</td>\n",
              "      <td>-0.000774</td>\n",
              "      <td>-0.002838</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.004477</td>\n",
              "      <td>-0.003108</td>\n",
              "      <td>-0.001198</td>\n",
              "      <td>-0.000292</td>\n",
              "      <td>-0.001390</td>\n",
              "      <td>-0.006348</td>\n",
              "      <td>0.005027</td>\n",
              "      <td>0.004568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disposition</th>\n",
              "      <td>-0.002121</td>\n",
              "      <td>-0.004489</td>\n",
              "      <td>-0.000319</td>\n",
              "      <td>0.002073</td>\n",
              "      <td>0.003154</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>-0.000930</td>\n",
              "      <td>-0.000874</td>\n",
              "      <td>-0.003768</td>\n",
              "      <td>0.005160</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.001669</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>-0.002322</td>\n",
              "      <td>-0.006536</td>\n",
              "      <td>-0.001962</td>\n",
              "      <td>0.001705</td>\n",
              "      <td>-0.001687</td>\n",
              "      <td>-0.001020</td>\n",
              "      <td>-0.001743</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>-0.004721</td>\n",
              "      <td>-0.002400</td>\n",
              "      <td>0.003388</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.001930</td>\n",
              "      <td>-0.002580</td>\n",
              "      <td>0.000692</td>\n",
              "      <td>0.003140</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>-0.003299</td>\n",
              "      <td>-0.001674</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>-0.002863</td>\n",
              "      <td>0.001015</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.000557</td>\n",
              "      <td>-0.002181</td>\n",
              "      <td>0.001427</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003209</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>-0.005867</td>\n",
              "      <td>-0.000196</td>\n",
              "      <td>0.001176</td>\n",
              "      <td>-0.002768</td>\n",
              "      <td>0.002775</td>\n",
              "      <td>-0.003579</td>\n",
              "      <td>0.002629</td>\n",
              "      <td>-0.000755</td>\n",
              "      <td>-0.001552</td>\n",
              "      <td>-0.000888</td>\n",
              "      <td>-0.001237</td>\n",
              "      <td>-0.002490</td>\n",
              "      <td>-0.000950</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.002716</td>\n",
              "      <td>0.007813</td>\n",
              "      <td>-0.004670</td>\n",
              "      <td>-0.002445</td>\n",
              "      <td>0.002341</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>-0.001638</td>\n",
              "      <td>0.002742</td>\n",
              "      <td>-0.001600</td>\n",
              "      <td>0.003969</td>\n",
              "      <td>-0.000847</td>\n",
              "      <td>-0.001395</td>\n",
              "      <td>-0.001749</td>\n",
              "      <td>0.003812</td>\n",
              "      <td>-0.000715</td>\n",
              "      <td>-0.002390</td>\n",
              "      <td>0.003486</td>\n",
              "      <td>-0.002048</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.005419</td>\n",
              "      <td>0.001447</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>-0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allele</th>\n",
              "      <td>-0.002036</td>\n",
              "      <td>-0.004579</td>\n",
              "      <td>0.002661</td>\n",
              "      <td>0.002024</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>0.003821</td>\n",
              "      <td>-0.005170</td>\n",
              "      <td>-0.001176</td>\n",
              "      <td>-0.002883</td>\n",
              "      <td>-0.000564</td>\n",
              "      <td>-0.000400</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>-0.000305</td>\n",
              "      <td>0.001056</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.005339</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>-0.001637</td>\n",
              "      <td>-0.002312</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>-0.000655</td>\n",
              "      <td>-0.001825</td>\n",
              "      <td>-0.002178</td>\n",
              "      <td>-0.001423</td>\n",
              "      <td>-0.003316</td>\n",
              "      <td>0.003270</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.004364</td>\n",
              "      <td>-0.003275</td>\n",
              "      <td>0.001252</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>-0.001721</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.000775</td>\n",
              "      <td>0.003525</td>\n",
              "      <td>-0.001141</td>\n",
              "      <td>-0.001175</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002458</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.001502</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>-0.000343</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.004899</td>\n",
              "      <td>-0.001948</td>\n",
              "      <td>0.001991</td>\n",
              "      <td>0.001742</td>\n",
              "      <td>-0.000697</td>\n",
              "      <td>-0.002708</td>\n",
              "      <td>-0.002832</td>\n",
              "      <td>0.003118</td>\n",
              "      <td>-0.002017</td>\n",
              "      <td>0.002354</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>-0.000901</td>\n",
              "      <td>-0.000333</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.002336</td>\n",
              "      <td>0.000819</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>-0.001700</td>\n",
              "      <td>0.005209</td>\n",
              "      <td>-0.000710</td>\n",
              "      <td>0.001862</td>\n",
              "      <td>-0.005646</td>\n",
              "      <td>0.002308</td>\n",
              "      <td>-0.003023</td>\n",
              "      <td>-0.003106</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.001740</td>\n",
              "      <td>-0.000381</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>-0.001282</td>\n",
              "      <td>0.003228</td>\n",
              "      <td>-0.003549</td>\n",
              "      <td>-0.000590</td>\n",
              "      <td>0.000123</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2   ...        97        98        99\n",
              "the         -0.248048  0.365965 -0.154181  ... -0.018286 -0.013211 -0.020255\n",
              "of          -0.194641  0.251691 -0.114704  ...  0.016436  0.004505 -0.055943\n",
              "and         -0.182942  0.225295 -0.010930  ...  0.003329  0.031929 -0.026995\n",
              "in          -0.165656  0.170582 -0.136067  ... -0.003486  0.036354  0.028384\n",
              "to          -0.148392  0.171562  0.083040  ... -0.004970 -0.027499 -0.014699\n",
              "...               ...       ...       ...  ...       ...       ...       ...\n",
              "drafting    -0.001838 -0.003358 -0.001966  ...  0.000664  0.008981 -0.000879\n",
              "surveying   -0.001911 -0.003502 -0.001009  ...  0.000817  0.005511  0.005364\n",
              "delegate    -0.001958 -0.003883 -0.002090  ... -0.006348  0.005027  0.004568\n",
              "disposition -0.002121 -0.004489 -0.000319  ...  0.001447  0.001973 -0.000595\n",
              "allele      -0.002036 -0.004579  0.002661  ... -0.003549 -0.000590  0.000123\n",
              "\n",
              "[14000 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiaR7TYjUudn"
      },
      "source": [
        "#### 3. Evaluation of alpha (0.4) transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nms9zh5uUudn"
      },
      "source": [
        "import csv   \n",
        "fields=['14000','100']\n",
        "with open(r'14000-100-alpha04.vec', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=' ')\n",
        "    writer.writerow(fields)\n",
        "V_df.to_csv(r'14000-100-alpha04-SVD.vec', header=None, index=column_vocab[:14000], sep=' ', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9xRW8luUudn",
        "outputId": "814e492f-968b-46c2-9542-3bac72a5294d"
      },
      "source": [
        "!python2 evaluate.py -m 14000-100-alpha04-SVD.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '14000-100-alpha04.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.68256\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.57182\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.63637\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.53946\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.43600\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.25321\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.43645\n",
            "\n",
            "19500/7777/19544: Add 0.29793, Mul 0.27864\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.29759, Mul Score: 0.27799\n",
            "8000/3876/8000: Add 0.32869, Mul 0.29334\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.32869, Mul Score: 0.29334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFnM8uLPanLW"
      },
      "source": [
        "### C. Transformation log en utilisant SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbS0UfX8UNo_"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoQ651zI50UC"
      },
      "source": [
        "def log_transformation(matrix):\n",
        "  from tqdm import tqdm\n",
        "  from math import log\n",
        "  tqdm.pandas()\n",
        "\n",
        "  new_matrix = matrix\n",
        "  for column in matrix.columns:\n",
        "    new_matrix[column] = matrix[column].apply(lambda x:log(x) if x!=0 else 0)\n",
        "  \n",
        "  return new_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WhohjlZ7bYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebb09dd-4a5d-4362-b1b7-c6ea20bdc6d0"
      },
      "source": [
        "log_mat = log_transformation(matrice_cooccurence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVXm8eFbUNpC"
      },
      "source": [
        "#### 2. SVD on Co-occurence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_17L_ih_ETs",
        "outputId": "c92533b6-d9b6-4e67-8870-d608dc15ae71"
      },
      "source": [
        "from numpy import array\n",
        "from scipy.linalg import svd\n",
        "from numpy import diag\n",
        "from numpy import dot\n",
        "\n",
        "# define a matrix\n",
        "A = array(log_mat)\n",
        "print('Matrix A is: \\n')\n",
        "print(A)\n",
        "# SVD\n",
        "U, s, VT = svd(A)\n",
        "print('*'*120)\n",
        "print('Matrix U is: \\n')\n",
        "print(U)\n",
        "print('*'*120)\n",
        "Sigma = diag(s)\n",
        "print('Matrix Sigma is: \\n')\n",
        "print(Sigma)\n",
        "print('*'*120)\n",
        "print('Matrix VT is: \\n')\n",
        "print(VT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A is: \n",
            "\n",
            "[[11.70565929 12.95403466 11.96634229 ...  2.39789527  0.\n",
            "   2.83321334]\n",
            " [12.70842245 10.3897643  10.66686018 ...  0.69314718  0.\n",
            "   0.69314718]\n",
            " [11.40303247 11.09305205  9.91704523 ...  2.19722458  1.09861229\n",
            "   2.07944154]\n",
            " ...\n",
            " [ 0.          0.          1.79175947 ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n",
            "************************************************************************************************************************\n",
            "Matrix U is: \n",
            "\n",
            "[[-1.97252530e-01 -2.59602096e-01 -2.62702329e-01 ... -9.10822806e-06\n",
            "  -4.80418189e-06  3.13048959e-06]\n",
            " [-1.67371360e-01 -2.06381197e-01 -1.73842541e-01 ...  1.81085980e-05\n",
            "  -4.76468212e-06  8.66378230e-06]\n",
            " [-1.76075109e-01 -2.34089221e-01 -2.45693912e-01 ... -2.24471609e-05\n",
            "  -8.85032810e-06 -4.18706456e-05]\n",
            " ...\n",
            " [-4.35696126e-04  8.44417476e-04 -1.06106331e-03 ... -7.80592321e-03\n",
            "   3.14012891e-03 -2.55112456e-03]\n",
            " [-2.95822755e-04 -7.12548384e-05  9.01204603e-04 ... -7.23478122e-03\n",
            "   1.00816574e-03  7.51925371e-03]\n",
            " [-3.15877081e-04  6.04062736e-05  5.18024993e-04 ...  1.60989038e-03\n",
            "   2.27723374e-03 -1.42094156e-03]]\n",
            "************************************************************************************************************************\n",
            "Matrix Sigma is: \n",
            "\n",
            "[[2.15183241e+03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.24359474e+03 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 5.95331874e+02 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 9.76200058e-05\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  9.27152752e-05 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 3.22677357e-05]]\n",
            "************************************************************************************************************************\n",
            "Matrix VT is: \n",
            "\n",
            "[[-1.82969176e-01 -1.62194180e-01 -1.59514348e-01 ... -1.21888101e-03\n",
            "  -3.69615006e-04 -1.21659666e-03]\n",
            " [ 3.05032051e-01  2.61045187e-01  2.53849028e-01 ... -2.15942292e-03\n",
            "  -4.66829699e-04 -2.34842913e-03]\n",
            " [-3.03396550e-01 -2.60340697e-01 -2.30622671e-01 ... -3.30476281e-03\n",
            "  -4.92098960e-04 -3.75017342e-03]\n",
            " ...\n",
            " [ 8.97160065e-06  1.25926470e-05 -4.70969609e-05 ...  6.76373489e-03\n",
            "  -1.32394165e-05  7.97678711e-04]\n",
            " [ 3.75930882e-05 -3.15013534e-05  2.82824230e-06 ...  9.94334573e-04\n",
            "   4.07401930e-04  4.48289732e-04]\n",
            " [ 4.46578497e-05 -3.56227902e-05  2.75920754e-05 ... -1.89706867e-02\n",
            "  -3.41221670e-04  1.64530214e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBEpK1iu_ETt"
      },
      "source": [
        "k = 100\n",
        "U_100=U[:,:k]\n",
        "Sigma_100=Sigma[:k,:k]\n",
        "VT_100=VT[:k,:14000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YLGjHfR_ETu"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "V_100 = VT_100.transpose()\n",
        "V_df=pd.DataFrame(V_100,index=column_vocab[:14000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "onzWtBzE_ETu",
        "outputId": "e6d6356c-807f-4d74-a2ba-c03bb64868af"
      },
      "source": [
        "V_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.182969</td>\n",
              "      <td>0.305032</td>\n",
              "      <td>-0.303397</td>\n",
              "      <td>-0.029938</td>\n",
              "      <td>0.033628</td>\n",
              "      <td>-0.186410</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>-0.037372</td>\n",
              "      <td>-0.022856</td>\n",
              "      <td>0.034474</td>\n",
              "      <td>-0.033411</td>\n",
              "      <td>0.136223</td>\n",
              "      <td>-0.044504</td>\n",
              "      <td>0.133866</td>\n",
              "      <td>-0.045453</td>\n",
              "      <td>0.143410</td>\n",
              "      <td>-0.183180</td>\n",
              "      <td>0.168125</td>\n",
              "      <td>-0.044610</td>\n",
              "      <td>0.076475</td>\n",
              "      <td>-0.096757</td>\n",
              "      <td>0.116768</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>-0.107781</td>\n",
              "      <td>0.013417</td>\n",
              "      <td>-0.033076</td>\n",
              "      <td>0.004923</td>\n",
              "      <td>-0.034416</td>\n",
              "      <td>0.074755</td>\n",
              "      <td>-0.041700</td>\n",
              "      <td>0.123790</td>\n",
              "      <td>-0.084518</td>\n",
              "      <td>0.073827</td>\n",
              "      <td>-0.091358</td>\n",
              "      <td>0.096117</td>\n",
              "      <td>-0.096501</td>\n",
              "      <td>0.037094</td>\n",
              "      <td>-0.045835</td>\n",
              "      <td>0.096744</td>\n",
              "      <td>-0.004173</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.054574</td>\n",
              "      <td>0.018764</td>\n",
              "      <td>-0.043353</td>\n",
              "      <td>0.052053</td>\n",
              "      <td>-0.012469</td>\n",
              "      <td>0.024164</td>\n",
              "      <td>-0.004583</td>\n",
              "      <td>0.051438</td>\n",
              "      <td>-0.167576</td>\n",
              "      <td>0.070535</td>\n",
              "      <td>-0.025442</td>\n",
              "      <td>0.033448</td>\n",
              "      <td>-0.053859</td>\n",
              "      <td>0.079156</td>\n",
              "      <td>-0.131923</td>\n",
              "      <td>0.050824</td>\n",
              "      <td>-0.016594</td>\n",
              "      <td>-0.097777</td>\n",
              "      <td>0.040538</td>\n",
              "      <td>-0.008275</td>\n",
              "      <td>-0.016022</td>\n",
              "      <td>-0.001210</td>\n",
              "      <td>0.117854</td>\n",
              "      <td>-0.083996</td>\n",
              "      <td>0.068935</td>\n",
              "      <td>-0.067506</td>\n",
              "      <td>-0.007070</td>\n",
              "      <td>-0.088946</td>\n",
              "      <td>0.117907</td>\n",
              "      <td>-0.054671</td>\n",
              "      <td>-0.010807</td>\n",
              "      <td>-0.054319</td>\n",
              "      <td>0.137466</td>\n",
              "      <td>0.033023</td>\n",
              "      <td>-0.086021</td>\n",
              "      <td>-0.045199</td>\n",
              "      <td>0.059548</td>\n",
              "      <td>0.014523</td>\n",
              "      <td>0.092768</td>\n",
              "      <td>-0.035407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.162194</td>\n",
              "      <td>0.261045</td>\n",
              "      <td>-0.260341</td>\n",
              "      <td>-0.060107</td>\n",
              "      <td>0.056903</td>\n",
              "      <td>-0.168827</td>\n",
              "      <td>0.005533</td>\n",
              "      <td>-0.030173</td>\n",
              "      <td>0.097253</td>\n",
              "      <td>-0.003054</td>\n",
              "      <td>0.003888</td>\n",
              "      <td>0.012419</td>\n",
              "      <td>-0.065333</td>\n",
              "      <td>-0.034123</td>\n",
              "      <td>0.038856</td>\n",
              "      <td>0.027345</td>\n",
              "      <td>-0.089827</td>\n",
              "      <td>0.047311</td>\n",
              "      <td>-0.020434</td>\n",
              "      <td>0.029049</td>\n",
              "      <td>-0.038618</td>\n",
              "      <td>-0.000975</td>\n",
              "      <td>0.027252</td>\n",
              "      <td>-0.035370</td>\n",
              "      <td>-0.089744</td>\n",
              "      <td>0.023397</td>\n",
              "      <td>0.002040</td>\n",
              "      <td>0.073537</td>\n",
              "      <td>0.010208</td>\n",
              "      <td>-0.025791</td>\n",
              "      <td>0.059868</td>\n",
              "      <td>-0.063017</td>\n",
              "      <td>0.016550</td>\n",
              "      <td>0.108105</td>\n",
              "      <td>0.061742</td>\n",
              "      <td>0.074856</td>\n",
              "      <td>0.011210</td>\n",
              "      <td>-0.009462</td>\n",
              "      <td>0.038205</td>\n",
              "      <td>-0.012724</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.054911</td>\n",
              "      <td>-0.061402</td>\n",
              "      <td>-0.034449</td>\n",
              "      <td>0.063349</td>\n",
              "      <td>-0.041062</td>\n",
              "      <td>0.087537</td>\n",
              "      <td>-0.070551</td>\n",
              "      <td>0.017891</td>\n",
              "      <td>0.019898</td>\n",
              "      <td>-0.020397</td>\n",
              "      <td>-0.008239</td>\n",
              "      <td>0.008884</td>\n",
              "      <td>0.086254</td>\n",
              "      <td>0.062400</td>\n",
              "      <td>0.037302</td>\n",
              "      <td>-0.054260</td>\n",
              "      <td>0.030609</td>\n",
              "      <td>0.057529</td>\n",
              "      <td>0.082433</td>\n",
              "      <td>-0.049746</td>\n",
              "      <td>-0.073688</td>\n",
              "      <td>0.048505</td>\n",
              "      <td>-0.089301</td>\n",
              "      <td>0.185824</td>\n",
              "      <td>-0.025483</td>\n",
              "      <td>0.063720</td>\n",
              "      <td>0.034705</td>\n",
              "      <td>0.212158</td>\n",
              "      <td>-0.110021</td>\n",
              "      <td>0.109757</td>\n",
              "      <td>0.056266</td>\n",
              "      <td>0.104030</td>\n",
              "      <td>-0.070303</td>\n",
              "      <td>-0.020833</td>\n",
              "      <td>0.118621</td>\n",
              "      <td>0.036886</td>\n",
              "      <td>-0.017472</td>\n",
              "      <td>0.039377</td>\n",
              "      <td>-0.175436</td>\n",
              "      <td>0.051111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.159514</td>\n",
              "      <td>0.253849</td>\n",
              "      <td>-0.230623</td>\n",
              "      <td>0.016435</td>\n",
              "      <td>-0.038982</td>\n",
              "      <td>-0.162202</td>\n",
              "      <td>0.061909</td>\n",
              "      <td>0.003995</td>\n",
              "      <td>0.117099</td>\n",
              "      <td>0.054127</td>\n",
              "      <td>0.050227</td>\n",
              "      <td>0.083561</td>\n",
              "      <td>0.075693</td>\n",
              "      <td>-0.055764</td>\n",
              "      <td>0.002507</td>\n",
              "      <td>0.001733</td>\n",
              "      <td>-0.067964</td>\n",
              "      <td>0.135951</td>\n",
              "      <td>-0.002363</td>\n",
              "      <td>0.065358</td>\n",
              "      <td>0.089695</td>\n",
              "      <td>-0.001501</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.001138</td>\n",
              "      <td>-0.056041</td>\n",
              "      <td>-0.067005</td>\n",
              "      <td>-0.040351</td>\n",
              "      <td>0.004161</td>\n",
              "      <td>-0.042778</td>\n",
              "      <td>0.103459</td>\n",
              "      <td>-0.005347</td>\n",
              "      <td>-0.029063</td>\n",
              "      <td>0.044736</td>\n",
              "      <td>-0.051024</td>\n",
              "      <td>-0.029586</td>\n",
              "      <td>0.006025</td>\n",
              "      <td>-0.103554</td>\n",
              "      <td>0.043685</td>\n",
              "      <td>-0.014882</td>\n",
              "      <td>-0.076584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.099153</td>\n",
              "      <td>0.085507</td>\n",
              "      <td>0.058203</td>\n",
              "      <td>-0.003454</td>\n",
              "      <td>-0.060836</td>\n",
              "      <td>-0.032334</td>\n",
              "      <td>-0.091841</td>\n",
              "      <td>-0.028023</td>\n",
              "      <td>0.207676</td>\n",
              "      <td>-0.165432</td>\n",
              "      <td>-0.119559</td>\n",
              "      <td>0.061139</td>\n",
              "      <td>-0.039593</td>\n",
              "      <td>-0.194289</td>\n",
              "      <td>0.117116</td>\n",
              "      <td>-0.001482</td>\n",
              "      <td>0.044411</td>\n",
              "      <td>0.017960</td>\n",
              "      <td>-0.090479</td>\n",
              "      <td>0.059354</td>\n",
              "      <td>0.014856</td>\n",
              "      <td>-0.007367</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.031604</td>\n",
              "      <td>0.058356</td>\n",
              "      <td>0.019125</td>\n",
              "      <td>0.044190</td>\n",
              "      <td>0.036572</td>\n",
              "      <td>-0.048284</td>\n",
              "      <td>0.040927</td>\n",
              "      <td>0.001388</td>\n",
              "      <td>-0.016674</td>\n",
              "      <td>-0.033230</td>\n",
              "      <td>-0.031459</td>\n",
              "      <td>-0.017136</td>\n",
              "      <td>0.006316</td>\n",
              "      <td>-0.027556</td>\n",
              "      <td>-0.095235</td>\n",
              "      <td>-0.052022</td>\n",
              "      <td>0.014766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.141507</td>\n",
              "      <td>0.207338</td>\n",
              "      <td>-0.164639</td>\n",
              "      <td>-0.071939</td>\n",
              "      <td>0.004971</td>\n",
              "      <td>-0.109011</td>\n",
              "      <td>-0.016336</td>\n",
              "      <td>-0.058153</td>\n",
              "      <td>-0.006040</td>\n",
              "      <td>-0.040836</td>\n",
              "      <td>-0.016791</td>\n",
              "      <td>-0.022752</td>\n",
              "      <td>-0.120004</td>\n",
              "      <td>-0.077750</td>\n",
              "      <td>-0.063519</td>\n",
              "      <td>-0.090747</td>\n",
              "      <td>-0.038545</td>\n",
              "      <td>-0.002992</td>\n",
              "      <td>-0.013272</td>\n",
              "      <td>0.025002</td>\n",
              "      <td>-0.040448</td>\n",
              "      <td>-0.024698</td>\n",
              "      <td>0.111021</td>\n",
              "      <td>-0.047624</td>\n",
              "      <td>0.050038</td>\n",
              "      <td>0.111021</td>\n",
              "      <td>0.256051</td>\n",
              "      <td>0.081820</td>\n",
              "      <td>0.087017</td>\n",
              "      <td>-0.009435</td>\n",
              "      <td>-0.006892</td>\n",
              "      <td>0.065314</td>\n",
              "      <td>0.039476</td>\n",
              "      <td>0.270663</td>\n",
              "      <td>-0.131176</td>\n",
              "      <td>0.039252</td>\n",
              "      <td>0.107058</td>\n",
              "      <td>-0.041247</td>\n",
              "      <td>-0.051469</td>\n",
              "      <td>0.228800</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.043641</td>\n",
              "      <td>0.010636</td>\n",
              "      <td>0.066082</td>\n",
              "      <td>-0.101101</td>\n",
              "      <td>0.099274</td>\n",
              "      <td>0.100175</td>\n",
              "      <td>0.062907</td>\n",
              "      <td>-0.078814</td>\n",
              "      <td>-0.071413</td>\n",
              "      <td>0.062989</td>\n",
              "      <td>0.129524</td>\n",
              "      <td>0.015254</td>\n",
              "      <td>0.040010</td>\n",
              "      <td>-0.044052</td>\n",
              "      <td>0.120373</td>\n",
              "      <td>-0.056561</td>\n",
              "      <td>0.033696</td>\n",
              "      <td>0.016744</td>\n",
              "      <td>0.020101</td>\n",
              "      <td>0.133916</td>\n",
              "      <td>0.091865</td>\n",
              "      <td>-0.018322</td>\n",
              "      <td>0.049334</td>\n",
              "      <td>-0.073069</td>\n",
              "      <td>-0.147333</td>\n",
              "      <td>-0.074435</td>\n",
              "      <td>-0.079255</td>\n",
              "      <td>-0.087645</td>\n",
              "      <td>0.098322</td>\n",
              "      <td>-0.066728</td>\n",
              "      <td>-0.089356</td>\n",
              "      <td>-0.058323</td>\n",
              "      <td>-0.073160</td>\n",
              "      <td>-0.000854</td>\n",
              "      <td>-0.025871</td>\n",
              "      <td>-0.079183</td>\n",
              "      <td>-0.037443</td>\n",
              "      <td>0.040028</td>\n",
              "      <td>0.101373</td>\n",
              "      <td>-0.012606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.136067</td>\n",
              "      <td>0.200339</td>\n",
              "      <td>-0.141380</td>\n",
              "      <td>0.048576</td>\n",
              "      <td>-0.005934</td>\n",
              "      <td>-0.078806</td>\n",
              "      <td>-0.045355</td>\n",
              "      <td>-0.021198</td>\n",
              "      <td>0.031690</td>\n",
              "      <td>0.107166</td>\n",
              "      <td>-0.042288</td>\n",
              "      <td>-0.105807</td>\n",
              "      <td>0.134338</td>\n",
              "      <td>-0.051445</td>\n",
              "      <td>-0.208211</td>\n",
              "      <td>0.043698</td>\n",
              "      <td>0.094797</td>\n",
              "      <td>-0.116025</td>\n",
              "      <td>-0.099529</td>\n",
              "      <td>0.014347</td>\n",
              "      <td>0.212284</td>\n",
              "      <td>0.067549</td>\n",
              "      <td>0.060252</td>\n",
              "      <td>-0.048908</td>\n",
              "      <td>0.119527</td>\n",
              "      <td>-0.016848</td>\n",
              "      <td>-0.072996</td>\n",
              "      <td>0.047807</td>\n",
              "      <td>-0.011488</td>\n",
              "      <td>-0.001631</td>\n",
              "      <td>-0.065936</td>\n",
              "      <td>0.028626</td>\n",
              "      <td>-0.125175</td>\n",
              "      <td>-0.043793</td>\n",
              "      <td>-0.059904</td>\n",
              "      <td>-0.026230</td>\n",
              "      <td>0.035091</td>\n",
              "      <td>0.064750</td>\n",
              "      <td>-0.081671</td>\n",
              "      <td>0.014188</td>\n",
              "      <td>...</td>\n",
              "      <td>0.092338</td>\n",
              "      <td>-0.059460</td>\n",
              "      <td>-0.085158</td>\n",
              "      <td>-0.050947</td>\n",
              "      <td>0.197957</td>\n",
              "      <td>-0.276221</td>\n",
              "      <td>0.004350</td>\n",
              "      <td>-0.044548</td>\n",
              "      <td>0.016308</td>\n",
              "      <td>0.095521</td>\n",
              "      <td>-0.000368</td>\n",
              "      <td>-0.147345</td>\n",
              "      <td>-0.003472</td>\n",
              "      <td>-0.176108</td>\n",
              "      <td>-0.110512</td>\n",
              "      <td>-0.084501</td>\n",
              "      <td>-0.006156</td>\n",
              "      <td>0.108765</td>\n",
              "      <td>-0.084590</td>\n",
              "      <td>-0.076507</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>-0.066990</td>\n",
              "      <td>-0.050055</td>\n",
              "      <td>-0.110011</td>\n",
              "      <td>-0.016062</td>\n",
              "      <td>0.036813</td>\n",
              "      <td>0.079714</td>\n",
              "      <td>-0.127918</td>\n",
              "      <td>-0.094843</td>\n",
              "      <td>-0.045428</td>\n",
              "      <td>0.037830</td>\n",
              "      <td>0.086353</td>\n",
              "      <td>-0.041847</td>\n",
              "      <td>-0.043140</td>\n",
              "      <td>0.045008</td>\n",
              "      <td>0.019840</td>\n",
              "      <td>-0.037292</td>\n",
              "      <td>0.061506</td>\n",
              "      <td>0.051903</td>\n",
              "      <td>-0.015061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drafting</th>\n",
              "      <td>-0.001632</td>\n",
              "      <td>-0.002922</td>\n",
              "      <td>-0.004573</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>-0.001052</td>\n",
              "      <td>0.006246</td>\n",
              "      <td>-0.000666</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.000769</td>\n",
              "      <td>0.001970</td>\n",
              "      <td>-0.000903</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>-0.001906</td>\n",
              "      <td>-0.001990</td>\n",
              "      <td>-0.002184</td>\n",
              "      <td>-0.003823</td>\n",
              "      <td>-0.004766</td>\n",
              "      <td>0.001136</td>\n",
              "      <td>-0.001413</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>-0.003602</td>\n",
              "      <td>0.004015</td>\n",
              "      <td>0.003195</td>\n",
              "      <td>-0.003485</td>\n",
              "      <td>-0.004395</td>\n",
              "      <td>-0.002569</td>\n",
              "      <td>-0.003081</td>\n",
              "      <td>-0.003184</td>\n",
              "      <td>-0.007211</td>\n",
              "      <td>0.001865</td>\n",
              "      <td>-0.004512</td>\n",
              "      <td>-0.002257</td>\n",
              "      <td>-0.001787</td>\n",
              "      <td>-0.000258</td>\n",
              "      <td>0.002156</td>\n",
              "      <td>0.002291</td>\n",
              "      <td>0.005010</td>\n",
              "      <td>-0.006338</td>\n",
              "      <td>-0.000750</td>\n",
              "      <td>-0.000261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003202</td>\n",
              "      <td>0.002426</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.005741</td>\n",
              "      <td>-0.000768</td>\n",
              "      <td>-0.004021</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.006727</td>\n",
              "      <td>-0.003554</td>\n",
              "      <td>0.005076</td>\n",
              "      <td>-0.001223</td>\n",
              "      <td>0.002029</td>\n",
              "      <td>-0.001305</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.001983</td>\n",
              "      <td>0.002006</td>\n",
              "      <td>-0.001808</td>\n",
              "      <td>0.001569</td>\n",
              "      <td>-0.002311</td>\n",
              "      <td>-0.002215</td>\n",
              "      <td>-0.004833</td>\n",
              "      <td>-0.000697</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>-0.006019</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>0.006984</td>\n",
              "      <td>0.003475</td>\n",
              "      <td>-0.004854</td>\n",
              "      <td>0.006063</td>\n",
              "      <td>0.001444</td>\n",
              "      <td>-0.002364</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>-0.001318</td>\n",
              "      <td>-0.001252</td>\n",
              "      <td>-0.000460</td>\n",
              "      <td>-0.003422</td>\n",
              "      <td>0.001445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surveying</th>\n",
              "      <td>-0.001714</td>\n",
              "      <td>-0.003224</td>\n",
              "      <td>-0.005310</td>\n",
              "      <td>0.001615</td>\n",
              "      <td>-0.001466</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>-0.000916</td>\n",
              "      <td>-0.000545</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>-0.002443</td>\n",
              "      <td>-0.002951</td>\n",
              "      <td>-0.001788</td>\n",
              "      <td>-0.002393</td>\n",
              "      <td>-0.001681</td>\n",
              "      <td>-0.001124</td>\n",
              "      <td>-0.001175</td>\n",
              "      <td>0.001731</td>\n",
              "      <td>-0.000618</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>-0.001884</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.003620</td>\n",
              "      <td>-0.004428</td>\n",
              "      <td>-0.006623</td>\n",
              "      <td>-0.001856</td>\n",
              "      <td>-0.001969</td>\n",
              "      <td>-0.002976</td>\n",
              "      <td>-0.000477</td>\n",
              "      <td>0.002376</td>\n",
              "      <td>-0.001668</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>-0.002662</td>\n",
              "      <td>-0.001972</td>\n",
              "      <td>0.001360</td>\n",
              "      <td>-0.002023</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000216</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>-0.003201</td>\n",
              "      <td>0.000444</td>\n",
              "      <td>-0.000745</td>\n",
              "      <td>-0.003576</td>\n",
              "      <td>-0.002681</td>\n",
              "      <td>0.002522</td>\n",
              "      <td>-0.002958</td>\n",
              "      <td>-0.003389</td>\n",
              "      <td>-0.000549</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.000733</td>\n",
              "      <td>-0.000552</td>\n",
              "      <td>0.002327</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>-0.000381</td>\n",
              "      <td>0.002763</td>\n",
              "      <td>0.000706</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>-0.001168</td>\n",
              "      <td>-0.000470</td>\n",
              "      <td>-0.000167</td>\n",
              "      <td>-0.000287</td>\n",
              "      <td>-0.002057</td>\n",
              "      <td>-0.001292</td>\n",
              "      <td>-0.002064</td>\n",
              "      <td>0.003254</td>\n",
              "      <td>-0.004257</td>\n",
              "      <td>0.000444</td>\n",
              "      <td>0.004147</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.001135</td>\n",
              "      <td>-0.000268</td>\n",
              "      <td>-0.000469</td>\n",
              "      <td>-0.003239</td>\n",
              "      <td>0.001791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delegate</th>\n",
              "      <td>-0.001816</td>\n",
              "      <td>-0.003351</td>\n",
              "      <td>-0.004978</td>\n",
              "      <td>0.001535</td>\n",
              "      <td>-0.002126</td>\n",
              "      <td>0.005166</td>\n",
              "      <td>-0.002931</td>\n",
              "      <td>-0.001916</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.002838</td>\n",
              "      <td>-0.001557</td>\n",
              "      <td>-0.000692</td>\n",
              "      <td>-0.000274</td>\n",
              "      <td>-0.001909</td>\n",
              "      <td>-0.001310</td>\n",
              "      <td>-0.000754</td>\n",
              "      <td>-0.001575</td>\n",
              "      <td>-0.000911</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>-0.005155</td>\n",
              "      <td>0.003071</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.002478</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.008458</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>-0.005032</td>\n",
              "      <td>-0.004592</td>\n",
              "      <td>0.006417</td>\n",
              "      <td>-0.002369</td>\n",
              "      <td>-0.003364</td>\n",
              "      <td>0.001530</td>\n",
              "      <td>-0.000397</td>\n",
              "      <td>0.008563</td>\n",
              "      <td>0.007963</td>\n",
              "      <td>0.004114</td>\n",
              "      <td>-0.006216</td>\n",
              "      <td>-0.000420</td>\n",
              "      <td>-0.000952</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004346</td>\n",
              "      <td>0.006216</td>\n",
              "      <td>0.001559</td>\n",
              "      <td>-0.001706</td>\n",
              "      <td>0.007773</td>\n",
              "      <td>0.002013</td>\n",
              "      <td>-0.001566</td>\n",
              "      <td>-0.001026</td>\n",
              "      <td>0.004483</td>\n",
              "      <td>0.000948</td>\n",
              "      <td>-0.002411</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.004039</td>\n",
              "      <td>0.002178</td>\n",
              "      <td>-0.004009</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>-0.002778</td>\n",
              "      <td>0.003873</td>\n",
              "      <td>-0.001799</td>\n",
              "      <td>0.003927</td>\n",
              "      <td>0.001557</td>\n",
              "      <td>-0.001044</td>\n",
              "      <td>0.002219</td>\n",
              "      <td>0.002309</td>\n",
              "      <td>-0.002398</td>\n",
              "      <td>-0.001288</td>\n",
              "      <td>-0.001864</td>\n",
              "      <td>-0.001712</td>\n",
              "      <td>-0.000222</td>\n",
              "      <td>-0.002552</td>\n",
              "      <td>-0.002724</td>\n",
              "      <td>-0.000520</td>\n",
              "      <td>0.000783</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>-0.006113</td>\n",
              "      <td>-0.000808</td>\n",
              "      <td>-0.000960</td>\n",
              "      <td>0.002315</td>\n",
              "      <td>0.004557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disposition</th>\n",
              "      <td>-0.002001</td>\n",
              "      <td>-0.003742</td>\n",
              "      <td>-0.005100</td>\n",
              "      <td>0.001959</td>\n",
              "      <td>-0.001362</td>\n",
              "      <td>0.004080</td>\n",
              "      <td>-0.001678</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>-0.001496</td>\n",
              "      <td>-0.002542</td>\n",
              "      <td>-0.002335</td>\n",
              "      <td>-0.000604</td>\n",
              "      <td>-0.000271</td>\n",
              "      <td>-0.001105</td>\n",
              "      <td>-0.001750</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>-0.001908</td>\n",
              "      <td>-0.000683</td>\n",
              "      <td>-0.003196</td>\n",
              "      <td>0.002835</td>\n",
              "      <td>0.006657</td>\n",
              "      <td>-0.000530</td>\n",
              "      <td>-0.001257</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>-0.000658</td>\n",
              "      <td>-0.006988</td>\n",
              "      <td>-0.008609</td>\n",
              "      <td>0.003632</td>\n",
              "      <td>-0.001783</td>\n",
              "      <td>-0.001706</td>\n",
              "      <td>-0.000810</td>\n",
              "      <td>-0.001903</td>\n",
              "      <td>0.000701</td>\n",
              "      <td>0.003748</td>\n",
              "      <td>0.002386</td>\n",
              "      <td>-0.005974</td>\n",
              "      <td>-0.001357</td>\n",
              "      <td>0.001575</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006442</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>-0.000094</td>\n",
              "      <td>-0.002345</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.003245</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>-0.003620</td>\n",
              "      <td>-0.002101</td>\n",
              "      <td>0.003623</td>\n",
              "      <td>0.005433</td>\n",
              "      <td>0.001580</td>\n",
              "      <td>-0.000339</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.002784</td>\n",
              "      <td>0.002772</td>\n",
              "      <td>-0.005994</td>\n",
              "      <td>-0.000373</td>\n",
              "      <td>-0.002632</td>\n",
              "      <td>0.002331</td>\n",
              "      <td>-0.003030</td>\n",
              "      <td>-0.002218</td>\n",
              "      <td>-0.001754</td>\n",
              "      <td>-0.003791</td>\n",
              "      <td>-0.001608</td>\n",
              "      <td>0.003599</td>\n",
              "      <td>-0.007165</td>\n",
              "      <td>0.002658</td>\n",
              "      <td>0.003384</td>\n",
              "      <td>-0.000521</td>\n",
              "      <td>-0.001990</td>\n",
              "      <td>-0.001252</td>\n",
              "      <td>0.001138</td>\n",
              "      <td>0.002383</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allele</th>\n",
              "      <td>-0.001997</td>\n",
              "      <td>-0.003414</td>\n",
              "      <td>-0.004029</td>\n",
              "      <td>0.002534</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>0.003213</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.002522</td>\n",
              "      <td>-0.001738</td>\n",
              "      <td>-0.002421</td>\n",
              "      <td>-0.004090</td>\n",
              "      <td>0.001021</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>-0.004658</td>\n",
              "      <td>-0.002949</td>\n",
              "      <td>0.001827</td>\n",
              "      <td>-0.005079</td>\n",
              "      <td>-0.003154</td>\n",
              "      <td>0.003390</td>\n",
              "      <td>-0.005092</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>-0.002549</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>0.003078</td>\n",
              "      <td>0.001370</td>\n",
              "      <td>-0.006563</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.002693</td>\n",
              "      <td>0.002161</td>\n",
              "      <td>-0.000174</td>\n",
              "      <td>0.003691</td>\n",
              "      <td>-0.002755</td>\n",
              "      <td>-0.000482</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>-0.006607</td>\n",
              "      <td>-0.001559</td>\n",
              "      <td>-0.006978</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.003816</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.007515</td>\n",
              "      <td>-0.003124</td>\n",
              "      <td>-0.002111</td>\n",
              "      <td>-0.002760</td>\n",
              "      <td>-0.005153</td>\n",
              "      <td>-0.003974</td>\n",
              "      <td>0.002294</td>\n",
              "      <td>0.002668</td>\n",
              "      <td>-0.006379</td>\n",
              "      <td>0.001964</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.002863</td>\n",
              "      <td>-0.000843</td>\n",
              "      <td>0.002121</td>\n",
              "      <td>-0.002605</td>\n",
              "      <td>0.005065</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>-0.002422</td>\n",
              "      <td>0.001043</td>\n",
              "      <td>0.000910</td>\n",
              "      <td>0.003070</td>\n",
              "      <td>0.005016</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.003726</td>\n",
              "      <td>-0.000307</td>\n",
              "      <td>-0.003706</td>\n",
              "      <td>-0.001162</td>\n",
              "      <td>0.001521</td>\n",
              "      <td>0.002177</td>\n",
              "      <td>-0.001104</td>\n",
              "      <td>0.004039</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2   ...        97        98        99\n",
              "the         -0.182969  0.305032 -0.303397  ...  0.014523  0.092768 -0.035407\n",
              "of          -0.162194  0.261045 -0.260341  ...  0.039377 -0.175436  0.051111\n",
              "and         -0.159514  0.253849 -0.230623  ... -0.095235 -0.052022  0.014766\n",
              "in          -0.141507  0.207338 -0.164639  ...  0.040028  0.101373 -0.012606\n",
              "to          -0.136067  0.200339 -0.141380  ...  0.061506  0.051903 -0.015061\n",
              "...               ...       ...       ...  ...       ...       ...       ...\n",
              "drafting    -0.001632 -0.002922 -0.004573  ... -0.000460 -0.003422  0.001445\n",
              "surveying   -0.001714 -0.003224 -0.005310  ... -0.000469 -0.003239  0.001791\n",
              "delegate    -0.001816 -0.003351 -0.004978  ... -0.000960  0.002315  0.004557\n",
              "disposition -0.002001 -0.003742 -0.005100  ...  0.004000  0.000236  0.000195\n",
              "allele      -0.001997 -0.003414 -0.004029  ...  0.004039  0.000990  0.000432\n",
              "\n",
              "[14000 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99g8y4QGUNpD"
      },
      "source": [
        "#### 3. Evaluation of Log transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9ZG5uek_ETv"
      },
      "source": [
        "import csv   \n",
        "fields=['14000','100']\n",
        "with open(r'14000-100-log.vec', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=' ')\n",
        "    writer.writerow(fields)\n",
        "V_df.to_csv(r'14000-100-log-SVD.vec', header=None, index=column_vocab[:14000], sep=' ', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS5qPhfc_ETw",
        "outputId": "c4b945a6-9ae4-440e-e729-f114453dd949"
      },
      "source": [
        "!python2 evaluate.py -m 14000-100-log-SVD.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '14000-100-log.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.59584\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.44574\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.54174\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.45973\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.37961\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.23053\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.44236\n",
            "\n",
            "19500/7777/19544: Add 0.19172, Mul 0.18259\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.19165, Mul Score: 0.18230\n",
            "8000/3876/8000: Add 0.23555, Mul 0.19969\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.23555, Mul Score: 0.19969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TZAile60a8W"
      },
      "source": [
        "### D. Transformation puissance alpha = 0.1 en utilisant PSDvec (taille embeddings = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIUBzAL90a8W"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jR0AU770a8X"
      },
      "source": [
        "def power_of_alpha(matrix, alpha):\n",
        "  \n",
        "  new_matrix = matrix.to_numpy()\n",
        "  new_matrix = np.power(matrix, alpha)\n",
        "\n",
        "  return new_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cODRB6C0a8X"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "alph_mat = power_of_alpha(matrice_cooccurence, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM6a5FBH1EJI"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_cooccurence = pd.DataFrame(alph_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVTnOYeE0a8X"
      },
      "source": [
        "#### 2. Modify Top2grams.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2zQHrC31QGL"
      },
      "source": [
        "words_dict = {}\n",
        "for column_elem in column_vocab : \n",
        "    if column_elem not in words_dict :\n",
        "        words_dict[column_elem] = {}\n",
        "        for row_elem in rows_vocab : \n",
        "            sub_word = df_cooccurence.loc[row_elem,column_elem]\n",
        "            if row_elem not in words_dict[column_elem] and sub_word != 0:\n",
        "                words_dict[column_elem][row_elem] = sub_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hix1l-Jf0a8X"
      },
      "source": [
        "import math \n",
        "\n",
        "with open('/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt', 'w') as f:\n",
        "  contents = open('/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_mini_wiki.txt').read()\n",
        "  f.write(contents[:contents.index(\"Bigrams:\\n\")])\n",
        "  f.write('Bigrams:\\n')\n",
        "  for i, (word_key, word_value) in enumerate(words_dict.items()):\n",
        "      #wordkey = myfair and its dict \n",
        "      nbr_cowords = len(word_value.keys())\n",
        "      nbr_occurence_cowords = int(sum(word_value.values())) #can change to word_value\n",
        "      bigram_word_row = [str(i), word_key, str(nbr_cowords), str(nbr_occurence_cowords), '0\\n']\n",
        "      f.write(','.join(bigram_word_row))\n",
        "      i = 1\n",
        "      for coword_key, coword_value in word_value.items() : \n",
        "          log_prob = math.log(coword_value/nbr_occurence_cowords)\n",
        "          bigram_coword_row = ['\\t' + coword_key, str(int(coword_value)), \"{:.3f}\".format(log_prob)]\n",
        "          #bigram_coword_row = ['\\t' + coword_key, str(int(coword_value)), \"{:.3f}\".format(log_prob) + '\\t']\n",
        "          f.write(','.join(bigram_coword_row))   \n",
        "          if i % 5 == 0 and i < nbr_cowords: \n",
        "              f.write('\\n')\n",
        "          i += 1\n",
        "      f.write('\\n')\n",
        "  contents = open('/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_mini_wiki.txt').read()\n",
        "  f.write(contents[contents.index('# Total kept bigram occurrences:'):])\n",
        "  #f.write('# Total kept bigram occurrences: 1339976\\n')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ABsE32DTEf7D",
        "outputId": "ceae8013-7250-4f7a-fba1-197efebf0e14"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/PSDVEC/topicvec/psdvec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wgRZN9tFoOs"
      },
      "source": [
        "%mkdir mini_wiki/alpha01/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxtODlonFwf9",
        "outputId": "3a720b9f-7e16-47cc-949f-dcce6b5a84ba"
      },
      "source": [
        "%cd mini_wiki/alpha01/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0g_hCIcGPXi"
      },
      "source": [
        "!cp -r /content/mydrive/PSDVEC/topicvec/psdvec/testsets/ /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71dycNCWMnM"
      },
      "source": [
        "#### 2. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UubfnCwoFUYW",
        "outputId": "75c65306-e3b5-4147-a80e-490f7420de07"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -w 2000 -n 50 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "1800\n",
            "0 (0.000%) elements in Weight-1 cut off at 1.73\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "1014 positive eigenvalues, sum: 31309.773\n",
            "Eigenvalues cut at the 53-th, 69.599 ~ 69.366\n",
            "All eigen sum: 62279.996, Kept eigen sum: 5963.655\n",
            "L1 Weighted: Gi: 751379.119, VV: 360536.397, Gsym-VV: 1091090.373, G-VV: 868371.482\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.62754\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.46207\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.67444\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.71527\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.55005\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.18368\n",
            "19500/571/19544: Add 0.34676, Mul 0.27846\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.34555, Mul Score: 0.27749\n",
            "8000/798/8000: Add 0.22180, Mul 0.12907\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.22180, Mul Score: 0.12907\n",
            "EM iter 1 elapsed: 1.80\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1120 positive eigenvalues, sum: 12651.306\n",
            "Eigenvalues cut at the 52-th, 70.271 ~ 69.476\n",
            "All eigen sum: 19338.955, Kept eigen sum: 5966.869\n",
            "L1 Weighted: Gi: 631743.411, VV: 411679.156, Gsym-VV: 1049610.146, G-VV: 721535.505\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.66063\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.46310\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.65408\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.72640\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.53973\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.19422\n",
            "19500/571/19544: Add 0.31699, Mul 0.24343\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.31588, Mul Score: 0.24258\n",
            "8000/798/8000: Add 0.19674, Mul 0.11905\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.19674, Mul Score: 0.11905\n",
            "EM iter 2 elapsed: 1.43\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1116 positive eigenvalues, sum: 11770.076\n",
            "Eigenvalues cut at the 51-th, 70.353 ~ 68.874\n",
            "All eigen sum: 17573.283, Kept eigen sum: 6015.010\n",
            "L1 Weighted: Gi: 687543.925, VV: 476392.962, Gsym-VV: 1040034.475, G-VV: 644835.828\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.64976\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.45027\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.62977\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.62578\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.51084\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16687\n",
            "19500/571/19544: Add 0.29247, Mul 0.23117\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.29145, Mul Score: 0.23037\n",
            "8000/798/8000: Add 0.17168, Mul 0.10025\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.17168, Mul Score: 0.10025\n",
            "EM iter 3 elapsed: 1.30\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1113 positive eigenvalues, sum: 11173.571\n",
            "Eigenvalues cut at the 50-th, 69.988 ~ 69.206\n",
            "All eigen sum: 16332.131, Kept eigen sum: 6014.796\n",
            "L1 Weighted: Gi: 724581.660, VV: 531798.349, Gsym-VV: 1042834.120, G-VV: 597067.662\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.62860\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.41878\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.61218\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.55365\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.52116\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16364\n",
            "19500/571/19544: Add 0.26795, Mul 0.21016\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.26702, Mul Score: 0.20942\n",
            "8000/798/8000: Add 0.15539, Mul 0.08772\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.15539, Mul Score: 0.08772\n",
            "EM iter 4 elapsed: 1.25\n",
            "we_factorize_EM() elapsed: 5.89\n",
            "\n",
            "Save matrix 'V' into 2000-50-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZr2x2uVFUYX"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XAkSqdCFUYY",
        "outputId": "a656b8a0-bbb8-4916-de41-f18c32871562"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-50-EM.vec -n 50 -o 4000 -t2 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-50-EM.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-50-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "\r1000    1000    0    \r\r2000    2000    0    \r\n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "5800 (2000 core, 3801 noncore)\n",
            "30 (0.000%) elements in Weight-1 cut off at 1.00\n",
            "30 (0.000%) elements in Weight-2 cut off at 1.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 6.60/0.16 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-50-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.57697\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.40520\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.56771\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.45596\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.34833\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.20318\n",
            "19500/2955/19544: Add 0.07039, Mul 0.05076\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.07037, Mul Score: 0.05084\n",
            "8000/2502/8000: Add 0.07154, Mul 0.03877\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.07154, Mul Score: 0.03877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DiTPshXFUYZ"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_rcOv37FUYZ",
        "outputId": "a7bf4c74-ccd7-41ce-be49-1ea93ff9daa9"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-6000-50-BLK-2.0.vec -n 50 -b 2000 -o 4000 -t4 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-6000-50-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-6000-50-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "6000    6000    0    \n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "9800 (2000 core, 3801 noncore)\n",
            "0 (0.000%) elements in Weight-1 cut off at 1.00\n",
            "0 (0.000%) elements in Weight-2 cut off at 1.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 7.68/0.18 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-50-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.53327\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.40482\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.51973\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.44975\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.25811\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.19028\n",
            "19500/5557/19544: Add 0.05075, Mul 0.03113\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.05074, Mul Score: 0.03119\n",
            "8000/3114/8000: Add 0.06005, Mul 0.02697\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.06005, Mul Score: 0.02697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIYq3uwfFUYa"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI4os6GuFUYb",
        "outputId": "0d485aff-bcc8-45ab-8de9-2d5963721caa"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-10000-50-BLK-4.0.vec -n 50 -b 2000 -o 4000 -t8 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-10000-50-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-10000-50-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "13800 (2000 core, 3801 noncore)\n",
            "0 (0.000%) elements in Weight-1 cut off at 1.00\n",
            "0 (0.000%) elements in Weight-2 cut off at 1.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 11.43/0.37 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-50-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.50003\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.40299\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.49955\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.44527\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.28649\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.19609\n",
            "19500/7777/19544: Add 0.03845, Mul 0.02250\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.03843, Mul Score: 0.02255\n",
            "8000/3876/8000: Add 0.04954, Mul 0.02064\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.04954, Mul Score: 0.02064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04_NwPQT0a8Z"
      },
      "source": [
        "#### . Evaluation of alpha (0.1) transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDvJ_lVq0a8Z",
        "outputId": "ddb27d34-8817-40d8-b414-60076b0ba50d"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/evaluate.py -m /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-14000-50-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01/2000-14000-50-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.50003\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.40299\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.49955\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.44527\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.28649\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.19609\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.58473\n",
            "\n",
            "19500/7777/19544: Add 0.03845, Mul 0.02250\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.03843, Mul Score: 0.02255\n",
            "8000/3876/8000: Add 0.04954, Mul 0.02064\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.04954, Mul Score: 0.02064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l8mtZxHXpek"
      },
      "source": [
        "### E. Transformation puissance alpha = 0.4 en utilisant PSDvec (taille embeddings = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8BfmQi8Xpek"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc0cCkkVXpel"
      },
      "source": [
        "def power_of_alpha(matrix, alpha):\n",
        "  \n",
        "  new_matrix = matrix.to_numpy()\n",
        "  new_matrix = np.power(matrix, alpha)\n",
        "\n",
        "  return new_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8bbIydtXpel"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "alph_mat = power_of_alpha(matrice_cooccurence, 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtyR2_HeXpem"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_cooccurence = pd.DataFrame(alph_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IjREnEVXpen"
      },
      "source": [
        "#### 2. Modify Top2grams.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mujkWgqWXpen"
      },
      "source": [
        "words_dict = {}\n",
        "for column_elem in column_vocab : \n",
        "    if column_elem not in words_dict :\n",
        "        words_dict[column_elem] = {}\n",
        "        for row_elem in rows_vocab : \n",
        "            sub_word = df_cooccurence.loc[row_elem,column_elem]\n",
        "            if row_elem not in words_dict[column_elem] and sub_word != 0:\n",
        "                words_dict[column_elem][row_elem] = sub_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwj8MOSoXpeo"
      },
      "source": [
        "import math \n",
        "path = '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/'\n",
        "with open(path + 'top2grams_alpha04.txt', 'w') as f:\n",
        "  contents = open(path + 'top2grams_mini_wiki.txt').read()\n",
        "  f.write(contents[:contents.index(\"Bigrams:\\n\")])\n",
        "  f.write('Bigrams:\\n')\n",
        "  for i, (word_key, word_value) in enumerate(words_dict.items()):\n",
        "      #wordkey = myfair and its dict \n",
        "      nbr_cowords = len(word_value.keys())\n",
        "      nbr_occurence_cowords = int(sum(word_value.values())) #can change to word_value\n",
        "      bigram_word_row = [str(i), word_key, str(nbr_cowords), str(nbr_occurence_cowords), '0\\n']\n",
        "      f.write(','.join(bigram_word_row))\n",
        "      i = 1\n",
        "      for coword_key, coword_value in word_value.items() : \n",
        "          log_prob = math.log(coword_value/nbr_occurence_cowords)\n",
        "          bigram_coword_row = ['\\t' + coword_key, str(int(coword_value)), \"{:.3f}\".format(log_prob)]\n",
        "          #bigram_coword_row = ['\\t' + coword_key, str(int(coword_value)), \"{:.3f}\".format(log_prob) + '\\t']\n",
        "          f.write(','.join(bigram_coword_row))   \n",
        "          if i % 5 == 0 and i < nbr_cowords: \n",
        "              f.write('\\n')\n",
        "          i += 1\n",
        "      f.write('\\n')\n",
        "  contents = open(path + 'top2grams_mini_wiki.txt').read()\n",
        "  f.write(contents[contents.index('# Total kept bigram occurrences:'):])\n",
        "  #f.write('# Total kept bigram occurrences: 1339976\\n')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7a-8xBWNXpeo",
        "outputId": "1c2bbde3-af1a-421a-9189-cd485325e5d6"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsHKnvP3Xpep",
        "outputId": "9d199de1-c66c-479d-f152-e1b486c3d100"
      },
      "source": [
        "path = '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/'\n",
        "%mkdir /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/\n",
        "%cd /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/\n",
        "!cp -r /content/mydrive/PSDVEC/topicvec/psdvec/testsets/ /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/’: File exists\n",
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQQfw6iRXpeq"
      },
      "source": [
        "#### 2. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ5MV4gwXpeq",
        "outputId": "3e98c4f9-4553-45fc-f4a4-19f68d1308d6"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -w 2000 -n 50 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "1800\n",
            "Cut point 10: 6/0.000%\n",
            "5 (0.000%) elements in Weight-1 cut off at 10.39\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "1006 positive eigenvalues, sum: 28222.715\n",
            "Eigenvalues cut at the 53-th, 62.302 ~ 61.995\n",
            "All eigen sum: 55440.848, Kept eigen sum: 5489.587\n",
            "L1 Weighted: Gi: 95072.579, VV: 66716.347, Gsym-VV: 199431.860, G-VV: 151365.821\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.63851\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.46028\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69125\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.71549\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.55418\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.18615\n",
            "19500/571/19544: Add 0.40981, Mul 0.36252\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.40838, Mul Score: 0.36126\n",
            "8000/798/8000: Add 0.26817, Mul 0.17419\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.26817, Mul Score: 0.17419\n",
            "EM iter 1 elapsed: 1.76\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1067 positive eigenvalues, sum: 6697.698\n",
            "Eigenvalues cut at the 52-th, 62.769 ~ 62.337\n",
            "All eigen sum: 7905.809, Kept eigen sum: 5398.909\n",
            "L1 Weighted: Gi: 72971.465, VV: 66566.247, Gsym-VV: 197854.227, G-VV: 148336.069\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.64569\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.48776\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69240\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.72818\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.51909\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16961\n",
            "19500/571/19544: Add 0.41856, Mul 0.36953\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.41710, Mul Score: 0.36824\n",
            "8000/798/8000: Add 0.27193, Mul 0.17669\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.27193, Mul Score: 0.17669\n",
            "EM iter 2 elapsed: 1.70\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1063 positive eigenvalues, sum: 6602.144\n",
            "Eigenvalues cut at the 51-th, 63.491 ~ 62.875\n",
            "All eigen sum: 7805.379, Kept eigen sum: 5338.151\n",
            "L1 Weighted: Gi: 73301.615, VV: 67368.085, Gsym-VV: 196224.368, G-VV: 144576.045\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.65574\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.49782\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.68876\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.73931\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.51290\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16158\n",
            "19500/571/19544: Add 0.42032, Mul 0.36602\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.41885, Mul Score: 0.36475\n",
            "8000/798/8000: Add 0.28321, Mul 0.18045\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.28321, Mul Score: 0.18045\n",
            "EM iter 3 elapsed: 1.65\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1061 positive eigenvalues, sum: 6528.420\n",
            "Eigenvalues cut at the 50-th, 63.743 ~ 63.573\n",
            "All eigen sum: 7718.688, Kept eigen sum: 5292.299\n",
            "L1 Weighted: Gi: 74538.023, VV: 68593.504, Gsym-VV: 194930.921, G-VV: 141226.419\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.65591\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.50320\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.68790\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.73931\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.48813\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16148\n",
            "19500/571/19544: Add 0.43257, Mul 0.36953\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.43106, Mul Score: 0.36824\n",
            "8000/798/8000: Add 0.29449, Mul 0.18797\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.29449, Mul Score: 0.18797\n",
            "EM iter 4 elapsed: 1.65\n",
            "we_factorize_EM() elapsed: 6.82\n",
            "\n",
            "Save matrix 'V' into 2000-50-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVI7J0VwXper"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC9IP2K3Xper",
        "outputId": "7f307a46-47e1-423e-e5a0-675cb0fe6a0d"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-50-EM.vec -n 50 -o 4000 -t2 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-50-EM.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-50-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "\r1000    1000    0    \r\r2000    2000    0    \r\n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "5800 (2000 core, 3801 noncore)\n",
            "350 (0.004%) elements in Weight-1 cut off at 3.32\n",
            "303 (0.004%) elements in Weight-2 cut off at 3.32\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 9.02/0.21 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-50-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.66103\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.51091\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.63064\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.62442\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.42077\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.21563\n",
            "19500/2955/19544: Add 0.17360, Mul 0.11810\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.17306, Mul Score: 0.11751\n",
            "8000/2502/8000: Add 0.16307, Mul 0.09073\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.16307, Mul Score: 0.09073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4GpgqyuXper"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA-bsLJrXper",
        "outputId": "ef4d7b49-abcc-4147-e5a7-da6b1ee1d7ac"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-6000-50-BLK-2.0.vec -n 50 -b 2000 -o 4000 -t4 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-6000-50-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-6000-50-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "6000    6000    0    \n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "9800 (2000 core, 3801 noncore)\n",
            "1781 (0.022%) elements in Weight-1 cut off at 2.24\n",
            "1401 (0.018%) elements in Weight-2 cut off at 2.24\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 6.62/0.16 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-50-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.64188\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.48956\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.57791\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.55305\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.33462\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.20926\n",
            "19500/5557/19544: Add 0.12705, Mul 0.07990\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.12675, Mul Score: 0.07960\n",
            "8000/3114/8000: Add 0.13295, Mul 0.07033\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.13295, Mul Score: 0.07033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPYzUv4mXpes"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkc3oXHrXpes",
        "outputId": "21b550a8-0ab3-4d6c-d40a-b8eed5638b5d"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-10000-50-BLK-4.0.vec -n 50 -b 2000 -o 4000 -t8 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-10000-50-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-10000-50-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "13800 (2000 core, 3801 noncore)\n",
            "1379 (0.017%) elements in Weight-1 cut off at 2.00\n",
            "943 (0.012%) elements in Weight-2 cut off at 2.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 5.99/0.15 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-50-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.60110\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.48817\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.55311\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.53332\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.35097\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.22579\n",
            "19500/7777/19544: Add 0.09554, Mul 0.05915\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.09531, Mul Score: 0.05893\n",
            "8000/3876/8000: Add 0.10630, Mul 0.05341\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.10630, Mul Score: 0.05341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh-bufgmXpes"
      },
      "source": [
        "#### 3. Evaluation of alpha (0.4) transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGKmTjR4Xpes",
        "outputId": "34b01590-800c-4ea0-833b-c9fa0404b044"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/evaluate.py -m /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-14000-50-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04/2000-14000-50-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.60110\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.48817\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.55311\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.53332\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.35097\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.22579\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.68768\n",
            "\n",
            "19500/7777/19544: Add 0.09554, Mul 0.05915\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.09531, Mul Score: 0.05893\n",
            "8000/3876/8000: Add 0.10630, Mul 0.05341\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.10630, Mul Score: 0.05341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5XJEtAcZewu"
      },
      "source": [
        "### F. Transformation Log en utilisant PSDvec (taille embeddings = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGtlCLhuZewu"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14x2ule3Zvbx"
      },
      "source": [
        "def log_transformation(matrix):\n",
        "  from tqdm import tqdm\n",
        "  from math import log\n",
        "  tqdm.pandas()\n",
        "\n",
        "  new_matrix = matrix\n",
        "  for column in matrix.columns:\n",
        "    new_matrix[column] = matrix[column].apply(lambda x:log(x) if x!=0 else 0)\n",
        "  \n",
        "  return new_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hznVaL5sZvbx",
        "outputId": "0bb28ef6-bd63-4553-f3ef-13a61cf5f45d"
      },
      "source": [
        "log_mat = log_transformation(matrice_cooccurence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bycpIpC9Zewv"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_cooccurence = pd.DataFrame(log_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCAunBUyZewv"
      },
      "source": [
        "#### 2. Modify Top2grams.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrFCbhz3Zewv"
      },
      "source": [
        "words_dict = {}\n",
        "for column_elem in column_vocab : \n",
        "    if column_elem not in words_dict :\n",
        "        words_dict[column_elem] = {}\n",
        "        for row_elem in rows_vocab : \n",
        "            sub_word = df_cooccurence.loc[row_elem,column_elem]\n",
        "            if row_elem not in words_dict[column_elem] and sub_word != 0:\n",
        "                words_dict[column_elem][row_elem] = sub_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boH-JXDEZeww"
      },
      "source": [
        "import math \n",
        "path = '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/'\n",
        "with open(path + 'top2grams_log.txt', 'w') as f:\n",
        "  contents = open(path + 'top2grams_mini_wiki.txt').read()\n",
        "  f.write(contents[:contents.index(\"Bigrams:\\n\")])\n",
        "  f.write('Bigrams:\\n')\n",
        "  for i, (word_key, word_value) in enumerate(words_dict.items()):\n",
        "      #wordkey = myfair and its dict \n",
        "      nbr_cowords = len(word_value.keys())\n",
        "      nbr_occurence_cowords = int(sum(word_value.values())) #can change to word_value\n",
        "      bigram_word_row = [str(i), word_key, str(nbr_cowords), str(nbr_occurence_cowords), '0\\n']\n",
        "      f.write(','.join(bigram_word_row))\n",
        "      i = 1\n",
        "      for coword_key, coword_value in word_value.items() : \n",
        "          log_prob = math.log(coword_value/nbr_occurence_cowords)\n",
        "          bigram_coword_row = ['\\t' + coword_key, str(int(coword_value)), \"{:.3f}\".format(log_prob)]\n",
        "          #bigram_coword_row = ['\\t' + coword_key, str(int(coword_value)), \"{:.3f}\".format(log_prob) + '\\t']\n",
        "          f.write(','.join(bigram_coword_row))   \n",
        "          if i % 5 == 0 and i < nbr_cowords: \n",
        "              f.write('\\n')\n",
        "          i += 1\n",
        "      f.write('\\n')\n",
        "  contents = open(path + 'top2grams_mini_wiki.txt').read()\n",
        "  f.write(contents[contents.index('# Total kept bigram occurrences:'):])\n",
        "  #f.write('# Total kept bigram occurrences: 1339976\\n')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mSW_1GfIZeww",
        "outputId": "a0a32515-2e91-46f6-f112-2fae76d057ee"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/PSDVEC/topicvec/psdvec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0FIQYyfZeww",
        "outputId": "f6a7cd34-be57-4575-9046-d07f9c98d708"
      },
      "source": [
        "path = '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/'\n",
        "%mkdir /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/\n",
        "%cd /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/\n",
        "!cp -r /content/mydrive/PSDVEC/topicvec/psdvec/testsets/ /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/’: File exists\n",
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e152gTxLZeww"
      },
      "source": [
        "#### 2. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iKskrLqZewx",
        "outputId": "ee6cc0e1-cd31-4f75-e01a-51ffb588d088"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -w 2000 -n 50 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "1800\n",
            "4 (0.000%) elements in Weight-1 cut off at 3.32\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "1016 positive eigenvalues, sum: 25058.326\n",
            "Eigenvalues cut at the 53-th, 60.008 ~ 59.669\n",
            "All eigen sum: 49015.844, Kept eigen sum: 5482.054\n",
            "L1 Weighted: Gi: 218046.379, VV: 141223.508, Gsym-VV: 306060.831, G-VV: 254261.296\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.58820\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.48207\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.67256\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.78807\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.37461\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.15204\n",
            "19500/571/19544: Add 0.46235, Mul 0.45009\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.46073, Mul Score: 0.44852\n",
            "8000/798/8000: Add 0.28195, Mul 0.21303\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.28195, Mul Score: 0.21303\n",
            "EM iter 1 elapsed: 1.35\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1036 positive eigenvalues, sum: 9256.871\n",
            "Eigenvalues cut at the 52-th, 64.461 ~ 63.800\n",
            "All eigen sum: 13031.690, Kept eigen sum: 5623.392\n",
            "L1 Weighted: Gi: 194751.894, VV: 146250.503, Gsym-VV: 292557.735, G-VV: 226877.907\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.59268\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.48543\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.67435\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.78762\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.34985\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.15628\n",
            "19500/571/19544: Add 0.50438, Mul 0.47285\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.50262, Mul Score: 0.47120\n",
            "8000/798/8000: Add 0.28571, Mul 0.19799\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.28571, Mul Score: 0.19799\n",
            "EM iter 2 elapsed: 1.25\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1029 positive eigenvalues, sum: 9220.645\n",
            "Eigenvalues cut at the 51-th, 68.068 ~ 67.999\n",
            "All eigen sum: 12817.898, Kept eigen sum: 5832.453\n",
            "L1 Weighted: Gi: 202657.468, VV: 154305.149, Gsym-VV: 287171.945, G-VV: 212333.139\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.59842\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.48052\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.67270\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.78695\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.35191\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.15523\n",
            "19500/571/19544: Add 0.50963, Mul 0.47636\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.50785, Mul Score: 0.47469\n",
            "8000/798/8000: Add 0.28697, Mul 0.22055\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.28697, Mul Score: 0.22055\n",
            "EM iter 3 elapsed: 1.24\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1028 positive eigenvalues, sum: 9276.527\n",
            "Eigenvalues cut at the 50-th, 72.281 ~ 71.490\n",
            "All eigen sum: 12720.603, Kept eigen sum: 6035.812\n",
            "L1 Weighted: Gi: 208834.158, VV: 161513.289, Gsym-VV: 285391.760, G-VV: 202989.041\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.60515\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.48345\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.67829\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.79764\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.33953\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16304\n",
            "19500/571/19544: Add 0.54116, Mul 0.47811\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.53927, Mul Score: 0.47644\n",
            "8000/798/8000: Add 0.29449, Mul 0.21930\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.29449, Mul Score: 0.21930\n",
            "EM iter 4 elapsed: 1.28\n",
            "we_factorize_EM() elapsed: 5.16\n",
            "\n",
            "Save matrix 'V' into 2000-50-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq59kdCCZewx"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBiIYQnwZewx",
        "outputId": "a772d23a-3bc9-4a49-cf1a-742818f51ff4"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-50-EM.vec -n 50 -o 4000 -t2 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-50-EM.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-50-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "\r1000    1000    0    \r\r2000    2000    0    \r\n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "5800 (2000 core, 3801 noncore)\n",
            "20 (0.000%) elements in Weight-1 cut off at 2.45\n",
            "24 (0.000%) elements in Weight-2 cut off at 2.45\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 6.07/0.14 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-50-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.69442\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.47432\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.62436\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.69770\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.48875\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.26775\n",
            "19500/2955/19544: Add 0.23689, Mul 0.17970\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.23569, Mul Score: 0.17879\n",
            "8000/2502/8000: Add 0.19784, Mul 0.11631\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.19784, Mul Score: 0.11631\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNZxIxbuZewx"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyqY6HzCZewx",
        "outputId": "45aabe2d-7f20-4d97-a4e2-814fced01a33"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-6000-50-BLK-2.0.vec -n 50 -b 2000 -o 4000 -t4 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-6000-50-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-6000-50-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "6000    6000    0    \n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "9800 (2000 core, 3801 noncore)\n",
            "621 (0.008%) elements in Weight-1 cut off at 2.00\n",
            "402 (0.005%) elements in Weight-2 cut off at 2.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 5.96/0.14 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-50-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.65473\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.48346\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.56037\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.59602\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.37452\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.24984\n",
            "19500/5557/19544: Add 0.17545, Mul 0.12309\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.17515, Mul Score: 0.12280\n",
            "8000/3114/8000: Add 0.15768, Mul 0.08799\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.15768, Mul Score: 0.08799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbD_ijPiZewy"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTutZB3eZewy",
        "outputId": "aa1209e9-d943-4db6-cde0-38098aac6af9"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-10000-50-BLK-4.0.vec -n 50 -b 2000 -o 4000 -t8 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-10000-50-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-10000-50-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "13800 (2000 core, 3801 noncore)\n",
            "1438 (0.018%) elements in Weight-1 cut off at 1.73\n",
            "993 (0.012%) elements in Weight-2 cut off at 1.73\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 5.90/0.14 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-50-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.63230\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.47502\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.54214\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.57482\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.36854\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.24032\n",
            "19500/7777/19544: Add 0.12820, Mul 0.08795\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.12798, Mul Score: 0.08775\n",
            "8000/3876/8000: Add 0.12848, Mul 0.06785\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.12848, Mul Score: 0.06785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luVIqlNEZewz"
      },
      "source": [
        "#### 3. Evaluation of log transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YfqQD-5Zewz",
        "outputId": "d1ea60b3-5d34-4d43-9ea8-f5b90a3d8637"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/evaluate.py -m /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-14000-50-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log/2000-14000-50-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.63230\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.47502\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.54214\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.57482\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.36852\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.24032\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.65419\n",
            "\n",
            "19500/7777/19544: Add 0.12820, Mul 0.08795\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.12798, Mul Score: 0.08775\n",
            "8000/3876/8000: Add 0.12848, Mul 0.06785\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.12848, Mul Score: 0.06785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGaOLzJ8alKK"
      },
      "source": [
        "### G. Transformation AFC en utilisant PSDvec (taille embeddings = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPbviOnZalKK"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrHPaDXToVig"
      },
      "source": [
        "def afc_transformation(matrix):\n",
        "  import numpy as np\n",
        "\n",
        "  #Dr somme selon les colonnes, axis = 1 on somme suivant les colonnes\n",
        "  #Dc some selon les lignes, axis = 0 on somme suivant les lignes\n",
        "  dr, dc = matrix.sum(1), matrix.sum(0)\n",
        "\n",
        "  dr, dc = np.array(dr), np.array(dc)\n",
        "  dr_inv, dc_inv = np.power(dr,-1/2), np.power(dc, -1/2)\n",
        "  Dr_inv, Dc_inv = np.diag(dr_inv), np.diag(dc_inv)\n",
        "\n",
        "  # return np.multiply(matrix - Dr_Dc, Dr_Dc_inv)\n",
        "  return Dr_inv.dot(matrice_cooccurence).dot(Dc_inv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeGvWgBaGg9f"
      },
      "source": [
        "AFC_mat = afc_transformation(matrice_cooccurence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcj1gd1HalKL"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_cooccurence = pd.DataFrame(AFC_mat, columns=column_vocab, index=rows_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3YcmkHQalKL"
      },
      "source": [
        "#### 2. Modify Top2grams.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDhBXBuIalKL"
      },
      "source": [
        "words_dict = {}\n",
        "for column_elem in column_vocab : \n",
        "    if column_elem not in words_dict :\n",
        "        words_dict[column_elem] = {}\n",
        "        for row_elem in rows_vocab : \n",
        "            sub_word = df_cooccurence.loc[row_elem,column_elem]\n",
        "            if row_elem not in words_dict[column_elem] and sub_word > 0:\n",
        "                words_dict[column_elem][row_elem] = sub_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDTlRw46alKM"
      },
      "source": [
        "import math \n",
        "path = '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/'\n",
        "with open(path + 'top2grams_afc.txt', 'w') as f:\n",
        "  contents = open(path + 'top2grams_mini_wiki.txt').read()\n",
        "  f.write(contents[:contents.index(\"Bigrams:\\n\")])\n",
        "  f.write('Bigrams:\\n')\n",
        "  for i, (word_key, word_value) in enumerate(words_dict.items()):\n",
        "      nbr_cowords = len(word_value.keys())\n",
        "      nbr_occurence_cowords = sum(word_value.values()) #can change to word_value\n",
        "      bigram_word_row = [str(i), word_key, str(nbr_cowords), str(nbr_occurence_cowords), '0\\n']\n",
        "      f.write(','.join(bigram_word_row))\n",
        "      i = 1\n",
        "      for coword_key, coword_value in word_value.items() : \n",
        "          log_prob = math.log(coword_value/nbr_occurence_cowords)\n",
        "          bigram_coword_row = ['\\t' + coword_key, str(coword_value), \"{:.3f}\".format(log_prob)]\n",
        "          f.write(','.join(bigram_coword_row))   \n",
        "          if i % 5 == 0 and i < nbr_cowords: \n",
        "              f.write('\\n')\n",
        "          i += 1\n",
        "      f.write('\\n')\n",
        "  contents = open(path + 'top2grams_mini_wiki.txt').read()\n",
        "  f.write(contents[contents.index('# Total kept bigram occurrences:'):])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "T0yPrHibalKM",
        "outputId": "b93386f5-0afa-40b9-ce8c-594096fb164c"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/PSDVEC/topicvec/psdvec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XJ30gdxalKM",
        "outputId": "aeb600f1-6e62-43ee-f37b-481a5a3a1864"
      },
      "source": [
        "%mkdir /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/\n",
        "%cd /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/\n",
        "!cp -r /content/mydrive/PSDVEC/topicvec/psdvec/testsets/ /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/’: File exists\n",
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/afc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8MlLggDalKM"
      },
      "source": [
        "#### 2. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prlM2BtSalKN",
        "outputId": "20ea971c-f26b-4f41-a288-1f55b97bc375"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -w 2000 -n 50 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "1800\n",
            "771 (0.019%) elements in Weight-1 cut off at 0.16\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "997 positive eigenvalues, sum: 26077.717\n",
            "Eigenvalues cut at the 53-th, 60.719 ~ 60.680\n",
            "All eigen sum: 50546.152, Kept eigen sum: 5637.553\n",
            "L1 Weighted: Gi: 129240.358, VV: 80266.254, Gsym-VV: 239139.372, G-VV: 191405.275\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.62276\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.49762\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69197\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.77471\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.48194\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.16736\n",
            "19500/571/19544: Add 0.49037, Mul 0.45709\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.48866, Mul Score: 0.45550\n",
            "8000/798/8000: Add 0.32832, Mul 0.25689\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.32832, Mul Score: 0.25689\n",
            "EM iter 1 elapsed: 2.30\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1012 positive eigenvalues, sum: 8207.706\n",
            "Eigenvalues cut at the 52-th, 63.922 ~ 62.344\n",
            "All eigen sum: 10777.859, Kept eigen sum: 5673.361\n",
            "L1 Weighted: Gi: 98631.580, VV: 81221.051, Gsym-VV: 238217.928, G-VV: 189901.348\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.63397\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.50232\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69435\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.76380\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.49020\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.15699\n",
            "19500/571/19544: Add 0.50788, Mul 0.47461\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.50611, Mul Score: 0.47295\n",
            "8000/798/8000: Add 0.32707, Mul 0.25689\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.32707, Mul Score: 0.25689\n",
            "EM iter 2 elapsed: 2.83\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1010 positive eigenvalues, sum: 8202.815\n",
            "Eigenvalues cut at the 51-th, 66.201 ~ 65.726\n",
            "All eigen sum: 10732.271, Kept eigen sum: 5719.059\n",
            "L1 Weighted: Gi: 99418.623, VV: 82795.389, Gsym-VV: 237212.768, G-VV: 188077.356\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.64197\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.51186\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69200\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.76781\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.46130\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.15004\n",
            "19500/571/19544: Add 0.52715, Mul 0.48161\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.52531, Mul Score: 0.47993\n",
            "8000/798/8000: Add 0.34211, Mul 0.26566\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.34211, Mul Score: 0.26566\n",
            "EM iter 3 elapsed: 2.94\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1009 positive eigenvalues, sum: 8208.673\n",
            "Eigenvalues cut at the 50-th, 68.375 ~ 67.841\n",
            "All eigen sum: 10698.287, Kept eigen sum: 5775.742\n",
            "L1 Weighted: Gi: 100801.479, VV: 84765.882, Gsym-VV: 236225.126, G-VV: 186079.255\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.64446\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.51124\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.68916\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.77560\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.44479\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.13692\n",
            "19500/571/19544: Add 0.52715, Mul 0.50788\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.52531, Mul Score: 0.50611\n",
            "8000/798/8000: Add 0.34211, Mul 0.27193\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.34211, Mul Score: 0.27193\n",
            "EM iter 4 elapsed: 2.26\n",
            "we_factorize_EM() elapsed: 10.39\n",
            "\n",
            "Save matrix 'V' into 2000-50-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYlfexgnalKN"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR1GbEbralKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993dccc5-0936-4466-e0ee-2d948aaef7b0"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-50-EM.vec -n 50 -o 4000 -t2 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-50-EM.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-50-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "\r1000    1000    0    \r\r2000    2000    0    \r\n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "5800 (2000 core, 3801 noncore)\n",
            "13173 (0.165%) elements in Weight-1 cut off at 0.06\n",
            "10940 (0.137%) elements in Weight-2 cut off at 0.06\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 14.77/0.36 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-50-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.67732\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.53219\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.63241\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.62661\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.42426\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.20726\n",
            "19500/2955/19544: Add 0.26193, Mul 0.21963\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.26128, Mul Score: 0.21919\n",
            "8000/2502/8000: Add 0.22662, Mul 0.14748\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.22662, Mul Score: 0.14748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzNbMjmJalKN"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWTg3UP6alKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e8720a-24d8-4b37-b7d9-cf99a556fce7"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-6000-50-BLK-2.0.vec -n 50 -b 2000 -o 4000 -t4 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-6000-50-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-6000-50-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "6000    6000    0    \n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "9800 (2000 core, 3801 noncore)\n",
            "40271 (0.503%) elements in Weight-1 cut off at 0.03\n",
            "36020 (0.450%) elements in Weight-2 cut off at 0.03\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 9.95/0.23 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-50-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.66314\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.49574\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.58069\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.52851\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.34731\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.20285\n",
            "19500/5557/19544: Add 0.19021, Mul 0.15368\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.18985, Mul Score: 0.15328\n",
            "8000/3114/8000: Add 0.19493, Mul 0.12107\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.19493, Mul Score: 0.12107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynIuRPzqalKO"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-szBP58EalKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa447a9-e5b6-405b-dbc5-8be7008d37c4"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-10000-50-BLK-4.0.vec -n 50 -b 2000 -o 4000 -t8 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-10000-50-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-10000-50-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "13800 (2000 core, 3801 noncore)\n",
            "59751 (0.747%) elements in Weight-1 cut off at 0.03\n",
            "55083 (0.689%) elements in Weight-2 cut off at 0.03\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 9.96/0.24 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-50-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.63101\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.49555\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.56197\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.51172\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.36783\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.22648\n",
            "19500/7777/19544: Add 0.14877, Mul 0.11701\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.14848, Mul Score: 0.11671\n",
            "8000/3876/8000: Add 0.16073, Mul 0.09727\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.16073, Mul Score: 0.09727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nd6VBYDalKO"
      },
      "source": [
        "#### 3. Evaluation of AFC transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEMfgCUbalKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85321f2-ad89-4600-d89f-7a724d451690"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/evaluate.py -m /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-14000-50-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc/2000-14000-50-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.63101\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.49555\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.56197\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.51172\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.36783\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.22648\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.64581\n",
            "\n",
            "19500/7777/19544: Add 0.14877, Mul 0.11701\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.14848, Mul Score: 0.11671\n",
            "8000/3876/8000: Add 0.16073, Mul 0.09727\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.16073, Mul Score: 0.09727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YufT8D46l6z"
      },
      "source": [
        "### H. Transformation puissance alpha = 0.1 en utilisant PSDvec (taille embeddings = 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8KW9xNKb6l69",
        "outputId": "ceae8013-7250-4f7a-fba1-197efebf0e14"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/PSDVEC/topicvec/psdvec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O814Qz7R6l6-"
      },
      "source": [
        "%mkdir mini_wiki/alpha01_100/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9de45r376l6-",
        "outputId": "fdb3dc5d-2cd7-4d01-8a53-0bb2c3ad8231"
      },
      "source": [
        "%cd mini_wiki/alpha01_100/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf0WBk8j6l6_"
      },
      "source": [
        "!cp -r /content/mydrive/PSDVEC/topicvec/psdvec/testsets/ /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJD0H2W6l7A"
      },
      "source": [
        "#### 2. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8XSMYwO6l7A",
        "outputId": "8f0781a8-806d-4d79-ec7f-b74ce8b9aa12"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -w 2000 -n 100 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "1800\n",
            "0 (0.000%) elements in Weight-1 cut off at 1.73\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "1014 positive eigenvalues, sum: 31309.773\n",
            "Eigenvalues cut at the 103-th, 60.031 ~ 59.972\n",
            "All eigen sum: 62279.996, Kept eigen sum: 9175.412\n",
            "L1 Weighted: Gi: 751379.119, VV: 397443.580, Gsym-VV: 1084545.101, G-VV: 846494.528\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.68004\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.55865\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69169\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.71883\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.60784\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.22167\n",
            "19500/571/19544: Add 0.26970, Mul 0.24168\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.26876, Mul Score: 0.24084\n",
            "8000/798/8000: Add 0.18421, Mul 0.11529\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.18421, Mul Score: 0.11529\n",
            "EM iter 1 elapsed: 1.53\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1155 positive eigenvalues, sum: 15503.043\n",
            "Eigenvalues cut at the 102-th, 60.219 ~ 59.987\n",
            "All eigen sum: 21830.676, Kept eigen sum: 9192.682\n",
            "L1 Weighted: Gi: 648851.564, VV: 445001.664, Gsym-VV: 1047002.950, G-VV: 704752.018\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.69529\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.56374\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.67218\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.71394\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.53148\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.22735\n",
            "19500/571/19544: Add 0.23818, Mul 0.18914\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.23735, Mul Score: 0.18848\n",
            "8000/798/8000: Add 0.16541, Mul 0.10652\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.16541, Mul Score: 0.10652\n",
            "EM iter 2 elapsed: 1.43\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1150 positive eigenvalues, sum: 14712.576\n",
            "Eigenvalues cut at the 101-th, 59.531 ~ 59.016\n",
            "All eigen sum: 20232.471, Kept eigen sum: 9253.684\n",
            "L1 Weighted: Gi: 701162.545, VV: 503260.642, Gsym-VV: 1038999.524, G-VV: 631327.122\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.68660\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.50960\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.64677\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.67053\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.49432\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.21735\n",
            "19500/571/19544: Add 0.20315, Mul 0.14536\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.20244, Mul Score: 0.14485\n",
            "8000/798/8000: Add 0.14035, Mul 0.08271\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.14035, Mul Score: 0.08271\n",
            "EM iter 3 elapsed: 1.26\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1145 positive eigenvalues, sum: 14181.726\n",
            "Eigenvalues cut at the 100-th, 58.986 ~ 58.503\n",
            "All eigen sum: 19109.768, Kept eigen sum: 9262.058\n",
            "L1 Weighted: Gi: 735555.352, VV: 553024.454, Gsym-VV: 1042634.510, G-VV: 586344.366\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.65113\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.47181\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.63302\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.58482\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.50877\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.20721\n",
            "19500/571/19544: Add 0.17688, Mul 0.12609\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.17627, Mul Score: 0.12565\n",
            "8000/798/8000: Add 0.11529, Mul 0.07644\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.11529, Mul Score: 0.07644\n",
            "EM iter 4 elapsed: 1.37\n",
            "we_factorize_EM() elapsed: 5.64\n",
            "\n",
            "Save matrix 'V' into 2000-100-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Wet6I16l7A"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJZmh6kr6l7B",
        "outputId": "46fcd72a-9be1-4b21-e9dd-6e0613edbed0"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-100-EM.vec -n 100 -o 4000 -t2 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-100-EM.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-100-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "2000    2000    0    \n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "5800 (2000 core, 3801 noncore)\n",
            "30 (0.000%) elements in Weight-1 cut off at 1.00\n",
            "30 (0.000%) elements in Weight-2 cut off at 1.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 31.07/0.81 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-100-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.55725\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.41795\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.53682\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.41294\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.36552\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.21102\n",
            "19500/2955/19544: Add 0.05415, Mul 0.03553\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.05421, Mul Score: 0.03569\n",
            "8000/2502/8000: Add 0.06155, Mul 0.03557\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.06155, Mul Score: 0.03557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrMFIwt36l7B"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ-94O7E6l7C",
        "outputId": "30b2c6fb-592e-4bb8-e847-8a828ac32be4"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-6000-100-BLK-2.0.vec -n 100 -b 2000 -o 4000 -t4 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-6000-100-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-6000-100-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "6000    6000    0    \n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "9800 (2000 core, 3801 noncore)\n",
            "0 (0.000%) elements in Weight-1 cut off at 1.00\n",
            "0 (0.000%) elements in Weight-2 cut off at 1.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 31.67/0.74 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-100-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.53030\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.42732\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.49561\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.40892\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.30831\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.19018\n",
            "19500/5557/19544: Add 0.03779, Mul 0.02213\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.03783, Mul Score: 0.02223\n",
            "8000/3114/8000: Add 0.05010, Mul 0.02858\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.05010, Mul Score: 0.02858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv_b6_Gw6l7C"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDm0e1736l7C",
        "outputId": "04ede0a1-7021-4fc4-968c-f3cfd64f4011"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-10000-100-BLK-4.0.vec -n 100 -b 2000 -o 4000 -t8 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-10000-100-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-10000-100-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha01.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "13800 (2000 core, 3801 noncore)\n",
            "0 (0.000%) elements in Weight-1 cut off at 1.00\n",
            "0 (0.000%) elements in Weight-2 cut off at 1.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 30.72/0.82 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-100-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.50758\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.43373\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.47936\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.40707\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.32636\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.19491\n",
            "19500/7777/19544: Add 0.02906, Mul 0.01594\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.02908, Mul Score: 0.01601\n",
            "8000/3876/8000: Add 0.03999, Mul 0.02270\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.03999, Mul Score: 0.02270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdaEuKNA6l7D"
      },
      "source": [
        "#### . Evaluation of alpha (0.1) transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NwoZqAI6l7D",
        "outputId": "9a29cfee-ad8d-46ff-83d0-ad3baad759fd"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/evaluate.py -m /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-14000-100-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01_100/2000-14000-100-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.50758\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.43373\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.47936\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.40707\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.32636\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.19491\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.66946\n",
            "\n",
            "19500/7777/19544: Add 0.02906, Mul 0.01594\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.02908, Mul Score: 0.01601\n",
            "8000/3876/8000: Add 0.03999, Mul 0.02270\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.03999, Mul Score: 0.02270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdGkb7SA6l7D"
      },
      "source": [
        "### I. Transformation puissance alpha = 0.4 en utilisant PSDvec (taille embeddings = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck_CfuMW6l7E"
      },
      "source": [
        "#### 1. Co-occurence matrix transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6DZxiv2A6l7F",
        "outputId": "1c2bbde3-af1a-421a-9189-cd485325e5d6"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/alpha01'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwYK-WM76l7F",
        "outputId": "1edb5a12-a602-482d-9004-ea62e5fd3697"
      },
      "source": [
        "path = '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/'\n",
        "%mkdir /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/\n",
        "%cd /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/\n",
        "!cp -r /content/mydrive/PSDVEC/topicvec/psdvec/testsets/ /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOIAvi2Y6l7G"
      },
      "source": [
        "#### 2. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUugtVQy6l7G",
        "outputId": "26a1343b-3a10-4cb7-8e85-77941e8db66e"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -w 2000 -n 100 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "1800\n",
            "Cut point 10: 6/0.000%\n",
            "5 (0.000%) elements in Weight-1 cut off at 10.39\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "1006 positive eigenvalues, sum: 28222.715\n",
            "Eigenvalues cut at the 103-th, 53.310 ~ 53.215\n",
            "All eigen sum: 55440.848, Kept eigen sum: 8344.787\n",
            "L1 Weighted: Gi: 95072.579, VV: 72635.977, Gsym-VV: 198506.139, G-VV: 148214.634\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.66753\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.57815\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.71100\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.74866\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.53148\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.22685\n",
            "19500/571/19544: Add 0.40105, Mul 0.37128\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.39965, Mul Score: 0.36998\n",
            "8000/798/8000: Add 0.25689, Mul 0.16667\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.25689, Mul Score: 0.16667\n",
            "EM iter 1 elapsed: 1.39\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1104 positive eigenvalues, sum: 9475.200\n",
            "Eigenvalues cut at the 102-th, 53.303 ~ 53.241\n",
            "All eigen sum: 10605.613, Kept eigen sum: 8255.602\n",
            "L1 Weighted: Gi: 78638.883, VV: 72543.216, Gsym-VV: 196900.392, G-VV: 144961.642\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.67256\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.58330\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.71421\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.73508\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.54799\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.23396\n",
            "19500/571/19544: Add 0.39229, Mul 0.36252\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.39092, Mul Score: 0.36126\n",
            "8000/798/8000: Add 0.25439, Mul 0.16792\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.25439, Mul Score: 0.16792\n",
            "EM iter 2 elapsed: 1.36\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1100 positive eigenvalues, sum: 9380.035\n",
            "Eigenvalues cut at the 101-th, 53.411 ~ 53.182\n",
            "All eigen sum: 10504.469, Kept eigen sum: 8201.360\n",
            "L1 Weighted: Gi: 79055.406, VV: 73392.582, Gsym-VV: 195343.739, G-VV: 141219.217\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.67608\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.58750\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.71294\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.74644\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.51496\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.23722\n",
            "19500/571/19544: Add 0.40280, Mul 0.35377\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.40140, Mul Score: 0.35253\n",
            "8000/798/8000: Add 0.26316, Mul 0.17794\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.26316, Mul Score: 0.17794\n",
            "EM iter 3 elapsed: 1.31\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1098 positive eigenvalues, sum: 9312.684\n",
            "Eigenvalues cut at the 100-th, 53.447 ~ 53.333\n",
            "All eigen sum: 10424.009, Kept eigen sum: 8160.939\n",
            "L1 Weighted: Gi: 80292.217, VV: 74590.299, Gsym-VV: 194121.369, G-VV: 137956.003\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.67382\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.58946\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.71562\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.75579\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.52941\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.24194\n",
            "19500/571/19544: Add 0.39930, Mul 0.37653\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.39791, Mul Score: 0.37522\n",
            "8000/798/8000: Add 0.26817, Mul 0.18672\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.26817, Mul Score: 0.18672\n",
            "EM iter 4 elapsed: 1.25\n",
            "we_factorize_EM() elapsed: 5.36\n",
            "\n",
            "Save matrix 'V' into 2000-100-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX90KovC6l7G"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_Utym8s6l7H",
        "outputId": "66f96af0-e71e-4884-d02d-2ffabf237465"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-100-EM.vec -n 100 -o 4000 -t2 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-100-EM.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-100-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "\r1000    1000    0    \r\r2000    2000    0    \r\n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "5800 (2000 core, 3801 noncore)\n",
            "350 (0.004%) elements in Weight-1 cut off at 3.32\n",
            "303 (0.004%) elements in Weight-2 cut off at 3.32\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 30.13/0.77 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-100-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.65703\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.54821\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.63347\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.60185\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.43826\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.24722\n",
            "19500/2955/19544: Add 0.15973, Mul 0.12961\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.15960, Mul Score: 0.12929\n",
            "8000/2502/8000: Add 0.15588, Mul 0.09792\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.15588, Mul Score: 0.09792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0AgyWMs6l7H"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf3iSotV6l7H",
        "outputId": "c4b1ea3a-4880-4531-c1b3-1cffbc108d0f"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-6000-100-BLK-2.0.vec -n 100 -b 2000 -o 4000 -t4 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-6000-100-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-6000-100-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "6000    6000    0    \n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "9800 (2000 core, 3801 noncore)\n",
            "1781 (0.022%) elements in Weight-1 cut off at 2.24\n",
            "1401 (0.018%) elements in Weight-2 cut off at 2.24\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 31.18/0.84 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-100-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.62577\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.52534\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.57951\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.54819\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.35553\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.22115\n",
            "19500/5557/19544: Add 0.12201, Mul 0.09016\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.12191, Mul Score: 0.09018\n",
            "8000/3114/8000: Add 0.13327, Mul 0.07803\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.13327, Mul Score: 0.07803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-9I3u6E6l7I"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF1DqJyu6l7I",
        "outputId": "04d95327-06c4-4015-baaa-a4b4db096d60"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-10000-100-BLK-4.0.vec -n 100 -b 2000 -o 4000 -t8 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-10000-100-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-10000-100-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_alpha04.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "13800 (2000 core, 3801 noncore)\n",
            "1379 (0.017%) elements in Weight-1 cut off at 2.00\n",
            "943 (0.012%) elements in Weight-2 cut off at 2.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 32.81/0.78 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-100-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.58410\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.52505\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.55133\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.52815\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.37720\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.24149\n",
            "19500/7777/19544: Add 0.08654, Mul 0.06429\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.08647, Mul Score: 0.06431\n",
            "8000/3876/8000: Add 0.10475, Mul 0.05882\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.10475, Mul Score: 0.05882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Yh_wAZ6l7I"
      },
      "source": [
        "#### 3. Evaluation of alpha (0.4) transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzdBJHC-6l7I",
        "outputId": "8b912773-ff7e-41d7-c018-8f39cc6b7d18"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/evaluate.py -m /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-14000-100-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/alpha04_100/2000-14000-100-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.58410\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.52505\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.55133\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.52815\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.37720\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.24149\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.70591\n",
            "\n",
            "19500/7777/19544: Add 0.08654, Mul 0.06429\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.08647, Mul Score: 0.06431\n",
            "8000/3876/8000: Add 0.10475, Mul 0.05882\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.10475, Mul Score: 0.05882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRbqMsJT6l7I"
      },
      "source": [
        "### J. Transformation Log en utilisant PSDvec (taille embeddings = 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BXU2-HRd6l7L",
        "outputId": "a0a32515-2e91-46f6-f112-2fae76d057ee"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/PSDVEC/topicvec/psdvec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_kRmyxW6l7M",
        "outputId": "01e05054-b29a-4b9e-df7a-820e0fee0e56"
      },
      "source": [
        "path = '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/'\n",
        "%mkdir /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/\n",
        "%cd /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/\n",
        "!cp -r /content/mydrive/PSDVEC/topicvec/psdvec/testsets/ /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/log_100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_12NLrJk6l7M"
      },
      "source": [
        "#### 2. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGYwukaQ6l7M",
        "outputId": "c37da9f6-2ef4-4e74-a40e-ac6058be7f9e"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -w 2000 -n 100 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "1800\n",
            "4 (0.000%) elements in Weight-1 cut off at 3.32\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "1016 positive eigenvalues, sum: 25058.326\n",
            "Eigenvalues cut at the 103-th, 47.038 ~ 46.992\n",
            "All eigen sum: 49015.844, Kept eigen sum: 8079.214\n",
            "L1 Weighted: Gi: 218046.379, VV: 155167.713, Gsym-VV: 305880.100, G-VV: 244706.774\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.63089\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.52259\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69329\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.77004\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.38700\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.21550\n",
            "19500/571/19544: Add 0.54991, Mul 0.55867\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.54799, Mul Score: 0.55672\n",
            "8000/798/8000: Add 0.33459, Mul 0.26566\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.33459, Mul Score: 0.26566\n",
            "EM iter 1 elapsed: 1.42\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1061 positive eigenvalues, sum: 11554.893\n",
            "Eigenvalues cut at the 102-th, 49.408 ~ 49.208\n",
            "All eigen sum: 15030.569, Kept eigen sum: 8309.912\n",
            "L1 Weighted: Gi: 204190.608, VV: 159841.139, Gsym-VV: 292856.528, G-VV: 217356.097\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.61329\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.52033\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69273\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.76158\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.33746\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.22306\n",
            "19500/571/19544: Add 0.58844, Mul 0.57968\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.58639, Mul Score: 0.57766\n",
            "8000/798/8000: Add 0.33584, Mul 0.28070\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.33584, Mul Score: 0.28070\n",
            "EM iter 2 elapsed: 1.42\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1056 positive eigenvalues, sum: 11606.229\n",
            "Eigenvalues cut at the 101-th, 51.892 ~ 51.325\n",
            "All eigen sum: 14902.547, Kept eigen sum: 8606.442\n",
            "L1 Weighted: Gi: 211641.285, VV: 167310.723, Gsym-VV: 287760.546, G-VV: 202683.334\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.61551\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.52807\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69414\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.74777\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.34159\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.21905\n",
            "19500/571/19544: Add 0.60771, Mul 0.60420\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.60558, Mul Score: 0.60209\n",
            "8000/798/8000: Add 0.34712, Mul 0.30201\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.34712, Mul Score: 0.30201\n",
            "EM iter 3 elapsed: 1.31\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1053 positive eigenvalues, sum: 11749.388\n",
            "Eigenvalues cut at the 100-th, 54.129 ~ 53.914\n",
            "All eigen sum: 14892.333, Kept eigen sum: 8900.604\n",
            "L1 Weighted: Gi: 217376.738, VV: 174029.740, Gsym-VV: 286116.427, G-VV: 193239.706\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.61284\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.52813\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.69761\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.73508\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.39112\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.21602\n",
            "19500/571/19544: Add 0.61821, Mul 0.60595\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.61606, Mul Score: 0.60384\n",
            "8000/798/8000: Add 0.36466, Mul 0.32080\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.36466, Mul Score: 0.32080\n",
            "EM iter 4 elapsed: 1.37\n",
            "we_factorize_EM() elapsed: 5.58\n",
            "\n",
            "Save matrix 'V' into 2000-100-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ofyLd86l7M"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdH5mnBk6l7M",
        "outputId": "7bdbd7c8-7ed3-483a-8288-fd37a2f6c142"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-100-EM.vec -n 100 -o 4000 -t2 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-100-EM.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-100-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "\r1000    1000    0    \r\r2000    2000    0    \r\n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "5800 (2000 core, 3801 noncore)\n",
            "20 (0.000%) elements in Weight-1 cut off at 2.45\n",
            "24 (0.000%) elements in Weight-2 cut off at 2.45\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 32.79/0.84 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-100-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.71466\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.51327\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.65255\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.70152\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.44975\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.29935\n",
            "19500/2955/19544: Add 0.28900, Mul 0.25144\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.28754, Mul Score: 0.25017\n",
            "8000/2502/8000: Add 0.25100, Mul 0.18026\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.25100, Mul Score: 0.18026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOB_yAAa6l7N"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ0vlV2b6l7N",
        "outputId": "10815f07-48b6-4dcb-f767-c79923bbc58c"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-6000-100-BLK-2.0.vec -n 100 -b 2000 -o 4000 -t4 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-6000-100-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-6000-100-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "\r1000    1000    0    \r\r2000    2000    0    \r\r3000    3000    0    \r\r4000    4000    0    \r\r5000    5000    0    \r\r6000    6000    0    \r\n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "9800 (2000 core, 3801 noncore)\n",
            "621 (0.008%) elements in Weight-1 cut off at 2.00\n",
            "402 (0.005%) elements in Weight-2 cut off at 2.00\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 31.59/0.77 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-100-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.67108\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.51932\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.57678\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.60412\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.37783\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.27350\n",
            "19500/5557/19544: Add 0.21828, Mul 0.18139\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.21764, Mul Score: 0.18089\n",
            "8000/3114/8000: Add 0.20424, Mul 0.14066\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.20424, Mul Score: 0.14066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7iRNoVZ6l7N"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh_dxPo36l7N",
        "outputId": "90619fdf-dbc7-4bc6-9d14-75016d3ec4a0"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-10000-100-BLK-4.0.vec -n 100 -b 2000 -o 4000 -t8 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-10000-100-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-10000-100-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_log.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "13800 (2000 core, 3801 noncore)\n",
            "1438 (0.018%) elements in Weight-1 cut off at 1.73\n",
            "993 (0.012%) elements in Weight-2 cut off at 1.73\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 31.96/0.78 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-100-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.65723\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.52385\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.55469\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.58717\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.37353\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.26710\n",
            "19500/7777/19544: Add 0.15109, Mul 0.12846\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.15078, Mul Score: 0.12823\n",
            "8000/3876/8000: Add 0.15531, Mul 0.10578\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.15531, Mul Score: 0.10578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjbgUJtB6l7O"
      },
      "source": [
        "#### 3. Evaluation of log transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmkrmHXA6l7O",
        "outputId": "605f419c-c5d7-452e-cffe-74f6436d864f"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/evaluate.py -m /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-14000-100-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/log_100/2000-14000-100-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.65723\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.52385\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.55469\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.58717\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.37353\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.26710\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.70197\n",
            "\n",
            "19500/7777/19544: Add 0.15109, Mul 0.12846\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.15078, Mul Score: 0.12823\n",
            "8000/3876/8000: Add 0.15531, Mul 0.10578\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.15531, Mul Score: 0.10578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNJ1LRzx6l7O"
      },
      "source": [
        "### K. Transformation AFC en utilisant PSDvec (taille embeddings = 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "k1sSPMDR6l7Q",
        "outputId": "2073d1bc-320a-459e-d1be-190c0782db2d"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/log_100'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFSZC0-O6l7Q",
        "outputId": "198831f8-7135-4257-c2b3-e186a560e5bc"
      },
      "source": [
        "%mkdir /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/\n",
        "%cd /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/\n",
        "!cp -r /content/mydrive/PSDVEC/topicvec/psdvec/testsets/ /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKO6QahE6l7Q"
      },
      "source": [
        "#### 2. Calculate 2000 core embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BDPPwOe6l7R",
        "outputId": "2e48db2c-f6d6-4928-a4cb-9ca4c2fffa88"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -w 2000 -n 100 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt':\n",
            "Totally 20259 words\n",
            "20259 words seen, top 2000 & 0 extra to keep. 2000 kept\n",
            "Read bigrams:\n",
            "1800\n",
            "771 (0.019%) elements in Weight-1 cut off at 0.16\n",
            "\n",
            "4 iterations of EM\n",
            "Begin EM of weighted factorization by bigram freqs\n",
            "\n",
            "EM Iter 1:\n",
            "Begin unweighted factorization\n",
            "997 positive eigenvalues, sum: 26077.717\n",
            "Eigenvalues cut at the 103-th, 49.328 ~ 49.154\n",
            "All eigen sum: 50546.152, Kept eigen sum: 8340.555\n",
            "L1 Weighted: Gi: 129240.358, VV: 87689.847, Gsym-VV: 238707.005, G-VV: 188077.653\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.65280\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.59915\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.71985\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.76714\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.52116\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.21661\n",
            "19500/571/19544: Add 0.53065, Mul 0.52715\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.52880, Mul Score: 0.52531\n",
            "8000/798/8000: Add 0.35088, Mul 0.27193\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.35088, Mul Score: 0.27193\n",
            "EM iter 1 elapsed: 1.59\n",
            "\n",
            "EM Iter 2:\n",
            "Begin unweighted factorization\n",
            "1047 positive eigenvalues, sum: 10635.423\n",
            "Eigenvalues cut at the 102-th, 50.197 ~ 50.083\n",
            "All eigen sum: 12930.293, Kept eigen sum: 8405.654\n",
            "L1 Weighted: Gi: 104371.254, VV: 89135.527, Gsym-VV: 237349.580, G-VV: 185768.517\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.65711\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.60171\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.72096\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.77827\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.52735\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.21321\n",
            "19500/571/19544: Add 0.56217, Mul 0.54991\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.56021, Mul Score: 0.54799\n",
            "8000/798/8000: Add 0.36842, Mul 0.28070\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.36842, Mul Score: 0.28070\n",
            "EM iter 2 elapsed: 1.40\n",
            "\n",
            "EM Iter 3:\n",
            "Begin unweighted factorization\n",
            "1045 positive eigenvalues, sum: 10639.756\n",
            "Eigenvalues cut at the 101-th, 51.167 ~ 50.965\n",
            "All eigen sum: 12873.855, Kept eigen sum: 8498.947\n",
            "L1 Weighted: Gi: 105592.917, VV: 91233.993, Gsym-VV: 236007.315, G-VV: 183193.056\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.66323\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.60512\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.72039\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.78673\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.50052\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.20442\n",
            "19500/571/19544: Add 0.59194, Mul 0.56217\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.58988, Mul Score: 0.56021\n",
            "8000/798/8000: Add 0.37343, Mul 0.28947\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.37343, Mul Score: 0.28947\n",
            "EM iter 3 elapsed: 1.21\n",
            "\n",
            "EM Iter 4:\n",
            "Begin unweighted factorization\n",
            "1045 positive eigenvalues, sum: 10673.209\n",
            "Eigenvalues cut at the 100-th, 52.148 ~ 51.839\n",
            "All eigen sum: 12847.472, Kept eigen sum: 8599.982\n",
            "L1 Weighted: Gi: 107385.929, VV: 93448.424, Gsym-VV: 234935.688, G-VV: 180896.074\n",
            "Precompute cosine matrix, will need 0.0GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 56 valid , 0.66538\n",
            "ws353_relatedness: 252 test pairs, 73 valid , 0.61414\n",
            "bruni_men: 3000 test pairs, 254 valid , 0.72089\n",
            "radinsky_mturk: 287 test pairs, 30 valid , 0.78317\n",
            "luong_rare: 2034 test pairs, 18 valid , 0.50052\n",
            "simlex_999a: 999 test pairs, 126 valid , 0.20243\n",
            "19500/571/19544: Add 0.61121, Mul 0.57793\n",
            "google: 19544 analogies, 573 valid . Add Score: 0.60908, Mul Score: 0.57592\n",
            "8000/798/8000: Add 0.38221, Mul 0.30827\n",
            "msr: 8000 analogies, 798 valid . Add Score: 0.38221, Mul Score: 0.30827\n",
            "EM iter 4 elapsed: 1.24\n",
            "we_factorize_EM() elapsed: 5.51\n",
            "\n",
            "Save matrix 'V' into 2000-100-EM.vec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUTLtB8h6l7R"
      },
      "source": [
        "#### 5. Calculate 4000 non core embeddings with tikhonov regularization 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95TL3wpp6l7R",
        "outputId": "ffbddfb3-1c91-4a49-aabf-7ebc6af3621c"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-100-EM.vec -n 100 -o 4000 -t2 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 2.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of all words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-100-EM.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-100-EM.vec'\n",
            "Will load embeddings of 2000 words\n",
            "2000    2000    0    \n",
            "2000 embeddings read, 2000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 0 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt' into 2 blocks. Will skip 0 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 0 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "5800 (2000 core, 3801 noncore)\n",
            "13173 (0.165%) elements in Weight-1 cut off at 0.06\n",
            "10940 (0.137%) elements in Weight-2 cut off at 0.06\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 31.62/0.88 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-6000-100-BLK-2.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.1GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 117 valid , 0.66592\n",
            "ws353_relatedness: 252 test pairs, 161 valid , 0.58408\n",
            "bruni_men: 3000 test pairs, 842 valid , 0.65717\n",
            "radinsky_mturk: 287 test pairs, 141 valid , 0.61430\n",
            "luong_rare: 2034 test pairs, 94 valid , 0.45665\n",
            "simlex_999a: 999 test pairs, 427 valid , 0.25989\n",
            "19500/2955/19544: Add 0.32420, Mul 0.29645\n",
            "google: 19544 analogies, 2970 valid . Add Score: 0.32357, Mul Score: 0.29562\n",
            "8000/2502/8000: Add 0.26019, Mul 0.18785\n",
            "msr: 8000 analogies, 2502 valid . Add Score: 0.26019, Mul Score: 0.18785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osSuIkez6l7R"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdNgUIHH6l7R",
        "outputId": "261b782d-a6d4-4ed3-af66-221706f8fa8a"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-6000-100-BLK-2.0.vec -n 100 -b 2000 -o 4000 -t4 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 4.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-6000-100-BLK-2.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-6000-100-BLK-2.0.vec'\n",
            "Will load embeddings of 6000 words\n",
            "6000    6000    0    \n",
            "6000 embeddings read, 6000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 4000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt' into 2 blocks. Will skip 4000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 4000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "9800 (2000 core, 3801 noncore)\n",
            "40271 (0.503%) elements in Weight-1 cut off at 0.03\n",
            "36020 (0.450%) elements in Weight-2 cut off at 0.03\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 31.14/0.75 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-10000-100-BLK-4.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.4GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 150 valid , 0.64820\n",
            "ws353_relatedness: 252 test pairs, 201 valid , 0.54538\n",
            "bruni_men: 3000 test pairs, 1436 valid , 0.60546\n",
            "radinsky_mturk: 287 test pairs, 204 valid , 0.54580\n",
            "luong_rare: 2034 test pairs, 174 valid , 0.37190\n",
            "simlex_999a: 999 test pairs, 627 valid , 0.22915\n",
            "19500/5557/19544: Add 0.24204, Mul 0.21360\n",
            "google: 19544 analogies, 5578 valid . Add Score: 0.24166, Mul Score: 0.21334\n",
            "8000/3114/8000: Add 0.22929, Mul 0.16249\n",
            "msr: 8000 analogies, 3114 valid . Add Score: 0.22929, Mul Score: 0.16249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JivwuvRR6l7R"
      },
      "source": [
        "#### 6. Calculate 4000 non core embeddings with tikhonov regularization 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm_m7afD6l7V",
        "outputId": "69c5ce31-40ba-4695-99ce-51e0ff217232"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/factorize.py -v /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-10000-100-BLK-4.0.vec -n 100 -b 2000 -o 4000 -t8 /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Tikhonov regularization with coeff: 8.0\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "Embeddings of top 2000 words in '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-10000-100-BLK-4.0.vec' will be loaded as core\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-10000-100-BLK-4.0.vec'\n",
            "Will load embeddings of 10000 words\n",
            "10000    10000    0    \n",
            "10000 embeddings read, 10000 kept\n",
            "2 blocks of 2000 core words and 4000 noncore words will be loaded. Skip 8000 words\n",
            "Loading bigram file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top2grams_afc.txt' into 2 blocks. Will skip 8000 words\n",
            "Totally 20259 words\n",
            "20259 words in file, top 6000 to read into vocab (2000 core, 4000 noncore), 8000 skipped\n",
            "Read bigrams:\n",
            "1800 (1801 core, 0 noncore)\n",
            "2000 core words are all read.\n",
            "13800 (2000 core, 3801 noncore)\n",
            "59751 (0.747%) elements in Weight-1 cut off at 0.03\n",
            "55083 (0.689%) elements in Weight-2 cut off at 0.03\n",
            "\n",
            "del G1, G21\n",
            "Begin finding embeddings of non-core words\n",
            "4000 / 4000. Elapsed: 31.05/0.75 \n",
            "del F21, WGsum, VW\n",
            "Save matrix 'V' into 2000-14000-100-BLK-8.0.vec\n",
            "Test embeddings derived from block factorization\n",
            "\n",
            "Precompute cosine matrix, will need 0.8GB RAM... Done.\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.61497\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.54588\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.58171\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.52580\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.38413\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.25291\n",
            "19500/7777/19544: Add 0.18966, Mul 0.16240\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.18934, Mul Score: 0.16218\n",
            "8000/3876/8000: Add 0.19247, Mul 0.13261\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.19247, Mul Score: 0.13261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4ocrpQw6l7V"
      },
      "source": [
        "#### 3. Evaluation of AFC transformation across 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NetimxaO6l7V",
        "outputId": "f16ad3b3-255a-4c8a-edb5-393a76ef57bc"
      },
      "source": [
        "!python2 /content/mydrive/PSDVEC/topicvec/psdvec/evaluate.py -m /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-14000-100-BLK-8.0.vec -d /content/mydrive/PSDVEC/topicvec/psdvec/testsets -u /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20260 words loaded from unigram file /content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/top1grams_mini_wiki.txt\n",
            "Load embedding text file '/content/mydrive/PSDVEC/topicvec/psdvec/mini_wiki/afc_100/2000-14000-100-BLK-8.0.vec'\n",
            "Will load embeddings of 14000 words\n",
            "14000    14000    0    \n",
            "14000 embeddings read, 14000 kept\n",
            "\n",
            "Read sim testset ./testsets/ws/ws353_similarity.txt\n",
            "Read sim testset ./testsets/ws/ws353_relatedness.txt\n",
            "Read sim testset ./testsets/ws/bruni_men.txt\n",
            "Read sim testset ./testsets/ws/radinsky_mturk.txt\n",
            "Read sim testset ./testsets/ws/luong_rare.txt\n",
            "Read sim testset ./testsets/ws/simlex_999a.txt\n",
            "Read sim testset ./testsets/ws/EN-RG-65.txt\n",
            "Read analogy testset ./testsets/analogy/google.txt\n",
            "Read analogy testset ./testsets/analogy/msr.txt\n",
            "\n",
            "ws353_similarity: 203 test pairs, 161 valid , 0.61497\n",
            "ws353_relatedness: 252 test pairs, 217 valid , 0.54588\n",
            "bruni_men: 3000 test pairs, 1817 valid , 0.58171\n",
            "radinsky_mturk: 287 test pairs, 237 valid , 0.52580\n",
            "luong_rare: 2034 test pairs, 262 valid , 0.38413\n",
            "simlex_999a: 999 test pairs, 774 valid , 0.25291\n",
            "EN-RG-65: 65 test pairs, 29 valid , 0.70394\n",
            "\n",
            "19500/7777/19544: Add 0.18966, Mul 0.16240\n",
            "google: 19544 analogies, 7806 valid . Add Score: 0.18934, Mul Score: 0.16218\n",
            "8000/3876/8000: Add 0.19247, Mul 0.13261\n",
            "msr: 8000 analogies, 3876 valid . Add Score: 0.19247, Mul Score: 0.13261\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}